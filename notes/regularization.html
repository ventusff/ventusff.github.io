<!DOCTYPE html>
<html>
  <head>
    
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  <title>Jianfei Guo | regularizations</title>
  <meta name="description" content="Jianfei Guo. ffventus (at) gmail.com. I am a learner in representation learning and decision making. 
">

  <link rel="shortcut icon" href="https://ventusff.github.io/assets/img/favicon.ico?v=1">

  <link rel="stylesheet" href="https://ventusff.github.io/assets/css/main.css">
  <link rel="canonical" href="https://ventusff.github.io/notes/regularization">
  

    
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams',
      inlineMath: [ ['$','$'], ["\\(","\\)"] ], // http://docs.mathjax.org/en/latest/input/tex/delimiters.html
      displayMath: [['\\[','\\]'], ['$$','$$']]
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    
<!-- Mermaid Js -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/mermaid/8.7.0/mermaid.min.js"></script>
<!-- <script src="https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js"></script> // this is the old version --> 

  </head>

  <body>

    <header class="site-header">

  <div class="wrapper">

    
    <span class="site-title">
        
        <strong>Jianfei</strong> Guo
    </span>
    

    <nav class="site-nav">
      <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
              <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
              <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

      <div class="trigger">
        <!-- About -->
        <a class="page-link" href="https://ventusff.github.io/">about</a>

        <!-- Blog -->
        <a class="page-link" href="https://ventusff.github.io/blog/">blog</a>

        <!-- Pages -->
        
          
        
          
        
          
        
          
            <a class="page-link" href="https://ventusff.github.io/notes/">notes</a>
          
        
          
        
          
        
          
        

        <!-- CV link -->
        <!-- <a class="page-link" href="https://ventusff.github.io/assets/pdf/CV.pdf">vitae</a> -->

      </div>
    </nav>

  </div>

</header>



    <div class="page-content">
      <div class="wrapper">
        <div class="post">

  <header class="post-header">
    <h1 class="post-title">regularizations</h1>
    <p class="post-subtitle">正则化基础讨论</p>
    <p class="post-meta"></p>
  </header>

  <article class="post-content">
    <ul id="markdown-toc">
  <li><a href="#vector-norm" id="markdown-toc-vector-norm">vector norm</a>    <ul>
      <li><a href="#l0-norm" id="markdown-toc-l0-norm">L0-norm</a></li>
      <li><a href="#l1-norm" id="markdown-toc-l1-norm">L1-norm</a></li>
      <li><a href="#l2-norm" id="markdown-toc-l2-norm">L2-norm</a></li>
    </ul>
  </li>
  <li><a href="#matrix-norm" id="markdown-toc-matrix-norm">matrix norm</a>    <ul>
      <li><a href="#frobenius-norm" id="markdown-toc-frobenius-norm">Frobenius Norm</a></li>
    </ul>
  </li>
  <li><a href="#math" id="markdown-toc-math">math</a>    <ul>
      <li><a href="#laplacebeltrami-operator" id="markdown-toc-laplacebeltrami-operator">Laplace–Beltrami operator</a></li>
    </ul>
  </li>
  <li><a href="#weight-decay" id="markdown-toc-weight-decay">weight decay</a></li>
  <li><a href="#tikhonov-regularization" id="markdown-toc-tikhonov-regularization">Tikhonov regularization</a></li>
  <li><a href="#manifold-regularization" id="markdown-toc-manifold-regularization">manifold regularization</a>    <ul>
      <li><a href="#laplacian-regularization--laplacian-norm" id="markdown-toc-laplacian-regularization--laplacian-norm">Laplacian regularization / Laplacian norm</a></li>
    </ul>
  </li>
</ul>

<hr />

<ul>
  <li>regularization：改进问题的conditioning；把问题从ill-posed变成well-posed</li>
</ul>

<h2 id="vector-norm">vector norm</h2>

<p>考虑向量 \(x=\begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{bmatrix}\)</p>

<h3 id="l0-norm">L0-norm</h3>
<ul>
  <li>approaximate: \(\exp^{-\alpha x}, \quad \text{where} \; \alpha \gg 1\)
    <ul>
      <li>接近信号与系统中的冲激函数：\(x=0\)时取1，x离0稍微远一点函数值取0</li>
      <li>e.g.  <em>Deformed Implicit Field: Modeling 3D Shapes with Learned Dense Correspondence</em>的SDF loss中的一项：对SDF训练过程中靠近表面的非表面点的惩罚</li>
    </ul>
  </li>
</ul>

<h3 id="l1-norm">L1-norm</h3>

<ul>
  <li>
\[\|x\|_1 = \sum_{k=1}^n | x_k |\]
  </li>
</ul>

<h3 id="l2-norm">L2-norm</h3>

<ul>
  <li>
\[\|x\|_2 = \sqrt{x_1^2+x_2^2+\ldots + x_n^2}=\left(\sum_{k=1}^n x_k^2 \right)^{1/2}\]
    <ul>
      <li>梯度：\(\nabla \|x\|_2 = \frac{x}{\|x\|_2}\)
        <ul>
          <li>其梯度大小与向量的magnitude无关</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>平方
    <ul>
      <li>平方为：\(f(x)=\|x\|^2_2= \left(\left(\sum_{k=1}^n x_k^2 \right)^{1/2}\right)^{2}=\sum_{k=1}^n x_k^2\)</li>
      <li><a href="https://math.stackexchange.com/questions/883016/gradient-of-l2-norm-squared">平方的梯度</a>：\(\frac{\partial}{\partial x_j}f(x)  =\frac{\partial}{\partial x_j}\sum_{k=1}^n x_k^2=\sum_{k=1}^n \underbrace{\frac{\partial}{\partial x_j}x_k^2}_{\substack{=0, \ \text{ if } j \neq k,\\=2x_j, \ \text{ else }}}= 2x_j.\)<br />i.e. \(\nabla \|x\|^2_2 = 2x.\)
        <h2 id="matrix-norm">matrix norm</h2>
      </li>
    </ul>
  </li>
  <li><a href="https://en.wikipedia.org/wiki/Matrix_norm">wiki</a></li>
</ul>

<h3 id="frobenius-norm">Frobenius Norm</h3>

<ul>
  <li>
\[\lVert \boldsymbol{\rm A} \rVert_{F}=\sqrt{\sum_{i=1}^m \sum_{j=1}^n \lvert a_{ij} \rvert^2} = \sqrt{\text{trace}(A^{\top}A)}\]
    <ul>
      <li>梯度：</li>
    </ul>
  </li>
  <li>平方：
    <ul>
      <li>平方为：</li>
      <li><a href="https://math.stackexchange.com/questions/229422/gradient-of-squared-frobenius-norm">平方的梯度</a>：</li>
    </ul>
  </li>
</ul>

<h2 id="math">math</h2>

<h3 id="laplacebeltrami-operator">Laplace–Beltrami operator</h3>

<ul>
  <li>拉普拉斯-贝尔特拉米算子</li>
  <li>在微分几何中，拉普拉斯算子可以推广定义到曲面上，或者更一般地黎曼流形、伪黎曼流形上，这个更一般的算子就叫做拉普拉斯-贝尔斯特拉算子；</li>
  <li>与拉普拉斯算子一样，拉普拉斯-贝尔特拉米算子定义为梯度的散度</li>
</ul>

<h2 id="weight-decay">weight decay</h2>

<ul>
  <li><a href="https://towardsdatascience.com/this-thing-called-weight-decay-a7cd4bcfccab">toward data science</a></li>
  <li></li>
</ul>

<h2 id="tikhonov-regularization"><a href="https://en.wikipedia.org/wiki/Tikhonov_regularization">Tikhonov regularization</a></h2>

<ul>
  <li>是<code class="language-plaintext highlighter-rouge">非适定性问题</code>的正则化的最常见的方法</li>
  <li>在统计学中被称为 <code class="language-plaintext highlighter-rouge">ridge regression</code>脊回归</li>
  <li>在机器学习中，被称为<code class="language-plaintext highlighter-rouge">weight decay</code> 权重衰减</li>
  <li>又被称作 <code class="language-plaintext highlighter-rouge">linear regularization</code> 线性正则化</li>
  <li>为了解\(\boldsymbol{A}\boldsymbol{\rm x}=\boldsymbol{\rm b}\)的问题，标准方法是最小二乘法<br />然而，如果x没有解或者有超出一个解(i.e. 方程不是unique的)<br />那么问题就是ill-posed；
    <ul>
      <li>这种情况下，ordinary 最小二乘问题问题变得 overdetermined / more often an undetermined system<br />许多现实世界问题，从A到b的映射往往是低通滤波器；因此，在解逆问题的时候，逆映射就会变成高通滤波器，表现出我们不需要的那些放大噪声（最小的 eigenvalues / singular values 在逆映射会变得最大）的趋向</li>
      <li>另外，ordinary 最小二乘implicitly nullifies every element of the reconstructed version of x that is in the null-space of A，rather than allowing for a model to be used as a prior for x。最小二乘对于位于A的null-space的那些x的元素不会利用先验，而是nullify 他们</li>
      <li>ordinary least squares 的目标：最小化平方残差和<br />\(\lVert \boldsymbol{A}\boldsymbol{\rm x}-\boldsymbol{\rm b} \rVert^2_2\)，其中\(\lVert \cdot \rVert^2_2\)代表欧几里得范数</li>
      <li>为了给出一个对带有需求属性的特定解的偏好，Tikhonov regularization 在最小化目标中额外加入一项正则化：<br />\(\lVert \boldsymbol{A}\boldsymbol{\rm x}-\boldsymbol{\rm b} \rVert^2_2 + \lVert \Gamma \boldsymbol{\rm x} \rVert^2_2\)<br />for some suitably chosen TIkhonov matrix \(\Gamma\)
        <ul>
          <li>如选择单位阵的乘积：\(\Gamma=\alpha I\)，意味着偏好那些有更小的norm的solution (i.e. \(L_2\) regularization)</li>
          <li>如选择high-pass 算子（如微分算子或加权傅里叶算子）可以用来保证平滑性，如果underlying vector基本是线性的</li>
        </ul>
      </li>
      <li>这样的正则化可以改善问题的conditioning，从而可以得到一个直接的数值解</li>
    </ul>
  </li>
  <li>更一般的tikhonov regularization
    <ul class="task-list">
      <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Q: (笔者的)大概理解：不应是欧几里得范数\(\boldsymbol{\rm x}^{\top} \boldsymbol{\rm x}\)，而是更一般的范数\(\boldsymbol{\rm x}^{\top}Q \boldsymbol{\rm x}\)</li>
      <li class="task-list-item">\(\lVert \boldsymbol{A}\boldsymbol{\rm x}-\boldsymbol{\rm b} \rVert^2_{P} + \lVert \Gamma \boldsymbol{\rm x} \rVert^2_{Q}\), 其中\(\lVert x \rVert^2_{Q}\)代表weighted norm squared \(\boldsymbol{\rm x}^{\top}Q \boldsymbol{\rm x}\)</li>
      <li class="task-list-item">这种意义下，Tikhonov matrix其实是给出的矩阵的分解矩阵：\(Q=\Gamma^{\top}\Gamma\)</li>
    </ul>
  </li>
</ul>

<h2 id="manifold-regularization"><a href="https://en.wikipedia.org/wiki/Manifold_regularization">manifold regularization</a></h2>

<ul>
  <li>在机器学习中，manifold regularization是一种 利用数据集的<u>shape</u> 来约束从数据集学到的函数的技术</li>
  <li><strong>manifold learning</strong>
    <ul>
      <li>因为在许多机器学习问题中，数据并没有cover整个输入空间；
        <ul>
          <li>e.g. 人脸识别系统 并不需要识别任何可能的图像，而是只学习那些包含人脸的图像空间子集</li>
        </ul>
      </li>
      <li>manifold learning技术假设数据来自于一个流形；</li>
      <li>manifold learning技术假设将要学到的函数是 <em>smooth</em>的：带有不同标签的数据不太可能彼此接近/集中在一起，因此标签函数在<u>含有很多数据点的区域</u>不应改变太快
        <ul>
          <li>基于这种假设，manifold regularization算法可以额外利用那些unlabeled data来inform 哪些地方学到的函数要变地快，哪些地方变地慢；</li>
          <li>这里用到了Tikhonov regularization的延伸</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>manifold regularization</strong>
    <ul>
      <li>是正则化的一种，防止过拟合，通过惩罚complex solution来保证问题well-posed</li>
      <li>是 <code class="language-plaintext highlighter-rouge">Tikhonov regularization</code> 吉洪诺夫正则化 的延伸</li>
      <li><code class="language-plaintext highlighter-rouge">manifold assumption</code>：即问题中的数据不是来自于整个输入空间\(X\)，而是来自于其中的一个非线性流形\(M \subset X\)
        <ul>
          <li>因此，这个流形 / <code class="language-plaintext highlighter-rouge">intrinsic space</code>的<u>geometry</u>就可以用来决定正则化项</li>
        </ul>
      </li>
      <li>这种算法常可以用来扩展<code class="language-plaintext highlighter-rouge">semi-supervised</code>半监督学习和<code class="language-plaintext highlighter-rouge">transductive learning</code>“直推”式学习，因为那些算法中都有unlabeled data</li>
    </ul>
  </li>
</ul>

<h3 id="laplacian-regularization--laplacian-norm">Laplacian regularization / Laplacian norm</h3>

<ul>
  <li>这个名称来自于拉普拉斯算子（梯度的散度）</li>
  <li>对流形M的梯度进行操作，即提供了一种目标函数有多平滑的衡量；</li>
  <li>一个平滑的函数，应当在输入数据稠密的地方改变缓慢；<br />i.e. 梯度\(\nabla_Mf(x)\)在那些<code class="language-plaintext highlighter-rouge">marginal probability density</code> \(\mathcal{P}_X(x)\)大的地方足应当小<br />\(\mathcal{P}_X(x)\)：the probabilistic density of a randomly drawn data point appearing at x，一个随机采样的数据点在x处出现的概率密度<br />即：\(\lVert f \rVert^2_I=\int_{x\in M} \lVert \nabla_Mf(x) \rVert^2 {\rm d}\mathcal{P}_X(x)\)</li>
  <li>然而实践中，由于边缘分布\(\mathcal{P}_X\)是未知的，这个范数并不能直接计算；但是它可以从所提供的数据中估计
    <ul>
      <li>具体来说，如果输入点之间的距离可以用<code class="language-plaintext highlighter-rouge">graph</code>图来表示，那么<code class="language-plaintext highlighter-rouge">graph</code>的<code class="language-plaintext highlighter-rouge">laplacian matrix</code>拉普拉斯矩阵可以用来估计边缘概率分布</li>
      <li>假设数据有\(\mathcal{l}\)个有标签的样本(有输入x和输出y的pair)，\(u\)个无标签的样本</li>
      <li>定义<em>W</em>是graph的权重矩阵，其中\(W_{ij}\)是输入数据点\(x_i\)和\(x_j\)之间的距离度量</li>
      <li>定义<em>D</em>是对角阵：\(D=\sum_{j=1}^{\mathcal{l}+u}W_{ij}\)</li>
      <li>定义<em>L</em>为\(L=D-W\)即为拉普拉斯矩阵</li>
      <li>当数据点的个数\(\mathcal{l}+u\)逐渐增加时，矩阵\(L\)就可以收敛到<code class="language-plaintext highlighter-rouge">Laplace–Beltrami operator</code>拉普拉斯-贝尔特拉米算子\(\Delta_M\) (推广到一般曲面、黎曼流形、伪黎曼流形的拉普拉斯算子)，即梯度的<code class="language-plaintext highlighter-rouge">散度</code></li>
      <li>定义\(\boldsymbol{\rm f}\)为函数\(f\)在这些数据点上的函数值\(\boldsymbol{\rm f}=[f(x_1), \ldots,f(x_{\mathcal{l}+u})]^{\top}\)</li>
      <li>则这个intrinsic norm可以这样被估计：<br />\(\lVert f \rVert^2_I=\frac {1}{(\mathcal{l}+u)^2} \boldsymbol{\rm f}^{\top}L\boldsymbol{\rm f}\)</li>
    </ul>
  </li>
</ul>

  </article>

  

</div>

      </div>
    </div>

    <footer>

  <div class="wrapper">
    &copy; Copyright 2021 Jianfei Guo.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank">Unsplash</a>.

    
  </div>

</footer>


    <!-- Load jQuery -->
<script src="//code.jquery.com/jquery-1.12.4.min.js"></script>

<!-- Load Common JS -->
<script src="https://ventusff.github.io/assets/js/common.js"></script>




<!-- Load Anchor JS -->
<script src="//cdnjs.cloudflare.com/ajax/libs/anchor-js/3.2.2/anchor.min.js"></script>
<script>
  anchors.options.visible = 'always';
  anchors.add('article h2, article h3, article h4, article h5, article h6');
</script>



<!-- Load fancybox -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css" />
<script src="https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js"></script>


<!-- Include custom icon fonts -->
<link rel="stylesheet" href="https://ventusff.github.io/assets/css/fontawesome-all.min.css">
<link rel="stylesheet" href="https://ventusff.github.io/assets/css/academicons.min.css">
<link rel="stylesheet" href="https://ventusff.github.io/assets/iconfont/iconfont.css">

<!-- Google Analytics -->
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-XXXXXXXXX', 'auto');
ga('send', 'pageview');
</script>

    
  </body>
  
<!-- Mermaid Js -->
<script>

    // mermaid rendering: inspired by https://stackoverflow.com/questions/53883747/how-to-make-github-pages-markdown-support-mermaid-diagram
    // mermiad in details tag: inspired by https://gitlab.com/gitlab-org/gitlab/-/issues/28495
    //          and by https://gitlab.com/gitlab-org/gitlab/-/blob/master/app/assets/javascripts/behaviors/markdown/render_mermaid.js

    function renderMermaids($els) {
        if (!$els.length) return;

        var config = {
            startOnLoad: false,
            theme: 'neutral', // forest
            securityLevel: 'loose',
            flowchart:{
                    useMaxWidth:true,
                    htmlLabels:false, // important for not squeezing blocks
                }
        };
        // const theme =localStorage.getItem("theme");
        // if(theme == "dark") config.theme = 'dark';  // currently needs to refresh to make dark mode toggle
        mermaid.initialize(config);

        $els.each((i, el) => {

            // Mermaid doesn't like `<br />` tags, so collapse all like tags into `<br>`, which is parsed correctly.
            const source = el.textContent.replace(/<br\s*\/>/g, '<br>');

            // Remove any extra spans added by the backend syntax highlighting.
            Object.assign(el, { textContent: source });

            mermaid.init(undefined, el, id => {
                const svg = document.getElementById(id);

                // As of https://github.com/knsv/mermaid/commit/57b780a0d,
                // Mermaid will make two init callbacks:one to initialize the
                // flow charts, and another to initialize the Gannt charts.
                // Guard against an error caused by double initialization.
                if (svg.classList.contains('mermaid')) {
                    console.log("return");
                    return;
                }

                // svg.classList.add('mermaid'); //will add new bug

                // pre > code > svg
                svg.closest('pre').replaceWith(svg);

                // We need to add the original source into the DOM to allow Copy-as-GFM
                // to access it.
                const sourceEl = document.createElement('text');
                sourceEl.classList.add('source');
                sourceEl.setAttribute('display', 'none');
                sourceEl.textContent = source;

                svg.appendChild(sourceEl);

            });
        });

    }

    const $els = $(document).find('.language-mermaid');
    if ($els.length)
    {
        const visibleMermaids = $els.filter(function filter() {
            return $(this).closest('details').length === 0 && $(this).is(':visible');
        });

        renderMermaids(visibleMermaids);

        $els.closest('details').one('toggle', function toggle() {
            if (this.open) {
                renderMermaids($(this).find('.language-mermaid'));
            }
        });
    }



</script>


  <!-- Auto config image to fancybox -->
<script>
    $(document).ready(function() {
        $("article img[class!='emoji']").each(function() {
            var currentImage = $(this);
            currentImage.wrap("<a href='" + currentImage.attr("src") + "' data-fancybox='lightbox' data-caption='" + currentImage.attr("alt") + "'></a>");
        });
    });
</script>

</html>
