<!DOCTYPE html>
<html>
  <head>
    
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  <title>Jianfei Guo | math and DL for <strong>shapes</strong>[as spatial surfaces]</title>
  <meta name="description" content="Jianfei Guo. ffventus (at) gmail.com. I am a learner in representation learning and decision making. 
">

  <link rel="shortcut icon" href="https://ventusff.github.io/assets/img/favicon.ico?v=1">

  <link rel="stylesheet" href="https://ventusff.github.io/assets/css/main.css">
  <link rel="canonical" href="https://ventusff.github.io/notes/neural_surface">
  

    
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams',
      inlineMath: [ ['$','$'], ["\\(","\\)"] ], // http://docs.mathjax.org/en/latest/input/tex/delimiters.html
      displayMath: [['\\[','\\]'], ['$$','$$']]
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    
<!-- Mermaid Js -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/mermaid/8.7.0/mermaid.min.js"></script>
<!-- <script src="https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js"></script> // this is the old version --> 

  </head>

  <body>

    <header class="site-header">

  <div class="wrapper">

    
    <span class="site-title">
        
        <strong>Jianfei</strong> Guo
    </span>
    

    <nav class="site-nav">
      <input type="checkbox" id="nav-trigger" class="nav-trigger">
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewbox="0 0 18 15" width="18px" height="15px">
              <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"></path>
              <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"></path>
              <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"></path>
            </svg>
          </span>
        </label>

      <div class="trigger">
        <!-- About -->
        <a class="page-link" href="https://ventusff.github.io/">about</a>

        <!-- Blog -->
        <a class="page-link" href="https://ventusff.github.io/blog/">blog</a>

        <!-- Pages -->
        
          
        
          
        
          
        
          
            <a class="page-link" href="https://ventusff.github.io/notes/">notes</a>
          
        
          
        
          
        
          
        

        <!-- CV link -->
        <!-- <a class="page-link" href="https://ventusff.github.io/assets/pdf/CV.pdf">vitae</a> -->

      </div>
    </nav>

  </div>

</header>



    <div class="page-content">
      <div class="wrapper">
        <div class="post">

  <header class="post-header">
    <h1 class="post-title">math and DL for <strong>shapes</strong>[as spatial surfaces]</h1>
    <p class="post-subtitle"><strong>形状</strong>[as空间曲面]有关的数学与DL类方法</p>
    <p class="post-meta"></p>
  </header>

  <article class="post-content">
    <ul id="markdown-toc">
  <li>
<a href="#math-implicit-surface" id="markdown-toc-math-implicit-surface">math: implicit surface</a>    <ul>
      <li><a href="#surface-implicit-form--parametric-form" id="markdown-toc-surface-implicit-form--parametric-form">surface: implicit form &amp; parametric form</a></li>
      <li><a href="#%E5%BD%A2%E7%8A%B6geometry-%E4%B8%8E-%E6%8B%93%E6%89%91topology" id="markdown-toc-形状geometry-与-拓扑topology">形状(geometry) 与 拓扑(topology)</a></li>
      <li><a href="#manifold%E6%B5%81%E5%BD%A2chart%E5%8D%A1%E5%9D%90%E6%A0%87%E5%8D%A1atlas%E5%9B%BE%E5%86%8C" id="markdown-toc-manifold流形chart卡坐标卡atlas图册"><code class="language-plaintext highlighter-rouge">manifold</code>流形，<code class="language-plaintext highlighter-rouge">chart</code>卡(坐标卡)，<code class="language-plaintext highlighter-rouge">atlas</code>图册</a></li>
      <li>
<a href="#algorithms" id="markdown-toc-algorithms">algorithms</a>        <ul>
          <li><a href="#marching-cubes-explain" id="markdown-toc-marching-cubes-explain">marching cubes [explain]</a></li>
          <li><a href="#losses" id="markdown-toc-losses">losses</a></li>
        </ul>
      </li>
      <li><a href="#implicit-form--implicit-field-%E4%B8%8E-parametric-form-%E4%B9%8B%E9%97%B4%E7%9A%84%E8%BD%AC%E6%8D%A2" id="markdown-toc-implicit-form--implicit-field-与-parametric-form-之间的转换">implicit form / implicit field 与 parametric form 之间的转换</a></li>
    </ul>
  </li>
  <li><a href="#representation-for-shapes" id="markdown-toc-representation-for-shapes">representation for shapes</a></li>
  <li>
<a href="#learning-parametric-surface" id="markdown-toc-learning-parametric-surface">learning parametric surface</a>    <ul>
      <li><a href="#explicit-shape-templates--deformation" id="markdown-toc-explicit-shape-templates--deformation">explicit shape templates + deformation</a></li>
      <li><a href="#continuous-patches" id="markdown-toc-continuous-patches">continuous patches</a></li>
    </ul>
  </li>
  <li>
<a href="#learning-implicit-surface-implicit-fieldsimplicit-functions" id="markdown-toc-learning-implicit-surface-implicit-fieldsimplicit-functions">learning implicit surface: implicit fields/implicit functions</a>    <ul>
      <li><a href="#sample-based-methods-to-extract-explicit-surface" id="markdown-toc-sample-based-methods-to-extract-explicit-surface">sample based methods to extract explicit surface</a></li>
      <li><a href="#initialization--priors-for-auto-decoders" id="markdown-toc-initialization--priors-for-auto-decoders">initialization / priors for auto-decoders</a></li>
      <li><a href="#differentiable-renderer" id="markdown-toc-differentiable-renderer">differentiable renderer</a></li>
      <li><a href="#compositional--multi-object-scene" id="markdown-toc-compositional--multi-object-scene">compositional / multi object scene</a></li>
      <li><a href="#analytic-exact-solution" id="markdown-toc-analytic-exact-solution">analytic exact solution</a></li>
    </ul>
  </li>
  <li><a href="#learning-parameterization--implicitization" id="markdown-toc-learning-parameterization--implicitization">learning parameterization / implicitization</a></li>
  <li><a href="#others" id="markdown-toc-others">others</a></li>
</ul>

<hr>

<h2 id="math-implicit-surface">math: implicit surface</h2>

<ul>
  <li>参考资料
    <ul>
      <li><a href="https://en.wikipedia.org/wiki/Implicit_surface">wiki: implicit surface</a></li>
      <li><a href="https://en.wikipedia.org/wiki/Atlas_(topology)#Charts">wiki: atlas</a></li>
      <li><a href="https://zhuanlan.zhihu.com/p/41563330">知乎: 光滑流形</a></li>
      <li>Clemson University - <a href="https://people.cs.clemson.edu/~dhouse/courses/405/">Computer Graphics 计算机图形学课程</a>
        <ul>
          <li>chapter 12 <a href="https://people.cs.clemson.edu/~dhouse/courses/405/notes/implicit-parametric.pdf">Implicit and Parametric Surfaces</a>
</li>
        </ul>
      </li>
      <li>JKU - <a href="https://www3.risc.jku.at/education/courses/ss2017/caag/">Commulative Algebra and Algebraic Geometry - 交换代数与代数几何课程</a>
        <ul>
          <li>chapter 7 <a href="https://www3.risc.jku.at/education/courses/ss2017/caag/07-local.pdf">Local properties of plane algebraic curves</a>
</li>
          <li>chapter 8 <a href="https://www3.risc.jku.at/education/courses/ss2017/caag/08-para.pdf">Rational Parametrization of Curves</a>
</li>
          <li>这则课程主要研究平面代数曲线，所有的定理、证明都十分严谨；大多数定理其实都可以延伸至空间超曲面</li>
        </ul>
      </li>
      <li>中科大 - <a href="http://staff.ustc.edu.cn/~msheng/references/moderna.pdf">近世代数</a>
</li>
      <li>[1990]Purdue University - <a href="http://graphics.stanford.edu/courses/cs348a-20-winter/Handouts/a228715.pdf">Conversion methods beween parametric and implicit curves and surfaces</a>
        <ul>
          <li>非常老的手稿影印版，过程比较简略</li>
        </ul>
      </li>
      <li>[2002]Purdue University - <a href="https://www.cs.purdue.edu/homes/cmh/distribution/books/geo.html">Geometric and Solid Modeling - 几何与实体造型课程</a>
        <ul>
          <li>chapter 5 <a href="https://www.cs.purdue.edu/homes/cmh/distribution/books/chap5.pdf">Representation of Curved Edges and Faces</a>
</li>
          <li>有些不严谨，跳步很多，需要一定代数几何基础</li>
        </ul>
      </li>
      <li>[2006]arXiv <a href="https://www.mn.uio.no/math/personer/vit/ragnip/monoids.pdf">monoid hypersurfaces</a>
</li>
      <li>[2020]Thibault GROUEIX’s slice  - <a href="http://imagine.enpc.fr/~langloip/data/DeepLearningFor3D_3.pdf">Deep Learning for 3D Toward Surface Generation</a>
        <ul>
          <li>非常详细，非常总结，概括性很好</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="surface-implicit-form--parametric-form">surface: implicit form &amp; parametric form</h3>

<details>
  <summary>Click to expand</summary>

  <ul>
    <li>一个球面形状的隐式形式和参数化形式：<code class="language-plaintext highlighter-rouge">implicit form</code> &amp; <code class="language-plaintext highlighter-rouge">parametric form</code><img src="media/image-20201207201258315.png" alt="image-20201207201258315">
</li>
    <li>implicit的形式无法直接通过其生成点，但是一般可以通过test来判断点在object内还是object外，对于ray-tracing非常友好
      <ul>
        <li><img src="media/image-20201207204020330.png" alt="image-20201207204020330"></li>
      </ul>
    </li>
    <li>parametric的形式可以直接通过其生成surface上的点，对于OpenGL等方法很有帮助
      <ul>
        <li><img src="media/image-20201207204043660.png" alt="image-20201207204043660"></li>
      </ul>
    </li>
  </ul>

</details>

<h3 id="形状geometry-与-拓扑topology">形状(geometry) 与 拓扑(topology)</h3>

<details>
  <summary>Click to expand</summary>

  <ul>
    <li>如果用mesh的数据结构来理解拓扑：
      <ul>
        <li>同形状代表相同的顶点位置和连接关系；同拓扑代表相同的顶点连接关系</li>
        <li>若顶点之间的连接关系不变，可以通过顶点位移变化出的几个形状，互相之间是同拓扑的
          <ul>
            <li>如甜甜圈和咖啡杯</li>
          </ul>
        </li>
        <li>拓扑不同的形状，只要顶点之间的连接关系保持不变，怎么位移顶点都无法得到</li>
        <li>当然，上述的“位移顶点位置”是一个粗糙的描述，具体在形变时是要符合一定规则的，即【<u>光滑同胚/微分同胚</u>】 [bilibili视频：<a href="https://www.bilibili.com/video/BV1k54y1R7J5">斯梅尔悖论：内翻球面和外翻球面是微分同胚的</a> ]</li>
      </ul>
    </li>
  </ul>
</details>

<h3 id="manifold流形chart卡坐标卡atlas图册">
<code class="language-plaintext highlighter-rouge">manifold</code>流形，<code class="language-plaintext highlighter-rouge">chart</code>卡(坐标卡)，<code class="language-plaintext highlighter-rouge">atlas</code>图册</h3>
<details>
  <summary>Click to expand</summary>

  <ul>
    <li>
<code class="language-plaintext highlighter-rouge">homeomorphism</code>同胚
      <ul>
        <li>同胚是两个<code class="language-plaintext highlighter-rouge">topological space</code>拓扑空间之间的函数</li>
        <li>a function \(f: X \rightarrow Y\) between two topological spaces is a homeomorphism if:
          <ul>
            <li>\(f\) is a <code class="language-plaintext highlighter-rouge">bijection</code>   (i.e. <code class="language-plaintext highlighter-rouge">one-to-one</code> and <code class="language-plaintext highlighter-rouge">onto</code>)
<br>\(f\)是一个双射，i.e.单射且满射</li>
            <li>\(f\) is a continuous function</li>
            <li>the inverse function \(f^{-1}\) is continuous</li>
          </ul>
        </li>
        <li>e.g. 咖啡杯和甜甜圈这两个拓扑空间同胚</li>
      </ul>
    </li>
    <li>
<code class="language-plaintext highlighter-rouge">manifold</code>流形， <code class="language-plaintext highlighter-rouge">chart</code>坐标卡，<code class="language-plaintext highlighter-rouge">parameterization</code>参数化
      <ul>
        <li>流形是一个拓扑空间</li>
        <li>
<code class="language-plaintext highlighter-rouge">2-manifold</code>(<code class="language-plaintext highlighter-rouge">two-dimensional manifold</code>)二维流形的定义：
          <ul>
            <li>a subset \(\mathcal{S}\) of \(\mathbb{R}^3\) is a 2-manifold if
              <ul>
                <li>for every point \(\boldsymbol{p} \in \mathcal{S}\)
<br>there is an open set \(V\) in \(\mathbb{R}^2\) and an open set \(W\) in \(\mathbb{R}^3\) containing \(\boldsymbol{p}\) <br> such that \(U=\mathcal{S} \cap W\) is homeomorphic to \(V\)
<br>对于 \(\mathcal{S}\)中的任意一个点 \(\boldsymbol{p}\) ，
<br>都存在\({[\mathbb{R}^2中的一个开集V]}_{欧式空间中的一个开子集}\)  和\({[\mathbb{R}^3中的包含点\boldsymbol{p}的一个开集W}]\) <br>
使得\({[\mathcal{S}和W的交集U]}_{\mathcal{S}的一个包含点\boldsymbol{p}的开子集}\)与\(V_{欧式空间的一个开子集}\)同胚</li>
                <li>这个同胚记为\(\varphi: U \rightarrow V\)，有序对 \((U,\varphi)\) 叫做包含\(p\)的坐标卡</li>
              </ul>
            </li>
          </ul>
        </li>
        <li>人话
          <ul>
            <li>\(S\)的一个开子集和欧式空间的一个开子集同胚，那么\(S\)就是一个流形</li>
            <li>从\(S\)的一个开子集到欧式空间的开子集的同胚叫做<code class="language-plaintext highlighter-rouge">chart</code>坐标卡</li>
            <li>坐标卡的逆(从低维欧式空间的开子集 到 \(S\)的一个开子集的同胚)叫做<code class="language-plaintext highlighter-rouge">parameterization</code>参数化</li>
          </ul>
        </li>
        <li>
<code class="language-plaintext highlighter-rouge">manifold</code>理解：局部区域线性，与(低维)欧式空间拓扑同胚</li>
        <li>“自由度”的理解：<br>一个m维空间的中的曲线/曲面有n个自由度，其实严格数学定义指的是这个曲面/曲线是一个n维流形，与某一个n维欧式空间(局部)同胚</li>
      </ul>
    </li>
    <li>
<code class="language-plaintext highlighter-rouge">chart</code>卡/坐标卡
      <ul>
        <li>坐标卡是一个同胚，一个函数，一个映射。</li>
        <li>A <code class="language-plaintext highlighter-rouge">chart</code> for a <code class="language-plaintext highlighter-rouge">topological space</code> <em>M</em> is a <code class="language-plaintext highlighter-rouge">homeomorphism</code> \(\varphi\) from an open subset <em>U</em> of <em>M</em> to an open subset of a Euclidean space.
<br>一个拓扑空间的坐标卡，就是这个拓扑空间的一个开子集到一个欧式空间的开子集的同胚</li>
        <li>the chart is traditionally recorded as the ordered pair \((U,\varphi)\) <br>坐标卡一般用有序对\((U,\varphi)\)表示</li>
      </ul>
    </li>
    <li>
<code class="language-plaintext highlighter-rouge">parameterization</code>参数化
      <ul>
        <li>参数化是一个同胚，一个函数，一个映射</li>
        <li>
<code class="language-plaintext highlighter-rouge">chart</code>坐标卡的逆映射就是参数化：从一个欧式空间的开子集到拓扑空间的开子集的同胚</li>
        <li>举例：
          <ul>
            <li>NeRF++中，显式手动建立了一种从欧式空间到球面坐标(4维，x,y,z定义方向，r定义球内球外)的映射，其本质就是一个从3维欧式空间到一个4维拓扑空间的同胚，一种参数化</li>
            <li>AtlasNet中，隐式地学出了一种从2维单位均匀分布到空间中一个物体表面局部patch的坐标的映射，其本质就是一个从2维欧式空间到一个3维拓扑空间(3维中的一个曲面)的同胚，一种参数化</li>
            <li>在代数几何/计算机视觉中，一个平面曲线的参数化，本质就是从一个1维欧式空间(参数的集合)到一个2维拓扑空间的映射；一个空间曲面的参数化，本质就是从一个2维欧式空间(参数的集合)到一个3维拓扑空间的映射；都是同胚，都是参数化</li>
          </ul>
        </li>
      </ul>
    </li>
    <li>
<code class="language-plaintext highlighter-rouge">image</code>像
      <ul>
        <li>像是一个点集。</li>
        <li>设\(f\)是一个从定义域\(X\)到值域\(Y\)的一个函数</li>
        <li>image of an element
If <em>x</em> is a member of <em>X</em>, then the image of <em>x</em> under <em>f</em>, denoted <em>f</em>(<em>x</em>), is the value of <em>f</em> when applied to <em>x.</em>
</li>
        <li>image of a subset
the image of subset \(A \subseteq X\) under <em>f</em>, denoted \(f[A]\) is the subset of <em>Y</em> which can be defined as:
<br>\(f[A] = \{f(x) \vert x \in A\}\)
<br>when there is no risk of confusion, \(f[A]\) is simply written as \(f(A)\)</li>
        <li>
<code class="language-plaintext highlighter-rouge">inverse image / preimage</code>原像：
<br>the preimage or inverse image of set \(B \subseteq Y\) under <em>f</em> , denoted by \(f^{-1}[B]\), is the subset of <em>X</em> defined by<br>
\(f^{-1}[B]=\{x\in X \vert f(x) \in B\}\)</li>
      </ul>
    </li>
    <li>
<code class="language-plaintext highlighter-rouge">atlas</code>图册
      <ul>
        <li>图册是一族坐标卡，一族同胚，一族函数，一族映射</li>
        <li>a index family \(\{(U_\alpha,\varphi_{\alpha}):\alpha \in I \}\) of charts on <em>M</em> which <code class="language-plaintext highlighter-rouge">covers</code> <em>M</em> (that is, \(\cup_{\alpha \in I} U_{\alpha}=M\))</li>
        <li>流形<em>M</em>上的一个图册是：
一族<em>M</em>上的卡\(\mathcal{A}=\{(U_{\alpha}, \varphi_{\alpha})\}\) ，使得定义域盖住了整个<em>M</em>
</li>
      </ul>
    </li>
    <li>
<code class="language-plaintext highlighter-rouge">disk-topology</code>圆盘拓扑
      <ul>
        <li>
<code class="language-plaintext highlighter-rouge">disk</code>, also spelled as <code class="language-plaintext highlighter-rouge">disc</code>
          <ul>
            <li>the region in a plane bounded by a circle</li>
            <li>在cartesian coordinates下的：<code class="language-plaintext highlighter-rouge">open disk</code><br>\(D=\{(x,y)\in \mathbb{R}^2: (x-a)^2+(y-b)^2&lt;R^2\}\)</li>
            <li>
<code class="language-plaintext highlighter-rouge">closed disk</code><br>\(D=\{(x,y)\in \mathbb{R}^2: (x-a)^2+(y-b)^2 \leq R^2\}\)</li>
          </ul>
        </li>
        <li>a surface <strong>homeomorphic</strong> to a disc in a plane</li>
      </ul>
    </li>
  </ul>

</details>

<h3 id="algorithms">algorithms</h3>

<h4 id="marching-cubes-explain">marching cubes <a href="http://www.cs.carleton.edu/cs_comps/0405/shape/marching_cubes.html">[explain]</a>
</h4>

<h4 id="losses">losses</h4>

<details>
  <summary>Click to expand</summary>

  <ul>
    <li>chamfer loss / <code class="language-plaintext highlighter-rouge">chamfer distance</code> (CD)</li>
  </ul>

  <table>
    <thead>
      <tr>
        <th> </th>
        <th> </th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><img src="media/image-20201208012017960.png" alt="image-20201208012017960"></td>
        <td><img src="media/image-20201208012035153.png" alt="image-20201208012035153"></td>
      </tr>
    </tbody>
  </table>

  <ul>
    <li>
<code class="language-plaintext highlighter-rouge">Earth Mover Distance</code> (EMD)</li>
    <li>
  </ul>

</details>

<h3 id="implicit-form--implicit-field-与-parametric-form-之间的转换">implicit form / implicit field 与 parametric form 之间的转换</h3>

<details>
  <summary>Click to expand</summary>

  <ul>
    <li>
<code class="language-plaintext highlighter-rouge">affine space </code>仿射空间</li>
    <li>
<code class="language-plaintext highlighter-rouge">projective space</code> 射影空间
      <ul>
        <li>射影空间是齐次坐标系</li>
      </ul>
    </li>
    <li>
<code class="language-plaintext highlighter-rouge">multiplicity</code> 重数</li>
    <li>
<code class="language-plaintext highlighter-rouge">rational function</code> 有理函数（多项式加减乘除，只在有限个点没有定义）</li>
    <li>
<code class="language-plaintext highlighter-rouge">monoid</code>, <code class="language-plaintext highlighter-rouge">monoidal</code> 幺半群 / 单位半群 / 具幺半群 / 独异点
      <ul>
        <li>
<strong>幺半群</strong>是一个带有二元运算 <em>: *M</em> × <em>M</em> → <em>M</em> 的集合 <em>M</em> ，其符合下列公理：
          <ul>
            <li>结合律：对任何在 <em>M</em> 内的<em>a</em>、<em>b</em>、<em>c</em> ， (<em>a</em>*<em>b</em>)*<em>c</em> = <em>a</em>*(<em>b</em>*<em>c</em>) 。</li>
            <li>单位元：存在一在 <em>M</em> 内的元素<em>e</em>，使得任一于 <em>M</em> 内的 <em>a</em> 都会符合 <em>a</em>*<em>e</em> = <em>e</em>*<em>a</em> = <em>a</em> 。</li>
          </ul>
        </li>
        <li>通常也会多加上另一个公理：
          <ul>
            <li>封闭性：对任何在 <em>M</em> 内的 <em>a</em> 、 <em>b</em> ， <em>a</em><em>*b</em> 也会在 <em>M</em> 内。</li>
            <li>但这不是必要的，因为在二元运算中即内含了此一公理。</li>
          </ul>
        </li>
        <li>幺半群除了没有<a href="https://zh.wikipedia.org/wiki/%E9%80%86%E5%85%83%E7%B4%A0">逆元素</a>之外，满足其他所有<a href="https://zh.wikipedia.org/wiki/%E7%BE%A4">群</a>的公理。因此，一个带有逆元素的幺半群和群是一样的。</li>
      </ul>
    </li>
    <li>
<code class="language-plaintext highlighter-rouge">monoidal surfaces</code> 独异点曲面
      <ul>
        <li>an algebraic(polynomial) surface \(f(x,y,z)=0\) of degree <em>n</em> that has an \((n-1)-fold\) point (a point of multiplicity n-1)<br>一个有n-1重点的n次代数曲面(线)即为一个monoidal curve</li>
        <li>monoidal surfaces include:
          <ul>
            <li>quadrics 二次曲面</li>
            <li>cubic surface with a double point 有二重点的三次曲面</li>
            <li>quartic surface with a triple point 有三重点的四次曲面</li>
            <li><em>etc.</em></li>
          </ul>
        </li>
      </ul>
    </li>
    <li>
<code class="language-plaintext highlighter-rouge">parameterization</code>: implicit -&gt; parametric
      <ul>
        <li>==本质==
          <ul>
            <li>这里的参数化，和拓扑学中的参数化，是一回事：从一个欧式空间到一个拓扑空间的同胚(映射)</li>
          </ul>
        </li>
        <li>curve
          <ul>
            <li>
<em>Noether’s theorem</em><br> A plane algebraic curve f(x,y)=0 possesses a rational paramtric form iff f has genus 0</li>
          </ul>
        </li>
        <li>surface
          <ul>
            <li>没有已知的通用工具来判断一个给定的implicit surface是否可以被参数化，以及if so, how</li>
          </ul>
        </li>
        <li>monoidal curves/ surfaces can be parameterized in a simple manner</li>
        <li>参数化时常用方式：parameterization using a <code class="language-plaintext highlighter-rouge">pencil</code> of lines
          <ul>
            <li>
<code class="language-plaintext highlighter-rouge">pencil</code>
              <ul>
                <li>in <a href="https://en.wikipedia.org/wiki/Geometry">geometry</a>, a <strong>pencil</strong> is a family of geometric objects with a common property</li>
                <li>a <em>pencil of lines</em> through a point <em>p</em> is a set of lines each containing <em>p</em>
</li>
              </ul>
            </li>
            <li>
<code class="language-plaintext highlighter-rouge">Bezout's Theorem</code> 贝组定理
              <ul>
                <li>Let \(\mathcal{C}\) and \(\mathcal{D}\) be projective plane curves without common components and degrees n and m, respectively. Then <br>\(n \cdot m = \underset{P \in \mathcal{C} \cap \mathcal{D}}{\sum} mult_P(\mathcal{C},\mathcal{D})\)</li>
                <li>即：在考虑重数设定的前提下，两个分别次数为n和m的仿射空间代数曲线(也可以是射影空间)，二者要么有共同项，要么没有共同项且相交mn次(相交点的重数和为mn)</li>
              </ul>
            </li>
            <li>因此，对于monoidal curves/surfaces来说，只要让a pencil of lines共同经过那个(n-1)重点，则这些直线一定与曲线/曲面还剩一个交点，如此便可实现参数化</li>
          </ul>
        </li>
      </ul>
    </li>
  </ul>

  <table>
    <thead>
      <tr>
        <th>让直线束经过二次曲线的一个”一重点”来参数化</th>
        <th>让直线束经过三次曲线的一个二重点来参数化</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><img src="media/image-20201209091156318.png" alt="image-20201209091156318"></td>
        <td><img src="media/image-20201209091136948.png" alt="image-20201209091136948"></td>
      </tr>
    </tbody>
  </table>

  <ul>
    <li>a <code class="language-plaintext highlighter-rouge">rational parameterization</code> of a surface in affine (x,y,z)-space corresponds to a <code class="language-plaintext highlighter-rouge">polynomial parameterization</code> of the same surface  in <code class="language-plaintext highlighter-rouge">projective (w,x,y,z)-space</code><br>一个曲面在(x,y,z)-仿射空间的有理参数化 对应 同样曲面在(w,x,y,z)-射影空间的多项式参数化</li>
    <li>
<code class="language-plaintext highlighter-rouge">implicitization</code>: parametric -&gt; implicit
      <ul>
        <li>all curves and surfaces with a rational parametric form can be converted to implicit form</li>
        <li>elimination algorithm, resultant, <em>etc.</em>
</li>
      </ul>
    </li>
  </ul>

</details>

<h2 id="representation-for-shapes">representation for shapes</h2>

<ul>
  <li>现有的形状表征模型：(a) voxel, (b) pointcloud, (c) mesh, (d) implicit field (occupancy, SDF, inside/outside, etc.) (e) parametric patches. <br>本篇笔记主要考虑implicit field与parametric patches.</li>
  <li><img src="media/image-20201203153023230.png" alt="image-20201203153023230"></li>
</ul>

<h2 id="learning-parametric-surface">learning parametric surface</h2>

<ul>
  <li>keyword
    <ul>
      <li>neural parametric surface</li>
      <li>parametric surface generation/generative</li>
    </ul>
  </li>
  <li>overview
    <ul>
      <li>用一个参数方程\([x(s,t),y(s,t),z(s,t)]\)表达一个曲面</li>
      <li>可以用显式的手动构建或者隐式的神经网络来构建这个从s,t到x,y,z的映射关系</li>
    </ul>
  </li>
</ul>

<h3 id="explicit-shape-templates--deformation">explicit shape templates + deformation</h3>

<hr>

<p><strong><code class="language-plaintext highlighter-rouge">"Learning Category-Specific Mesh Reconstruction from Image Collections"</code></strong><br>
<strong>[</strong> <code class="language-plaintext highlighter-rouge">ECCV2018</code> <strong>]</strong> <strong><a href="https://arxiv.org/pdf/1803.07549.pdf">[paper]</a></strong> <strong><a href="https://akanazawa.github.io/cmr/">[web]</a></strong> <strong><a href="https://github.com/akanazawa/cmr">[code]</a></strong> <strong>[</strong> <img class="emoji" title=":mortar_board:" alt=":mortar_board:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f393.png" height="20" width="20"> <code class="language-plaintext highlighter-rouge">UCB</code> <strong>]</strong> <br>
<strong>[</strong>  <code class="language-plaintext highlighter-rouge">Angjoo Kanazawa</code>, <code class="language-plaintext highlighter-rouge">Shubham Tulsiani</code>, <code class="language-plaintext highlighter-rouge">Alexei A. Efros</code>, <code class="language-plaintext highlighter-rouge">Jitendra Malik</code>  <strong>]</strong><br>
<strong>[</strong> <em><code class="language-plaintext highlighter-rouge">category-specific canonical shape template</code></em> <strong>]</strong></p>

<details>
  <summary>Click to expand</summary>

  <ul>
    <li>
<strong>Motivation</strong>
<img src="media/image-20201207225935008.png" alt="image-20201207225935008">
</li>
    <li>
<strong>Overview</strong>
<img src="media/image-20201207230015167.png" alt="image-20201207230015167">
      <ul>
        <li>一张图片encode到一个latent space, 被三个模块共享</li>
        <li>shape predictor，学到的是从mean shape出发的顶点的位移改变量</li>
        <li>texture predictor，学到的是从输入图像的texture flow</li>
        <li>camera predictor，学到的是canonical space下的camera pose</li>
      </ul>
    </li>
    <li>deformation predictor事实上学到的是从一个learned mean shape的变形
texture使用标准UV映射定义</li>
    <li>mesh定义在canonical frame下
  mean shape和sphere有相同的geometry
      <ul>
        <li>相同的顶点连接性，相当于fixed topology，拓扑是固定的
          <ul>
            <li>思考甜甜圈和咖啡杯的拓扑是一样的：通过顶点移位变形可以变形过去</li>
            <li>a fixed and pre-determined mesh connectivity 连接性是固定的</li>
          </ul>
        </li>
        <li>所谓shape predictor，其实是预测固定个数的vertices的位置改变<br>
<img src="media/image-20201207231029039.png" alt="image-20201207231029039">
</li>
        <li>我们可以从uv图的坐标映射到球面坐标，再映射到mean shape上的坐标，再通过shape 变形（顶点移位）映射到当前shape上的顶点坐标</li>
      </ul>
    </li>
    <li>texture predictor 事实上学到的是从单张图片出发的texture flow
<img src="media/image-20201207232213580.png" alt="image-20201207232213580">
</li>
  </ul>

</details>

<hr>

<p><strong><code class="language-plaintext highlighter-rouge">"Learning Shape Templates with Structured Implicit Functions"</code></strong><br>
<strong>[</strong> <code class="language-plaintext highlighter-rouge">ICCV2019</code> <strong>]</strong> <strong><a href="https://arxiv.org/pdf/1904.06447.pdf">[paper]</a></strong>  <strong>[</strong> <img class="emoji" title=":mortar_board:" alt=":mortar_board:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f393.png" height="20" width="20"> <code class="language-plaintext highlighter-rouge">Princeton</code> <strong>]</strong> <strong>[</strong> <img class="emoji" title=":office:" alt=":office:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f3e2.png" height="20" width="20"> <code class="language-plaintext highlighter-rouge">Google</code> <strong>]</strong><br>
<strong>[</strong>  <code class="language-plaintext highlighter-rouge">Kyle Genova</code>, <code class="language-plaintext highlighter-rouge">Forrester Cole</code>, <code class="language-plaintext highlighter-rouge">Daniel Vlasic</code>, <code class="language-plaintext highlighter-rouge">Aaron Sarna</code>,  <code class="language-plaintext highlighter-rouge">William T. Freeman</code>, <code class="language-plaintext highlighter-rouge">Thomas Funkhouser</code> <strong>]</strong><br>
<strong>[</strong> <em><code class="language-plaintext highlighter-rouge">general canonical shape template</code></em> <strong>]</strong></p>

<p>learning generalized templates comprised of elements</p>

<details>
  <summary>Click to expand</summary>

  <ul>
    <li>
<strong>Motivation</strong>
      <ul>
        <li>给这类从canonical space下的shape template学出物体shape的方法，提供一种更通用于各种类别的shape template 学习方法</li>
        <li>由于现实世界的形状和拓扑变化丰富，过去的_<u>这类</u>_方法一般用a library of handmade templates</li>
        <li>本篇使用了一种基于若干个local shape elements的组合来构成shape template；<br>
每个element是一个隐式的surface representation
          <ul>
            <li>每个element可以当做一个高斯椭球形状</li>
            <li>这样，不同的elements位置、扁圆、大小组合，就可以组合出==<u>不同形状、不同拓扑</u>==的shape template</li>
          </ul>
        </li>
        <li>使用10，25，100个不同的elements训练的效果<br><img src="media/image-20201207235340273.png" alt="image-20201207235340273">
</li>
      </ul>
    </li>
    <li>隐式的shape表征：
      <ul>
        <li>假定每一个input shape都可以建模为一个watertight surface，由一个函数的 \(\mathcal{l}\) level set描述（l-等值面集）；</li>
        <li>这个函数可以由N个local elements构成</li>
        <li>每个elements是一个 <em>scaled axis-aligned anisotropic 3D Gaussians</em> 
<br>由参数\(\theta_i\)描述，\(\theta_i\)包含\(c_i, p_i \in \mathbb{R}^3, r_i \in \mathbb{R}^3\)
<br><img src="media/image-20201208000148898.png" alt="image-20201208000148898">
</li>
      </ul>
    </li>
  </ul>

</details>

<hr>

<p><strong><code class="language-plaintext highlighter-rouge">"Deep Mesh Reconstruction from Single RGB Images via Topology Modification Networks"</code></strong><br>
<strong>[</strong> <code class="language-plaintext highlighter-rouge">CVPR2019</code> <strong>]</strong> <strong><a href="https://arxiv.org/pdf/1909.00321.pdf">[paper]</a></strong>  <strong>[</strong> <img class="emoji" title=":mortar_board:" alt=":mortar_board:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f393.png" height="20" width="20"> <code class="language-plaintext highlighter-rouge">CUHK(Shenzhen)</code>, <code class="language-plaintext highlighter-rouge">USC</code> <strong>]</strong> <br>
<strong>[</strong>  <code class="language-plaintext highlighter-rouge">Junyi Pan</code>, <code class="language-plaintext highlighter-rouge">Xiaoguang Han</code>, <code class="language-plaintext highlighter-rouge">Weikai Chen</code>, <code class="language-plaintext highlighter-rouge">Jiapeng Tang</code>, <code class="language-plaintext highlighter-rouge">Kui Jia</code>  <strong>]</strong><br>
<strong>[</strong> <em><code class="language-plaintext highlighter-rouge">topology modification</code></em> <strong>]</strong></p>

<details>
  <summary>Click to expand</summary>

  <ul>
    <li>
<strong>Motivation</strong>
      <ul>
        <li>
<br><img src="media/image-20201208110645619.png" alt="image-20201208110645619">
</li>
        <li>优化的时候，可以alternates between shape deformation和topology modification</li>
      </ul>
    </li>
    <li>
<strong>overview</strong>
      <ul>
        <li>topology modification
          <ul>
            <li>通过动态地修改 faces-to-vertices关系来实现</li>
            <li>学一个per face error estimation network</li>
            <li>通过去掉那些deviate significantly的face来更新topology structure</li>
          </ul>
        </li>
      </ul>
    </li>
    <li>效果
      <ul>
        <li><img src="media/image-20201208111115100.png" alt="image-20201208111115100"></li>
        <li><img src="media/image-20201208111142570.png" alt="image-20201208111142570"></li>
      </ul>
    </li>
  </ul>

</details>

<h3 id="continuous-patches">continuous patches</h3>

<hr>

<p><strong><code class="language-plaintext highlighter-rouge">"AtlasNet: A Papier-Mâché Approach to Learning 3D Surface Generation"</code></strong><br>
<strong>[</strong> <code class="language-plaintext highlighter-rouge">CVPR2018</code> <strong>]</strong> <strong><a href="https://arxiv.org/pdf/1802.05384.pdf">[paper]</a></strong> <strong><a href="http://imagine.enpc.fr/~groueixt/atlasnet/">[web]</a></strong> <strong><a href="https://github.com/ThibaultGROUEIX/AtlasNet">[code]</a></strong> <strong><a href="https://github.com/ThibaultGROUEIX/AtlasNet/tree/V2.2">[code-easy-to-understand]</a></strong> <strong>[</strong> <img class="emoji" title=":mortar_board:" alt=":mortar_board:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f393.png" height="20" width="20"> <code class="language-plaintext highlighter-rouge">University</code> <strong>]</strong> <strong>[</strong> <img class="emoji" title=":office:" alt=":office:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f3e2.png" height="20" width="20"> <code class="language-plaintext highlighter-rouge">Adobe</code> <strong>]</strong><br>
<strong>[</strong>  <code class="language-plaintext highlighter-rouge">Thibault Groueix</code>  <strong>]</strong><br>
<strong>[</strong> <em><code class="language-plaintext highlighter-rouge">continous 2D patches</code>, <code class="language-plaintext highlighter-rouge">learning 2-manifold parameterization</code>, <code class="language-plaintext highlighter-rouge">2-manifold generation</code></em> <strong>]</strong></p>

<details>
  <summary>Click to expand</summary>

  <ul>
    <li><img src="media/image-20201208004500075.png" alt="image-20201208004500075"></li>
    <li>
<strong>Motivation</strong>
      <ul>
        <li>represents a surface as a collection of parametric surface elements
<br>把一个表面表征为一组parametric surface元素的集合</li>
        <li>学到的一族从单位方到局部 2-流形的映射，非常类似一个surface 的 atlas 图册</li>
        <li>每一个3D点最终都可以得到一个2D UV值</li>
      </ul>
    </li>
    <li>
<strong>overview</strong>
      <ul>
        <li><img src="media/image-20201208004950236.png" alt="image-20201208004950236"></li>
        <li>pointcloud基线，是把一个latent shape code输出为一组点</li>
        <li>本篇方法，额外输入一个从均匀单位方内采样的2D坐标点，用其来产生surface上的一个single point
          <ul>
            <li>从点云/数据中学出这种<code class="language-plaintext highlighter-rouge">2-manifold</code>（i.e. <a href="https://www2.cs.duke.edu/courses/fall06/cps296.1/Lectures/sec-II-1.pdf">two-dimensional manifolds</a>，二维流形）的parameterization</li>
            <li>属于parametric approaches 分支</li>
            <li>==<strong><u>这里本质上就是一个从二维均匀分布到空间二维流形分布的映射，condition on一个shape code</u></strong>==</li>
          </ul>
        </li>
        <li>很容易扩展多次，来把一个3D shape表征为几个surface 元素的联合</li>
      </ul>
    </li>
    <li>局部参数化表面的生成 locally parameterized surface generation
      <ul>
        <li>把surface看做一个广义的2-manifold（允许self-intersection &amp; disjoint sets），考虑局部的参数化<br>
consider a <code class="language-plaintext highlighter-rouge">2-manifold</code> \(\mathcal{S}\), a point \(\boldsymbol{p} \in \mathcal{S}\), a <code class="language-plaintext highlighter-rouge">parameterization</code> \(\varphi\) of \(\mathcal{S}\) in a local neighborhood of \(\boldsymbol{p}\)</li>
        <li>假定这个局部参数化就是从单位方 \(]0,1[^2\) 到2-manifold \(\mathcal{S}_{\theta}\)的映射 \(\varphi_{\theta}(x)\) : \(\mathcal{S}_\theta=\varphi_{\theta}(]0,1[^2)\)
 <br>让\(\mathcal{S}_{\theta}\)去估计/近似局部2-manifold \(S_{loc}\)</li>
        <li>i.e.寻找 参数\(\theta\)来最小化目标函数\(\underset{\theta}{\min}   \mathcal{L}(\mathcal{S}_\theta, \mathcal{S}_{loc})+\lambda\mathcal{R}(\theta)\)
<br>上式的\(\mathcal{L}\)是两个2-manifold之间的loss，\(\mathcal{R}\)是参数\(\theta\)的正则化项；
<br>实践中，计算的不是两个2-manifold之间的loss，<u>而是这两个2-manifold采样出的点集的chamfer 和 earth-mover距离</u>
</li>
        <li>证明了MLP+ReLU就可以产生2-manifolds</li>
        <li>证明了MLP+ReLU产生的2-manifolds can be learned to 很好地近似 target 2-manifolds
<br>用了universal representation theorum：<br>
Approximation capabilities of multilayer feedforward networks. <em>Neural Networks</em>, 1991</li>
      </ul>
    </li>
    <li>related work:  learning representations for 2-manifolds
      <ul>
        <li>polygon mesh</li>
        <li>建立一套3D shape和2D domain之间的连接是几何处理的一个存在已久的问题，它的应用有：texture mapping, re-meshing, shape correspondance</li>
        <li>过去的方法需要input data就是parameterized；本篇直接从点云中学出这种parameterization</li>
      </ul>
    </li>
  </ul>

</details>

<hr>

<p><strong><code class="language-plaintext highlighter-rouge">"Deep Geometric Prior for Surface Reconstruction"</code></strong><br>
<strong>[</strong> <code class="language-plaintext highlighter-rouge">CVPR2019</code> <strong>]</strong> <strong><a href="https://arxiv.org/pdf/1811.10943.pdf">[paper]</a></strong> <strong><a href="https://github.com/fwilliams/deep-geometric-prior">[code]</a></strong>  <strong>[</strong> <img class="emoji" title=":mortar_board:" alt=":mortar_board:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f393.png" height="20" width="20"> <code class="language-plaintext highlighter-rouge">New York University</code> <strong>]</strong> <br>
<strong>[</strong>  <code class="language-plaintext highlighter-rouge">Francis Williams</code>, <code class="language-plaintext highlighter-rouge">Teseo Schneider</code>, <code class="language-plaintext highlighter-rouge">Claudio Silva</code>, <code class="language-plaintext highlighter-rouge">Denis Zorin</code>, <code class="language-plaintext highlighter-rouge">Joan Bruna</code>, <code class="language-plaintext highlighter-rouge">Daniele Panozzo</code>  <strong>]</strong><br>
<strong>[</strong> <em><code class="language-plaintext highlighter-rouge">chart representation</code>, <code class="language-plaintext highlighter-rouge">auto-decoder</code></em> <strong>]</strong></p>

<details>
  <summary>Click to expand</summary>

  <ul>
    <li>
<strong>Motivation</strong>
      <ul>
        <li>首先把输入点云分成若干个重叠的部分，然后用MLP流形学习每个部分；</li>
        <li>每个local流形学习用<code class="language-plaintext highlighter-rouge">2-Wasserstein loss</code> / <code class="language-plaintext highlighter-rouge">EMD loss</code>；<br>并在所有流形之间保证consistency</li>
        <li><img src="media/image-20201228171157982.png" alt="image-20201228171157982"></li>
      </ul>
    </li>
    <li>
<strong>results</strong>
      <ul>
        <li><img src="media/image-20201228174443654.png" alt="image-20201228174443654"></li>
      </ul>
    </li>
  </ul>

</details>

<hr>

<p><strong><code class="language-plaintext highlighter-rouge">"Pix2Surf: Learning Parametric 3D Surface Models of Objects from Images"</code></strong><br>
<strong>[</strong> <code class="language-plaintext highlighter-rouge">ECCV2020</code> <strong>]</strong> <strong><a href="https://arxiv.org/pdf/2008.07760.pdf">[paper]</a></strong> <strong><a href="https://geometry.stanford.edu/projects/pix2surf/pub/pix2surf_supp.pdf">[supp]</a></strong> <strong><a href="https://geometry.stanford.edu/projects/pix2surf/">[web]</a></strong> <strong><a href="https://github.com/JiahuiLei/Pix2Surf">[code(trained)]</a></strong> <strong>[</strong> <img class="emoji" title=":mortar_board:" alt=":mortar_board:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f393.png" height="20" width="20"> <code class="language-plaintext highlighter-rouge">Zhejiang University</code>, <code class="language-plaintext highlighter-rouge">Stanford</code>, <code class="language-plaintext highlighter-rouge">UCL</code> <strong>]</strong> <strong>[</strong> <img class="emoji" title=":office:" alt=":office:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f3e2.png" height="20" width="20"> <code class="language-plaintext highlighter-rouge">Adobe</code> <strong>]</strong><br>
<strong>[</strong>  <code class="language-plaintext highlighter-rouge">Jiahui Lei</code>, <code class="language-plaintext highlighter-rouge">Srinath Sridhar</code>, <code class="language-plaintext highlighter-rouge">Niloy Mitra</code>, <code class="language-plaintext highlighter-rouge">Leonidas J. Guibas</code>  <strong>]</strong><br>
<strong>[</strong> <em><code class="language-plaintext highlighter-rouge">parametric 3D shape/parameterization</code>, <code class="language-plaintext highlighter-rouge">3D reconstruction</code>, <code class="language-plaintext highlighter-rouge">multi-view</code>, <code class="language-plaintext highlighter-rouge">single-view</code>, <code class="language-plaintext highlighter-rouge">surface reconstruction in NOCS</code></em> <strong>]</strong></p>

<details>
  <summary>Click to expand</summary>

  <ul>
    <li>
<strong>Result</strong>
      <ul>
        <li>评价：可以看到学出来的曲面可以不是闭合的</li>
        <li>
<img src="media/image-20201207204146033.png" alt="image-20201207204146033">
<img src="media/image-20201207204206853.png" alt="image-20201207204206853">
</li>
      </ul>
    </li>
    <li>
<strong>Motivation</strong>
      <ul>
        <li>learning to generate 3D parametric surface representations for novel object instances, as seen from one or more views</li>
        <li>使用2D patch来作为UV parameterization，处理多个non-adjacent views，并且建立2D pixels和3D surface points之间的correspondence</li>
        <li>那些用implicit functions表达的surface，想要得到显式的表面，需要昂贵的后处理步骤：如Marching Cubes；本文直接学习生成显式的表面</li>
      </ul>
    </li>
    <li>
<strong>主要贡献</strong>
      <ul>
        <li>high-quality parametric surfaces 遵循multi view一致性</li>
        <li>生成的3D表面保留了精确的图像像素到3D表面点的correspondance，使得可以lift texture information去reconstruct 带有丰富集合与外观的 shapes</li>
      </ul>
    </li>
    <li>
<strong>引用的directly reconstruct a parametric representation of a shape’s surface</strong>
      <ul>
        <li>class-specific templates  <strong><u>(canonical template / mean shape in canonical space)</u></strong>
<br>逐个类别手动设计的shape template
          <ul>
            <li>[ECCV2018] Learning category-specific mesh reconstruction from image collections.</li>
            <li>[ICCV2019] Canonical surface mapping via geometric cycle consistency</li>
          </ul>
        </li>
        <li>general structured templates
<br>适用于各种类别的通用shape template学习方法（应对不同的形状、拓扑）
          <ul>
            <li>[ICCV2019] Learning shape templates with structured implicit functions.</li>
          </ul>
        </li>
        <li>more generic surface representations
          <ul>
            <li>meshes deform
              <ul>
                <li>[ECCV2018] Pixel2mesh: Generating 3d mesh models from single rgb images.</li>
                <li>[ICCV2019] Pixel2mesh++: Multi-view 3d mesh generation via deformation</li>
                <li>[CVPR2019] 3DN: 3d deformation network.</li>
              </ul>
            </li>
            <li>differentiable mesh renderer + image supervision
              <ul>
                <li>[CVPR2018] Neural 3d mesh renderer</li>
                <li>[2019]  Soft rasterizer: A differentiable renderer for image-based 3d reasoning</li>
                <li>[2019] Pix2vex: Image-togeometry reconstruction using a smooth differentiable renderer.</li>
                <li>[CVPR2019] Learning view priors for single-view 3d reconstruction.</li>
              </ul>
            </li>
            <li>==continuous 2D patches== 本篇类似：使用2D patch来作为UV parameterization
              <ul>
                <li>[CVPR2018] Atlasnet: A papier-mâché approach to learning 3d surface generation.</li>
                <li>AtlasNet for video clip <br>[CVPR2019] Photometric mesh optimization for video-aligned 3d object reconstruction.</li>
                <li>introduce topology modification to atlasnet <br>[ICCV2019] Deep mesh reconstruction from single rgb images via topology modification networks</li>
              </ul>
            </li>
          </ul>
        </li>
      </ul>
    </li>
    <li>
<strong>preliminaries</strong>
      <ul>
        <li>NOCS
          <ul>
            <li>可以预测出一张图片的nocs map和mask</li>
          </ul>
        </li>
        <li>surface parameterization
          <ul>
            <li>表面的UV参数化即一个<code class="language-plaintext highlighter-rouge">chart</code>
</li>
            <li>用一组全连接网络学习多个<code class="language-plaintext highlighter-rouge">chart</code>
</li>
          </ul>
        </li>
      </ul>
    </li>
    <li>overview
      <ul>
        <li>==注意==：不同于atlas net，uv不是来自于均匀采样，而是来自于一个learned network，uv predictor<br>所以是先预测出图像每个像素的uv值，再把图像上属于这个物体的uv值集合和图像的feature 拼接一起来 输出 三维点集合(二维流形的三维点坐标集)<br>
</li>
        <li>
          <pre><code class="language-mermaid">graph LR
	img[image coordinate] -.per index prediction.-&gt; uv[uv value] --&gt; MLP
	image --&gt; z[global latent code z] --&gt; MLP
	MLP --&gt; 3d[3D surface coordinate]
</code></pre>
        </li>
        <li>
<br><img src="media/image-20201208103708582.png" alt="image-20201208103708582">
</li>
      </ul>
    </li>
    <li>single view single chart pix2surf
      <ul>
        <li>NOCS-UV branch
          <ul>
            <li>在过去的NOCS输出上额外加两个channel，输出uv值</li>
            <li>uv不是均匀采样来的，而是直接从图像预测出一张2-channel uv image <br><img src="media/image-20201208101930111.png" alt="image-20201208101930111">
</li>
            <li>发现可以emergence of a chart，并且这个chart几乎已经multi view consistent，multi object consistent
              <ul>
                <li>即网络可以自己学出来如何把一个物体shape unrap到一个flat 空间</li>
              </ul>
            </li>
            <li>code-extractor 一个小CNN
              <ul>
                <li>单张图片输入，输出一个global latent code z</li>
              </ul>
            </li>
            <li>UV amplifier
              <ul>
                <li>因为UV坐标只有2维，而global latent code z维度很大，这两个信息不平衡</li>
                <li>所以就是用一组MLP先把UV升维</li>
              </ul>
            </li>
          </ul>
        </li>
        <li>SP(surface parameterization) branch
          <ul>
            <li>类似atlas net，以升维后的UV和global latent code的拼接为输入，输出三维点坐标</li>
            <li>与atlas net的不同：
              <ul>
                <li>uv升维了</li>
                <li>有一个learned chart，建立起图像坐标和3D surface坐标的直接相关</li>
                <li>uv不是来自于均匀采样，而是从一个网络学出来的（即上面的NOCS-UV branch）</li>
              </ul>
            </li>
            <li>输出的三维点坐标位于NOCS空间</li>
          </ul>
        </li>
        <li>loss / train
          <ul>
            <li>NOCS map的真值</li>
            <li>3D surface point的真值（从shapenet 3d model直接得到）</li>
            <li>其余都是端到端的</li>
          </ul>
        </li>
      </ul>
    </li>
    <li>multi view atlas pix2surf
      <ul>
        <li>不同view的latent code取max pooling，max pooled code和该view的code concat在一起</li>
        <li>从一个view的pixel的NOCS map的真值，找到这个真值在另一个view下的绝对对应pixel位置<br>最小化这两个pixel预测出的3D 点距离，即为所定义的multi view consistency loss<br><img src="media/image-20201208105212477.png" alt="image-20201208105212477">
</li>
      </ul>
    </li>
  </ul>

</details>

<hr>

<p><strong><code class="language-plaintext highlighter-rouge">"Meshlet Priors for 3D Mesh Reconstruction"</code></strong><br>
<strong>[</strong> <code class="language-plaintext highlighter-rouge">CVPR2020</code> <strong>]</strong> <strong><a href="https://arxiv.org/pdf/2001.01744.pdf">[paper]</a></strong> <strong><a href="https://openaccess.thecvf.com/content_CVPR_2020/supplemental/Badki_Meshlet_Priors_for_CVPR_2020_supplemental.pdf">[supp]</a></strong> <strong><a href="https://github.com/NVlabs/meshlets">[code]</a></strong>   <strong>[</strong> <img class="emoji" title=":mortar_board:" alt=":mortar_board:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f393.png" height="20" width="20"> <code class="language-plaintext highlighter-rouge">UCSB</code> <strong>]</strong> <strong>[</strong> <img class="emoji" title=":office:" alt=":office:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f3e2.png" height="20" width="20"> <code class="language-plaintext highlighter-rouge">NVIDIA</code> <strong>]</strong><br>
<strong>[</strong>  <code class="language-plaintext highlighter-rouge">Abhishek Badki</code>, <code class="language-plaintext highlighter-rouge">Orazio Gallo</code>, <code class="language-plaintext highlighter-rouge">Jan Kautz</code>, <code class="language-plaintext highlighter-rouge">Pradeep Sen</code>  <strong>]</strong><br>
<strong>[</strong> <em><code class="language-plaintext highlighter-rouge">local shape prior</code>, <code class="language-plaintext highlighter-rouge">geodesic parameterization</code>, <code class="language-plaintext highlighter-rouge">VAE</code></em> <strong>]</strong></p>

<details>
  <summary>Click to expand</summary>

  <ul>
    <li>
<strong>Motivation</strong>
      <ul>
        <li>输入点云，输出mesh</li>
        <li>过去的学习shape的方法，在学习先验时有两种：
          <ul>
            <li>object级别的先验，没有和pose解耦；</li>
            <li>smooth regularizer先验，会损失local detail</li>
          </ul>
        </li>
        <li>本篇想学习的是那些处于canonical pose下的local natural meshlets，用local natural meshlets，这种meshlets在不同物体、不同类别之间完全是shared，然后用这样纯粹的局部先验来拼出一个完整mesh</li>
      </ul>
    </li>
  </ul>

  <table>
    <thead>
      <tr>
        <th> </th>
        <th> </th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><img src="media/image-20201216162000056.png" alt="image-20201216162000056"></td>
        <td>P指的是测试时的物体在数据集物体pose分布内，红P指不在数据集pose分布内<br>N指的是低噪声，红N指moderate noise<br>T指训练集见过的物体类别，T指训练集没有见过的物体类别<br>可以看到，本篇重点强调学出那些和pose解耦了的局部的meshlets，用这些meshlets来拼出完整mesh</td>
      </tr>
    </tbody>
  </table>

  <ul>
    <li>
<strong>geodesic parameterization</strong>
      <ul>
        <li><em>Geodesic polar coordinates on polygonal meshes.</em></li>
        <li>把一个顶点和周围的点映射到这个顶点的切平面的坐标上；然后把切平面通过变换变换到canonical pose（即顶点位移到坐标原点，切平面的法向量即z轴，切平面的u,v轴和x,y轴重合）</li>
        <li>这样，可以实现pose解耦，学到那些各种各样的局部的meshlets</li>
        <li><img src="media/image-20201216163007426.png" alt="image-20201216163007426"></li>
      </ul>
    </li>
    <li>
<strong>VAE</strong>
      <ul>
        <li>用VAE把各种meshlets压缩到一个latent space</li>
        <li>然后应用它fit一个点云集合的时候，首先用encoder提取一个初始的latent code，然后auto-decoder来更新几步latent code</li>
        <li><img src="media/image-20201216162658302.png" alt="image-20201216162658302"></li>
      </ul>
    </li>
    <li>
<strong>overall optimization</strong>
      <ul>
        <li>首先随便初始化一个rough mesh，从这个rough mesh提取meshlets，保证每个vertex至少被3个meshlets cover
          <ul>
            <li>注意，这样训练时就有两个量要迭代优化更新：一个是mesh，一个是一组meshlets；</li>
            <li>其中，每个meshlets由顶点和形状code构成</li>
          </ul>
        </li>
        <li>迭代：更新每一个局部的local shape
          <ul>
            <li>用point cloud和meshlets“拼成的mesh”的loss来更新每一个meshlet的形状</li>
          </ul>
        </li>
        <li>迭代：再让local shape形成global consistency
          <ul>
            <li>最小化更新后的meshlets的形状和“拼成的mesh”的误差</li>
            <li>首先固定meshlets的形状code，更新mesh顶点</li>
            <li>然后固定mesh顶点，更新meshlet的形状code</li>
          </ul>
        </li>
        <li><img src="media/image-20201216163105638.png" alt="image-20201216163105638"></li>
      </ul>
    </li>
  </ul>

</details>

<hr>

<p><strong><code class="language-plaintext highlighter-rouge">"Shape Reconstruction by Learning Differentiable Surface Representations"</code></strong><br>
<strong>[</strong> <code class="language-plaintext highlighter-rouge">CVPR2020</code> <strong>]</strong> <strong><a href="https://arxiv.org/pdf/1911.11227">[paper]</a></strong> <strong><a href="https://github.com/bednarikjan/differential_surface_representation">[code]</a></strong> <strong><a href="https://www.youtube.com/watch?v=6xEIDvCk1wA">[video]</a></strong> <strong>[</strong> <img class="emoji" title=":mortar_board:" alt=":mortar_board:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f393.png" height="20" width="20"> <code class="language-plaintext highlighter-rouge">EPFL</code> <strong>]</strong> <br>
<strong>[</strong>  <code class="language-plaintext highlighter-rouge">Jan Bednarik</code>, <code class="language-plaintext highlighter-rouge">Shaifali Parashar</code>, <code class="language-plaintext highlighter-rouge">Erhan Gundogdu</code>, <code class="language-plaintext highlighter-rouge">Mathieu Salzmann</code>, <code class="language-plaintext highlighter-rouge">Pascal Fua</code>  <strong>]</strong><br>
<strong>[</strong> <em><code class="language-plaintext highlighter-rouge">patch</code>, <code class="language-plaintext highlighter-rouge">control over patches</code>, <code class="language-plaintext highlighter-rouge">overlap</code>, <code class="language-plaintext highlighter-rouge">collapse</code>, <code class="language-plaintext highlighter-rouge">differential surface properties</code></em> <strong>]</strong></p>

<details>
  <summary>Click to expand</summary>

  <ul>
    <li>
<strong>Motivation</strong>
      <ul>
        <li>目前有一些学习an ensumble of Parametric表征的方法
          <ul>
            <li>但是这些方法并没有控制表面patch的变形，因此并不能阻止patches彼此重叠或者折叠成一个点、一条线</li>
            <li>这种情况下，计算表面法向量就会变得困难、不可靠</li>
          </ul>
        </li>
        <li>本篇提出 在训练时，开发深度神经网络的天生的可微性
          <ul>
            <li>来利用表面的微分属性去阻止patch折叠、显著减少互相重叠</li>
            <li>并且这让我们可以可靠地计算表面法向量、曲率等</li>
          </ul>
        </li>
        <li><img src="media/image-20201224164231425.png" alt="image-20201224164231425"></li>
      </ul>
    </li>
    <li>
<strong>related works</strong>: 在训练时使用differential surface properties
      <ul>
        <li><em>Learning to Reconstruct Texture-Less Deformable Surfaces. 3DV2018</em></li>
        <li><em>Marr Revisited: 2D-3D Model Alignment via Surface Normal Prediction. CVPR2016</em></li>
        <li><em>A Two-Stream Network for Fast and Accurate 3D Cloth Draping. ICCV2019</em></li>
      </ul>
    </li>
    <li>
<strong>overview</strong>
      <ul>
        <li><img src="media/image-20201224193837533.png" alt="image-20201224193837533"></li>
      </ul>
    </li>
    <li>
<strong>results</strong>
      <ul>
        <li>主要对比基线就是atlasNet</li>
        <li>Pointcloud Autoencoding (PCAE)<br><img src="media/image-20201224175724685.png" alt="image-20201224175724685">
</li>
        <li>single view reconstruction (SVR) 单目重建<br><img src="media/image-20201224175853915.png" alt="image-20201224175853915">
</li>
      </ul>
    </li>
  </ul>

</details>

<hr>

<p><strong><code class="language-plaintext highlighter-rouge">"Better Patch Stitching for Parametric Surface Reconstruction"</code></strong><br>
<strong>[</strong> <code class="language-plaintext highlighter-rouge">arXiv2020</code> <strong>]</strong> <strong><a href="https://arxiv.org/pdf/2010.07021.pdf">[paper]</a></strong> <strong>[</strong> <img class="emoji" title=":mortar_board:" alt=":mortar_board:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f393.png" height="20" width="20"> <code class="language-plaintext highlighter-rouge">EPFL</code> <strong>]</strong> <br>
<strong>[</strong>  <code class="language-plaintext highlighter-rouge">Zhantao Deng</code>, <code class="language-plaintext highlighter-rouge">Jan Bednařík</code>, <code class="language-plaintext highlighter-rouge">Mathieu Salzmann</code>, <code class="language-plaintext highlighter-rouge">Pascal Fua</code>  <strong>]</strong><br>
<strong>[</strong> <em><code class="language-plaintext highlighter-rouge">patch stitching</code>, <code class="language-plaintext highlighter-rouge">atlas</code>, <code class="language-plaintext highlighter-rouge">learning</code></em> <strong>]</strong></p>

<details>
  <summary>Click to expand</summary>

  <ul>
    <li>
<strong>Motivation</strong>
      <ul>
        <li>对目前的multiple patch based parametric surface representations（atlas），改进patches的<code class="language-plaintext highlighter-rouge">global consistency</code>（即防止<strong><u>孔洞</u></strong>和多个patch不正确<strong><u>交叉</u></strong>“jagged/带<strong><u>锯齿</u></strong>的”的情况）</li>
        <li><img src="media/image-20201224174834579.png" alt="image-20201224174834579"></li>
        <li>典型的缝合问题（1D表示）<br><img src="media/image-20201224175209265.png" alt="image-20201224175209265">
</li>
      </ul>
    </li>
    <li>
<strong>Related works：patch-wise representations</strong>
      <ul>
        <li>FoldingNet <em>Foldingnet: Point Cloud Auto-Encoder via Deep Grid Deformation.CVPR2018</em><br>第一个基于深度神经网络的工作：学到一个参数化的函数来在3D空间中嵌入一个2D流形</li>
        <li>后面的工作shifted to ensembles of such learned functions来做patch-wise表征：
          <ul>
            <li>learning (encoder)
              <ul>
                <li><em>Atlasnet: A papier-mâché approach to learning 3d surface generation. CVPR2018</em></li>
                <li><em>Learning elementary structures for 3d shape generation and matching. NeurIPS2019</em></li>
                <li>
<em>Shape reconstruction by learning differentiable surface representations. CVPR2020</em> 这是作者的前作，用正则化来减轻表面的扭曲、重叠</li>
                <li><em>Tearingnet: Point cloud autoencoder to learn topology-friendly representations. arXiv, 2020.</em></li>
              </ul>
            </li>
            <li>optimization (auto-decoder)
              <ul>
                <li><em>Deep geometric prior for surface reconstruction. CVPR2019</em></li>
                <li><em>Meshlet priors for 3d mesh reconstruction. CVPR2020</em></li>
              </ul>
            </li>
            <li>2D output domain
              <ul>
                <li><em>Deep parametric shape predictions using distance fields. CVPR2020</em></li>
              </ul>
            </li>
            <li>因为连续的patch可以以任意精度采样，因此在拟合的时候可以有很高的精度</li>
            <li>目前方法的主要缺陷
              <ul>
                <li>学到的表面高度扭曲、大规模重叠；只能通过适当的regularization正则化来减轻（即作者前一篇工作<em>Shape reconstruction by learning differentiable surface representations</em>）</li>
                <li>更紧急的问题：individual patches的放置时的global inconsistency，导致surface artifacts，比如孔洞，或者一些多个patch不正确交叉的区域
                  <ul>
                    <li>这个问题在meshlet和<em>Deep geometric prior for surface reconstruction.</em> 两篇里有一定程度攻击，但是只在optimization settings，很缓慢，并且在test time还需要几何观测（如带噪声的点云）；</li>
                  </ul>
                </li>
                <li>本篇主要基于learning-based (带encoder) 前作，利用它的低扭曲、低重叠属性，改进patches的global consistency</li>
              </ul>
            </li>
          </ul>
        </li>
      </ul>
    </li>
  </ul>

</details>

<h2 id="learning-implicit-surface-implicit-fieldsimplicit-functions">learning implicit surface: implicit fields/implicit functions</h2>

<ul>
  <li>overview
    <ul>
      <li>既然可以用一个隐函数\(f(x,y,z)=0\)表达一个隐曲面<br>
</li>
      <li>那当然可以先用\(某种神经网络_{一般是MLP+ReLU}\)去拟合构建一个空间数量值函数\(f(x,y,z)_{数量值一般物理意义为占用概率/与表面距离/表面内外等}\) ，然后训练这个神经网络</li>
      <li>训练好以后，如果需要从这个隐函数中提取mesh，一般就用marching cubes类方法空间采样</li>
    </ul>
  </li>
</ul>

<h3 id="sample-based-methods-to-extract-explicit-surface">sample based methods to extract explicit surface</h3>

<hr>

<p><strong><code class="language-plaintext highlighter-rouge">&lt;IM-Net&gt; "Learning Implicit Fields for Generative Shape Modeling"</code></strong><br>
<strong>[</strong> <code class="language-plaintext highlighter-rouge">CVPR2019</code> <strong>]</strong> <strong><a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Chen_Learning_Implicit_Fields_for_Generative_Shape_Modeling_CVPR_2019_paper.pdf">[paper]</a></strong> <strong><a href="https://www.sfu.ca/~zhiqinc/imgan/Readme.html">[web]</a></strong> <strong><a href="https://github.com/czq142857/implicit-decoder">[code]</a></strong> <strong><a href="https://github.com/czq142857/IM-NET">[code-improve]</a></strong>  <strong><a href="https://github.com/czq142857/IM-NET-pytorch">[code-pytorch]</a></strong> <strong>[</strong> <img class="emoji" title=":mortar_board:" alt=":mortar_board:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f393.png" height="20" width="20"> <code class="language-plaintext highlighter-rouge">SFU</code> <strong>]</strong> <br>
<strong>[</strong>  <code class="language-plaintext highlighter-rouge">Zhiqin Chen</code>, <code class="language-plaintext highlighter-rouge">Hao Zhang</code>  <strong>]</strong><br>
<strong>[</strong> <em><code class="language-plaintext highlighter-rouge">implicit shape representation</code>, <code class="language-plaintext highlighter-rouge">inside-outside indicator</code></em> <strong>]</strong></p>

<details>
  <summary>Click to expand</summary>

  <ul>
    <li>
<strong>Motivation</strong>
      <ul>
        <li>inside / outside indicator</li>
        <li>其实是一种类别级别的连续函数隐式的shape表征，类似occupancy networks；
<br>输入code + one point 坐标，输出在shape 内/外；（类似SDF）</li>
        <li><img src="media/image-20201203174748033.png" alt="image-20201203174748033"></li>
      </ul>
    </li>
  </ul>

</details>

<hr>

<p><strong><code class="language-plaintext highlighter-rouge">"Occupancy Networks: Learning 3D Reconstruction in Function Space"</code></strong><br>
<strong>[</strong> <code class="language-plaintext highlighter-rouge">CVPR2019</code> <strong>]</strong> <strong><a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Mescheder_Occupancy_Networks_Learning_3D_Reconstruction_in_Function_Space_CVPR_2019_paper.pdf">[paper]</a></strong> <strong><a href="https://github.com/autonomousvision/occupancy_networks">[code]</a></strong> <strong>[</strong> <img class="emoji" title=":mortar_board:" alt=":mortar_board:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f393.png" height="20" width="20"> <code class="language-plaintext highlighter-rouge">MPI</code>, <code class="language-plaintext highlighter-rouge">University of Tubingen</code> <strong>]</strong> <strong>[</strong> <img class="emoji" title=":office:" alt=":office:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f3e2.png" height="20" width="20"> <code class="language-plaintext highlighter-rouge">Google AI Berlin</code> <strong>]</strong><br>
<strong>[</strong>  <code class="language-plaintext highlighter-rouge">Lars Mescheder</code>， <code class="language-plaintext highlighter-rouge">Andreas Geiger</code>  <strong>]</strong><br>
<strong>[</strong> <em><code class="language-plaintext highlighter-rouge">continuous function occupancy</code>, <code class="language-plaintext highlighter-rouge">multi-resolution isosurface extraction</code>, <code class="language-plaintext highlighter-rouge">marching cubes</code></em> <strong>]</strong></p>

<details>
  <summary>Click to expand</summary>

  <ul>
    <li>
<strong>Motivation</strong>
      <ul>
        <li>用一个隐式函数来表达占用概率，从而可以实现任意分辨率的表达<br><img src="media/image-20201203153023230.png" alt="image-20201203153023230">
</li>
      </ul>
    </li>
    <li>
<strong>主要框架</strong>
      <ul>
        <li>
<strong>多分辨率等值面提取技术</strong> [Multiresolution IsoSurface Extraction (MISE)]<br><img src="media/image-20201203153114826.png" alt="image-20201203153114826">
</li>
      </ul>
    </li>
  </ul>

</details>

<hr>

<p><strong><code class="language-plaintext highlighter-rouge">"Convolutional Occupancy Networks"</code></strong><br>
<strong>[</strong> <code class="language-plaintext highlighter-rouge">ECCV2020</code> <strong>]</strong> <strong><a href="https://arxiv.org/pdf/2003.04618.pdf">[paper]</a></strong> <strong><a href="https://github.com/autonomousvision/convolutional_occupancy_networks">[code]</a></strong> <strong>[</strong> <img class="emoji" title=":mortar_board:" alt=":mortar_board:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f393.png" height="20" width="20"> <code class="language-plaintext highlighter-rouge">ETH</code>, <code class="language-plaintext highlighter-rouge">MPI</code>, <code class="language-plaintext highlighter-rouge">University of Tubingen</code> <strong>]</strong> <strong>[</strong> <img class="emoji" title=":office:" alt=":office:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f3e2.png" height="20" width="20"> <code class="language-plaintext highlighter-rouge">Amazon</code>, <code class="language-plaintext highlighter-rouge">Microsoft</code> <strong>]</strong><br>
<strong>[</strong>  <code class="language-plaintext highlighter-rouge">Songyou Peng</code>, <code class="language-plaintext highlighter-rouge">Michael Niemeyer</code>, <code class="language-plaintext highlighter-rouge">Lars Mescheder</code>, <code class="language-plaintext highlighter-rouge">Marc Pollefeys</code>, <code class="language-plaintext highlighter-rouge">Andreas Geiger</code>  <strong>]</strong><br>
<strong>[</strong> <em><code class="language-plaintext highlighter-rouge">Occupancy Networks</code></em> <strong>]</strong></p>

<details>
  <summary>Click to expand</summary>

  <ul>
    <li>
<strong>Motivation</strong>
      <ul>
        <li>从Occupancy Network的continuous feature function到voxelized features + 3D conv<br><img src="media/image-20201222145923538.png" alt="image-20201222145923538">
</li>
      </ul>
    </li>
  </ul>

</details>

<hr>

<p><strong><code class="language-plaintext highlighter-rouge">"Dynamic Plane Convolutional Occupancy Networks"</code></strong><br>
<strong>[</strong> <code class="language-plaintext highlighter-rouge">WACV2021</code> <strong>]</strong> <strong><a href="https://arxiv.org/pdf/2011.05813.pdf">[paper]</a></strong> <strong><a href="https://github.com/dsvilarkovic/dynamic_plane_convolutional_onet">[code]</a></strong> <strong>[</strong> <img class="emoji" title=":mortar_board:" alt=":mortar_board:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f393.png" height="20" width="20"> <code class="language-plaintext highlighter-rouge">MPI</code>, <code class="language-plaintext highlighter-rouge">ETH</code> <strong>]</strong> <br>
<strong>[</strong>  <code class="language-plaintext highlighter-rouge">Stefan Lionar</code>, <code class="language-plaintext highlighter-rouge">Daniil Emtsev</code>, <code class="language-plaintext highlighter-rouge">Dusan Svilarkovic</code>, <code class="language-plaintext highlighter-rouge">Songyou Peng</code>  <strong>]</strong><br>
<strong>[</strong> <em><code class="language-plaintext highlighter-rouge">3D reconstruction</code>, <code class="language-plaintext highlighter-rouge">occupancy networks</code></em> <strong>]</strong></p>

<details>
  <summary>Click to expand</summary>

  <ul>
    <li>
<strong>Motivation</strong>
      <ul>
        <li>occupancy networks是continuous function；<br>convolutional occupancy networks是voxelized features；<br>本篇是动态平面组上的features</li>
        <li><img src="media/image-20201222161043946.png" alt="image-20201222161043946"></li>
      </ul>
    </li>
  </ul>

</details>

<hr>

<p><strong><code class="language-plaintext highlighter-rouge">"DeepSDF: Learning Continuous Signed Distance Functions for Shape Representation"</code></strong><br>
<strong>[</strong> <code class="language-plaintext highlighter-rouge">CVPR2019</code> <strong>]</strong> <strong><a href="https://arxiv.org/pdf/1901.05103.pdf">[paper]</a></strong> <strong><a href="https://github.com/facebookresearch/DeepSDF">[code]</a></strong> <strong>[</strong> <img class="emoji" title=":mortar_board:" alt=":mortar_board:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f393.png" height="20" width="20"> <code class="language-plaintext highlighter-rouge">UW</code>, <code class="language-plaintext highlighter-rouge">MIT</code> <strong>]</strong> <strong>[</strong> <img class="emoji" title=":office:" alt=":office:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f3e2.png" height="20" width="20"> <code class="language-plaintext highlighter-rouge">Faceboook reality labs</code> <strong>]</strong><br>
<strong>[</strong>  <code class="language-plaintext highlighter-rouge">Jeong Joon Park</code>, <code class="language-plaintext highlighter-rouge">Peter Florence</code>, <code class="language-plaintext highlighter-rouge">Julian Straub</code>, <code class="language-plaintext highlighter-rouge">Richard Newcombe</code>, <code class="language-plaintext highlighter-rouge">Steven Lovegrove</code>  <strong>]</strong><br>
<strong>[</strong> <em><code class="language-plaintext highlighter-rouge">SDF</code></em> <strong>]</strong></p>

<details>
  <summary>Click to expand</summary>

  <ul>
    <li>
<strong>Motivation</strong>
      <ul>
        <li>SDF是CG领域又一个形状的表征；本篇是first to use deep SDF functions to model shapes</li>
        <li><img src="media/image-20201210100006815.png" alt="image-20201210100006815"></li>
        <li><img src="media/image-20201210095931233.png" alt="image-20201210095931233"></li>
      </ul>
    </li>
    <li>
<strong>overview</strong>
      <ul>
        <li>单个形状用单个SDF网络，一个category用code conditioned<br><img src="media/image-20201210100154456.png" alt="image-20201210100154456">
</li>
        <li>使用auto-decoder<br><img src="media/image-20201210100244493.png" alt="image-20201210100244493">
</li>
      </ul>
    </li>
  </ul>

</details>

<hr>

<p><strong><code class="language-plaintext highlighter-rouge">"DISN: Deep Implicit Surface Network for High-quality Single-view 3D Reconstruction"</code></strong><br>
<strong>[</strong> <code class="language-plaintext highlighter-rouge">NeurIPS2019</code> <strong>]</strong> <strong><a href="https://arxiv.org/pdf/1905.10711.pdf">[paper]</a></strong> <strong><a href="https://github.com/laughtervv/DISN">[code]</a></strong> <strong>[</strong> <img class="emoji" title=":mortar_board:" alt=":mortar_board:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f393.png" height="20" width="20"> <code class="language-plaintext highlighter-rouge">University of Southern California</code> <strong>]</strong> <strong>[</strong> <img class="emoji" title=":office:" alt=":office:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f3e2.png" height="20" width="20"> <code class="language-plaintext highlighter-rouge">Adobe</code> <strong>]</strong><br>
<strong>[</strong>  <code class="language-plaintext highlighter-rouge">Weiyue Wang</code>, <code class="language-plaintext highlighter-rouge">Qiangeng Xu</code>, <code class="language-plaintext highlighter-rouge">Duygu Ceylan</code>, <code class="language-plaintext highlighter-rouge">Radomir Mech</code>, <code class="language-plaintext highlighter-rouge">Ulrich Neumann</code>  <strong>]</strong><br>
<strong>[</strong> <em><code class="language-plaintext highlighter-rouge">SDF</code>, <code class="language-plaintext highlighter-rouge">single-view</code>, <code class="language-plaintext highlighter-rouge">encoder</code></em> <strong>]</strong></p>

<details>
  <summary>Click to expand</summary>

  <ul>
    <li>
<strong>review</strong>
      <ul>
        <li>训练时是有3D shape 的SDF的真值数据的；图像feature只是提供一个辅助的code输入而已</li>
      </ul>
    </li>
    <li>
<strong>Motivation</strong>
      <ul>
        <li>希望学到的shape，不仅全局特征好，还想有局部fine grained details 细粒度细节</li>
      </ul>
    </li>
    <li>
<strong>overview</strong>
      <ul>
        <li>同时用global features和local features来infer SDF<br><img src="media/image-20201209122023941.png" alt="image-20201209122023941">
</li>
      </ul>
    </li>
  </ul>

</details>

<hr>

<p><strong><code class="language-plaintext highlighter-rouge">"SAL: Sign agnostic learning of shapes from raw data."</code></strong><br>
<strong>[</strong> <code class="language-plaintext highlighter-rouge">CVPR2020</code> <strong>]</strong> <strong><a href="https://arxiv.org/pdf/1911.10414.pdf">[paper]</a></strong> <strong><a href="https://github.com/matanatz/SAL">[code]</a></strong> <strong>[</strong> <img class="emoji" title=":mortar_board:" alt=":mortar_board:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f393.png" height="20" width="20"> <code class="language-plaintext highlighter-rouge">Weizmann Institute of Science</code> <strong>]</strong> <br>
<strong>[</strong>  <code class="language-plaintext highlighter-rouge">Matan Atzmon</code>, <code class="language-plaintext highlighter-rouge">Yaron Lipman</code>  <strong>]</strong><br>
<strong>[</strong> <em><code class="language-plaintext highlighter-rouge">sign agnostic</code></em> <strong>]</strong></p>

<details>
  <summary>Click to expand</summary>

  <ul>
    <li><strong>Motivation</strong></li>
  </ul>

</details>

<hr>

<p><strong><code class="language-plaintext highlighter-rouge">"BSP-Net: Generating Compact Meshes via Binary Space Partitioning"</code></strong><br>
<strong>[</strong> <code class="language-plaintext highlighter-rouge">CVPR2020(Oral)</code> <strong>]</strong> <strong><a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Chen_BSP-Net_Generating_Compact_Meshes_via_Binary_Space_Partitioning_CVPR_2020_paper.pdf">[paper]</a></strong> <strong><a href="https://github.com/czq142857/BSP-NET-original">[code(tf)]</a></strong> <strong><a href="https://github.com/czq142857/BSP-NET-pytorch">[code(pytorch)]</a></strong> <strong><a href="https://bsp-net.github.io/">[web]</a></strong>  <strong>[</strong> <img class="emoji" title=":mortar_board:" alt=":mortar_board:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f393.png" height="20" width="20"> <code class="language-plaintext highlighter-rouge">SFU</code> <strong>]</strong> <strong>[</strong> <img class="emoji" title=":office:" alt=":office:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f3e2.png" height="20" width="20"> <code class="language-plaintext highlighter-rouge">Google</code> <strong>]</strong><br>
<strong>[</strong>  <code class="language-plaintext highlighter-rouge">Zhiqin Chen</code>, <code class="language-plaintext highlighter-rouge">Andrea Tagliasacchi</code>, <code class="language-plaintext highlighter-rouge">Hao Zhang</code>  <strong>]</strong><br>
<strong>[</strong> <em><code class="language-plaintext highlighter-rouge">low-poly</code>, <code class="language-plaintext highlighter-rouge">convex composition</code>, <code class="language-plaintext highlighter-rouge">category-shape correspondence</code>,  <code class="language-plaintext highlighter-rouge">part correpondence</code>, <code class="language-plaintext highlighter-rouge">inside-outside indicator</code></em> <strong>]</strong></p>

<details>
  <summary>Click to expand</summary>

  <ul>
    <li>
<strong>review</strong>
      <ul>
        <li>IM-Net同作的续作</li>
        <li>效果很好；但是对于thin-structure表现不佳</li>
      </ul>
    </li>
    <li>
<strong>Motivation</strong>
      <ul>
        <li>take inspiration from binary space partitions，学到更<code class="language-plaintext highlighter-rouge">compact</code> / 紧致 / low-poly的mesh表征 <br><img src="media/image-20201229112704532.png" alt="image-20201229112704532">
</li>
      </ul>
    </li>
    <li>
<strong>overview</strong>
      <ul>
        <li>依旧是输入point坐标 + shape code condition，输出inside / outside；</li>
        <li>不同之处在于构造的内部模型是<code class="language-plaintext highlighter-rouge">n</code>个平面方程，靠<code class="language-plaintext highlighter-rouge">n</code>个这样的binary space partition的组合来表征shape</li>
        <li>靠binary partition的组合来表达shape的示意图：<br>首先组合出一个个的<code class="language-plaintext highlighter-rouge">convex</code>凸包，再组合成 whole shape
          <ul>
            <li>其实做的事情本质上类似于把MLP+ReLU的空间线性划分过程显式化，不过这里的convex的概念值得思考</li>
          </ul>
        </li>
      </ul>
    </li>
  </ul>

  <table>
    <thead>
      <tr>
        <th>示意图</th>
        <th>网络结构</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><img src="media/image-20201229113148367.png" alt="image-20201229113148367"></td>
        <td><img src="media/image-20201229113341539.png" alt="image-20201229113341539"></td>
      </tr>
    </tbody>
  </table>

  <ul>
    <li>
<strong>few shot segmentation</strong>
      <ul>
        <li>因为同category的shape的convex组合之间已经建立起了<code class="language-plaintext highlighter-rouge">correspondence</code>，只需要手动给几个shape标一下convex id对应的part label，就可以利用correspondence获得其他同category shape的标注<br><img src="media/image-20201229113640866.png" alt="image-20201229113640866">
</li>
      </ul>
    </li>
    <li>
<strong>results</strong>
      <ul>
        <li><img src="media/image-20201229113556212.png" alt="image-20201229113556212"></li>
        <li><img src="media/image-20201229114054936.png" alt="image-20201229114054936"></li>
      </ul>
    </li>
  </ul>

</details>

<hr>

<p><strong><code class="language-plaintext highlighter-rouge">"CvxNet: Learnable Convex Decomposition"</code></strong><br>
<strong>[</strong> <code class="language-plaintext highlighter-rouge">CVPR2020</code> <strong>]</strong> <strong><a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Deng_CvxNet_Learnable_Convex_Decomposition_CVPR_2020_paper.pdf">[paper]</a></strong> <strong><a href="https://github.com/tensorflow/graphics/tree/master/tensorflow_graphics/projects/cvxnet">[code(tf-graphics official repo)]</a></strong> <strong><a href="https://cvxnet.github.io/">[web]</a></strong>  <strong>[</strong> <img class="emoji" title=":office:" alt=":office:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f3e2.png" height="20" width="20"> <code class="language-plaintext highlighter-rouge">Google</code> <strong>]</strong><br>
<strong>[</strong>  <code class="language-plaintext highlighter-rouge">Boyang Deng</code>, <code class="language-plaintext highlighter-rouge">Kyle Genova</code>, <code class="language-plaintext highlighter-rouge">Soroosh Yazdani</code>, <code class="language-plaintext highlighter-rouge">Sofien Bouaziz</code>, <code class="language-plaintext highlighter-rouge">Geoffrey Hinton</code>, <code class="language-plaintext highlighter-rouge">Andrea Tagliasacchi</code>  <strong>]</strong><br>
<strong>[</strong> <em><code class="language-plaintext highlighter-rouge">convex composition</code>, <code class="language-plaintext highlighter-rouge">inside-outside indicator</code></em> <strong>]</strong></p>

<details>
  <summary>Click to expand</summary>

  <ul>
    <li>
<strong>review</strong>
      <ul class="task-list">
        <li>和BSP-Net的概念很像；用一个个由平面包围出的<code class="language-plaintext highlighter-rouge">convex</code>定义surface；输入是点坐标，输出是 <code class="language-plaintext highlighter-rouge">inside/outside indicator</code>
</li>
        <li class="task-list-item">
<input type="checkbox" class="task-list-item-checkbox" disabled>和BSP-Net的区别是这里是softmax？</li>
      </ul>
    </li>
    <li>
<strong>Motivation</strong>
      <ul>
        <li>from hyperplanes to occupancy<br><img src="media/image-20201230092051009.png" alt="image-20201230092051009">
</li>
        <li><img src="media/CvxNet.gif" alt="CvxNet"></li>
      </ul>
    </li>
  </ul>

</details>

<hr>

<p><strong><code class="language-plaintext highlighter-rouge">"Neural-Pull: Learning Signed Distance Functions from Point Clouds by Learning to Pull Space onto Surfaces"</code></strong><br>
<strong>[</strong> <code class="language-plaintext highlighter-rouge">arXiv2020</code> <strong>]</strong> <strong><a href="https://arxiv.org/pdf/2011.13495.pdf">[paper]</a></strong> <strong>[[code]]</strong> <strong>[[web]]</strong> <strong>[</strong> <img class="emoji" title=":mortar_board:" alt=":mortar_board:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f393.png" height="20" width="20"> <code class="language-plaintext highlighter-rouge">Tsinghua</code>, , <code class="language-plaintext highlighter-rouge">University of Maryland</code> <strong>]</strong> <br>
<strong>[</strong>  <code class="language-plaintext highlighter-rouge">Baorui Ma</code>, <code class="language-plaintext highlighter-rouge">Zhizhong Han</code>, <code class="language-plaintext highlighter-rouge">Yu-Shen Liu</code>, <code class="language-plaintext highlighter-rouge">Matthias Zwicker</code>  <strong>]</strong><br>
<strong>[</strong> <em><code class="language-plaintext highlighter-rouge">reconstructing surfaces from 3D pointcloud</code>, <code class="language-plaintext highlighter-rouge">surface reconstruction</code></em> <strong>]</strong></p>

<details>
  <summary>Click to expand</summary>

  <ul>
    <li>
<strong>Motivation</strong>
      <ul>
        <li>训练一个神经网络去把query 3D locations “拉” 到他们在表面上的最近邻居；<br>拉的操作，方向是query locations处的网络梯度，步长是query locations处的网络SDF值，这两个都是从网络自身计算出来的<br>
</li>
        <li>让我们可以同时更新sdf值和梯度</li>
        <li><img src="media/image-20201228162639806.png" alt="image-20201228162639806"></li>
      </ul>
    </li>
    <li>
<strong>overview</strong>
      <ul>
        <li>loss functions直接从GT点云本身定义，而不是利用GT SDF作回归；<br><img src="media/image-20201228163704020.png" alt="image-20201228163704020"><br><img src="media/image-20201228163648881.png" alt="image-20201228163648881">
</li>
      </ul>
    </li>
  </ul>

</details>

<hr>

<p><strong><code class="language-plaintext highlighter-rouge">"DUDE: Deep Unsigned Distance Embeddings for Hi-Fidelity Representation of Complex 3D Surfaces"</code></strong><br>
<strong>[</strong> <code class="language-plaintext highlighter-rouge">arxiv2020</code> <strong>]</strong> <strong><a href="https://arxiv.org/pdf/2011.02570">[paper]</a></strong> <strong>[[code]]</strong> <strong>[</strong> <img class="emoji" title=":mortar_board:" alt=":mortar_board:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f393.png" height="20" width="20"> <code class="language-plaintext highlighter-rouge">CMU</code> <strong>]</strong> <strong>[</strong> <img class="emoji" title=":office:" alt=":office:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f3e2.png" height="20" width="20"> <code class="language-plaintext highlighter-rouge">Verisk Analytics</code> <strong>]</strong><br>
<strong>[</strong>  <code class="language-plaintext highlighter-rouge">Rahul Venkatesh</code>, <code class="language-plaintext highlighter-rouge">Sarthak Sharma</code>, <code class="language-plaintext highlighter-rouge">Aurobrata Ghosh</code>, <code class="language-plaintext highlighter-rouge">Laszlo Jeni</code>, <code class="language-plaintext highlighter-rouge">Maneesh Singh</code>  <strong>]</strong><br>
<strong>[</strong> <em><code class="language-plaintext highlighter-rouge">unsigned distance field</code>, <code class="language-plaintext highlighter-rouge">normal vector field</code>, <code class="language-plaintext highlighter-rouge">open topogoly surfaces</code></em> <strong>]</strong></p>

<details>
  <summary>Click to expand</summary>

  <ul>
    <li>
<strong>Motivation</strong>
      <ul>
        <li>现有的隐式表面deep networks方法只能表征拓扑上闭合的形状；<br>并且结果是，训练时候经常需要clean watertight meshes</li>
        <li>本篇提出无符号的距离嵌入减轻了上述问题
          <ul>
            <li>利用<code class="language-plaintext highlighter-rouge">unsigned distance field (uDF)</code>无符号距离场来表达对表面的接近程度</li>
            <li>利用<code class="language-plaintext highlighter-rouge">normal vector field (nVF)</code>法向量场来表达表面朝向</li>
            <li>uDF + nVF 可以表达任意开/闭拓扑的high fidelity形状</li>
            <li>可以从带噪声的triangle soups学习，不需要watertight mehses</li>
            <li>并且额外提供了学到的表征提取、渲染等值面的新方法</li>
          </ul>
        </li>
        <li><img src="media/image-20201222150310307.png" alt="image-20201222150310307"></li>
      </ul>
    </li>
    <li>
<strong>overview</strong>
      <ul>
        <li>uDF+nVF<br><img src="media/image-20201222151610880.png" alt="image-20201222151610880">
</li>
      </ul>
    </li>
  </ul>

</details>

<hr>

<p><strong><code class="language-plaintext highlighter-rouge">"Deformed Implicit Field: Modeling 3D Shapes with Learned Dense Correspondence"</code></strong><br>
<strong>[</strong> <code class="language-plaintext highlighter-rouge">arXiv2020</code> <strong>]</strong> <strong><a href="https://arxiv.org/pdf/2011.13650.pdf">[paper+supp]</a></strong> <strong><a href="https://github.com/microsoft/DIF-Net">[code]</a></strong> <strong>[</strong> <img class="emoji" title=":office:" alt=":office:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f3e2.png" height="20" width="20"> <code class="language-plaintext highlighter-rouge">MSRA</code> <strong>]</strong> <strong>[</strong> <img class="emoji" title=":mortar_board:" alt=":mortar_board:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f393.png" height="20" width="20"> <code class="language-plaintext highlighter-rouge">Tsinghua</code> <strong>]</strong> <br>
<strong>[</strong>  <code class="language-plaintext highlighter-rouge">Yu Deng</code>, <code class="language-plaintext highlighter-rouge">Jiaolong Yang</code>, <code class="language-plaintext highlighter-rouge">Xin Tong</code>  <strong>]</strong><br>
<strong>[</strong> <em><code class="language-plaintext highlighter-rouge">3D deformation field</code>, <code class="language-plaintext highlighter-rouge">template field</code>, <code class="language-plaintext highlighter-rouge">category shape correspondence</code></em> <strong>]</strong></p>

<details>
  <summary>Click to expand</summary>

  <ul>
    <li>
<strong>Motivation</strong>
      <ul>
        <li>把每个具体instance shape表达为一个template的shape的deformation</li>
        <li>用deformation field建立起 <strong><u>`shape correspondence`</u></strong>，这样就可以做texture transfer、label transfer等</li>
        <li><img src="media/image-20201222155438709.png" alt="image-20201222155438709"></li>
      </ul>
    </li>
    <li>
<strong>overview</strong>
      <ul>
        <li>用一个超网络从code预测DeformNet \(D\)的参数；<br>然后在空间中的每一处，从同一个template SDF，DeformNet \(D\)产生位置修正\(v\)与标量距离修正\(\Delta s\)，总共4维输出<br>即最终的\(p\)点处的SDF值为：\(s=T(p+v)+\Delta s=T(p+D^v_{\omega}(p))+D^{\Delta s}_{\omega}(p)\)<br>注意变形向量\(v\)其实反映的是从shape instance场 到 template 场所需的变形向量<br><img src="media/image-20201222153322051.png" alt="image-20201222153322051">
</li>
      </ul>
    </li>
    <li>
<strong>losses</strong>
      <ul>
        <li>SDF loss
          <ul>
            <li>被训练的量：变形场超网络\(\Psi\)，SDF输出场\(\Phi\)，模板场\(T\)，learned latent codes \(\{\alpha_j\}\)；\(\Psi_i(p)\)代表predicted SDF值\(\Phi_{\Psi(\alpha_i)}(p)\)，\(\Omega\)代表3D空间，\(\mathcal{S}_i\) 代表形状表面
              <ul>
                <li>
\[\Phi_{\Psi(\alpha)}(p)=T(p+D_{\Psi(\alpha)}^v(p)) + D_{\Psi(\alpha)}^{\Delta s}(p)\]
                </li>
              </ul>
            </li>
            <li>
\[L_{sdf}=\underset {i}{\sum} \left( L_1 + L_2 + L_3 + L_4 \right)\]
              <ul>
                <li>\(\underset {p \in \Omega}{\sum} \lvert \Phi_i(p)-\overline{s}\rvert\) 代表预测SDF和正确SDF的误差
                  <ul>
                    <li>\(p \in \Omega\) 这里是在3D空间中采样</li>
                  </ul>
                </li>
                <li>\(\underset{p\in \mathcal{S}_i}{\sum} (1-\langle \nabla\Phi_i(p), \overline{n} \rangle)\) 代表预测法向量和正确法向量的误差（角度误差，用夹角余弦应接近1来表达）
                  <ul>
                    <li>\(p \in \mathcal{S}_i\)，这里是在表面上采点</li>
                  </ul>
                </li>
                <li>\(\underset{p\in\Omega}{\sum} \lvert \Vert \nabla\Phi_i(p) \rVert_2 - 1 \rvert\) 代表预测法向量的模应该是1 （因为是SDF）
                  <ul>
                    <li>\(p \in \Omega\) 这里是在3D空间中采样</li>
                  </ul>
                </li>
                <li>\(\underset{p\in\Omega \backslash \mathcal{S}_i}{\sum} \rho(\Phi_i(p)), \;where \; \rho(s)=\exp(-\delta \cdot \lvert s \rvert), \delta \gg 1\) 代表对 <strong>SDF值靠近0</strong> 的 <strong>非表面</strong> 点的惩罚；
                  <ul>
                    <li>\(\delta \gg 1\)就代表只有靠近0的时候这项loss才有值
                      <ul class="task-list">
                        <li class="task-list-item">
<input type="checkbox" class="task-list-item-checkbox" disabled>Q: 类似一种负的L0-norm ？</li>
                      </ul>
                    </li>
                    <li>详见 <em>(SIREN) Implicit neural representations with periodic activation functions. NeurIPS2020</em> 论文</li>
                  </ul>
                </li>
              </ul>
            </li>
          </ul>
        </li>
        <li>正则化
          <ul>
            <li>regularization loss to constrain the learned latent codes: \(L_{reg}=\underset{i}{\sum} \lVert \alpha_i \rVert_2^2\)</li>
            <li>可以用一些其他更强的正则化，比如VAE训练时用的 最小化latent code后验分布和高斯分布的KL散度</li>
          </ul>
        </li>
        <li>normal consistency prior 法向量一致性先验
          <ul>
            <li>考虑到表面点和语义 高度关联：e.g. （在canonical space假设下）车顶总是指向天空，左车门总是指向左侧</li>
            <li>因此，让相关的点的法向量互相一致
              <ul>
                <li>鼓励 模板场中的点处的法向量 和 <strong>所有给定shape instance</strong> 中的相关点 处的法向量 一致</li>
                <li>
\[L_{normal}=\underset{i}{\sum} \underset{p\in\mathcal{S}_i}{\sum} \left( 1 - \langle \nabla T(p+D_{\omega_i}^v (p)), \overline{n} \rangle \right)\]
                </li>
                <li>即让模板场中的 对应位置p的点 和 真值法向量保持一致</li>
                <li>\(p \in \mathcal{S}_i\)，这里是在表面上采点</li>
                <li>
<del>如果没有标量修正场，模板场对应位置p的点处的法向量就是 最终输出场的法向量，和\(L_{sdf}\)的第2项一样</del>
                  <ul class="task-list">
                    <li class="task-list-item">
<input type="checkbox" class="task-list-item-checkbox" disabled>Q: 以下为笔者猜想。有待代码检查验证。</li>
                    <li class="task-list-item">变形后的形状shape instance场中的点坐标是\(p\)，模板场中的 <strong>相关</strong> 点坐标是 \(p+D_{\omega_i}^v (p)\)</li>
                    <li class="task-list-item">
<strong>相关</strong> 点处的法向量 其实是\(\nabla_{p+D_{\omega_i}^v (p)} T(p+D_{\omega_i}^v (p))\)，而非\(\nabla_p T(p+D_{\omega_i}^v (p))\)</li>
                    <li class="task-list-item">\(L_{sdf}\)第2项是\(\nabla_p\Phi_i(p)=\nabla_p \left( \quad T(p+D_{\omega_i}^v (p)) \; (+D_{\omega_i}^{\Delta s}(p)) \quad \right)\)</li>
                    <li class="task-list-item">即其主要是强调 模板场 和 变形后的形状实例场 中 相关点处的 两个场的法向量保持一致性</li>
                    <li class="task-list-item">其实应该是\(\nabla_{p+D_{\omega_i}^v (p)} T(p+D_{\omega_i}^v (p))\)和\(\nabla_p\Phi_i(p)\)的夹角，而不是和\(\overline{n}\)的夹角；<br>只不过\(\nabla_p\Phi_i(p)\)就是\(\overline{n}\)的近似，所以用\(\overline{n}\)也可</li>
                  </ul>
                </li>
              </ul>
            </li>
          </ul>
        </li>
        <li>deformation smoothness prior 变形平滑先验
          <ul>
            <li>鼓励平滑的变形、防止巨大的形状扭曲，引入一个对变形场的平滑loss</li>
            <li>
\[L_{smooth}=\underset{i}{\sum} \underset{p\in\Omega}{\sum} \underset{d\in{X,Y,Z}}{\sum} \lVert \nabla D_{\omega_i}^v \vert_d (p) \rVert_2\]
              <ul>
                <li>
<img class="emoji" title=":heavy_check_mark:" alt=":heavy_check_mark:" src="https://github.githubassets.com/images/icons/emoji/unicode/2714.png" height="20" width="20"> \(\begin{pmatrix} \frac{\partial v_x}{\partial x} \\ \frac{\partial v_x}{\partial y} \\ \frac{\partial v_x}{\partial z} \end{pmatrix}\), \(\begin{pmatrix} \frac{\partial v_y}{\partial x} \\ \frac{\partial v_y}{\partial y} \\ \frac{\partial v_y}{\partial z} \end{pmatrix}\), \(\begin{pmatrix} \frac{\partial v_z}{\partial x} \\ \frac{\partial v_z}{\partial y} \\ \frac{\partial v_z}{\partial z} \end{pmatrix}\)
                  <ul>
                    <li>把\(v = \begin{pmatrix} v_x \\ v_y \\ v_z \end{pmatrix} =D_{\omega_i}^v(p)\) 函数看作3个标量函数构成的向量值函数，每个标量值函数有自己的梯度式</li>
                  </ul>
                </li>
                <li>
<img class="emoji" title=":x:" alt=":x:" src="https://github.githubassets.com/images/icons/emoji/unicode/274c.png" height="20" width="20"> \(\begin{pmatrix} \frac{\partial v_x}{\partial x} \\ \frac{\partial v_y}{\partial x} \\ \frac{\partial v_z}{\partial x} \end{pmatrix}\), \(\begin{pmatrix} \frac{\partial v_x}{\partial y} \\ \frac{\partial v_y}{\partial y} \\ \frac{\partial v_z}{\partial y} \end{pmatrix}\), \(\begin{pmatrix} \frac{\partial v_x}{\partial z} \\ \frac{\partial v_y}{\partial z} \\ \frac{\partial v_z}{\partial z} \end{pmatrix}\)</li>
              </ul>
            </li>
            <li>penalizes the spatial gradient of the deformation field along X, Y and Z directions.<br>惩罚变形场函数沿着X,Y,Z方向的空间梯度</li>
            <li>\(p \in \Omega\) 这里是在3D空间中采样</li>
          </ul>
        </li>
        <li>minimal correction prior
          <ul>
            <li>鼓励形状表征主要是通过 形变场，而不是通过标量修正</li>
            <li>\(L_c=\underset{i}{\sum} \underset{p\in\Omega}{\sum} \lvert D_{\omega_i}^{\Delta s}(p) \rvert\) 惩罚标量修正L1大小</li>
            <li>\(p \in \Omega\) 这里是在3D空间中采样</li>
          </ul>
        </li>
        <li>\(\underset{\{\alpha_j\}, \Psi, T }{\arg\min} L_{sdf} + w_1 L_{normal}+w_2 L_{smooth}+w_3 L_c + w_4 L_{reg}\)，<br>\(L_{sdf}\)中的4项：3e3, 1e2, 5e1, 5e2<br>\(w_1=1{\rm e}2, w_2=\{1,2,5\}, w_3=\{1{\rm e}2, 5{\rm e}1\}, w_4 = 1{\rm e}2\)</li>
      </ul>
    </li>
    <li>
<strong>相关性 uncertainty measurement</strong>
      <ul>
        <li>两个物体\(\mathcal{O}_i\)和 \(\mathcal{O}_j\)之间的 <code class="language-plaintext highlighter-rouge">相关性</code> 可以通过在 <code class="language-plaintext highlighter-rouge">template space</code>中进行 <strong><code class="language-plaintext highlighter-rouge">最近邻搜索</code></strong> 来建立；
          <ul class="task-list">
            <li class="task-list-item">
<input type="checkbox" class="task-list-item-checkbox" disabled>Q: 最近邻不会出现错误的相关性么？
              <ul>
                <li>考虑应该尽量鼓励标量修正为0，主要通过位置修正，当出现因结构改变而发生的实在无法反映的形状变化时，才用标量修正</li>
                <li>文中的<code class="language-plaintext highlighter-rouge">minimum correction prior</code>已经在做这个事</li>
              </ul>
            </li>
          </ul>
        </li>
        <li>假设物体\(\mathcal{O}_i\)（表面）上一点\(p_i\)，通过最近邻搜索找到了点\(p_i\)在物体\(\mathcal{O}_j\)上<code class="language-plaintext highlighter-rouge">相关</code>的（表面上的）一点\(p_j\)
          <ul>
            <li>那么二者之间相关性的不确定性可以通过一个simple yet surprisingly-effective的uncertainty metric评估：</li>
            <li>
\[u(p_i,p_j)=1-\exp(-\gamma \lVert (p_i+v_i) - (p_j+v_j) \rVert_2^2)\]
              <ul>
                <li>其中\(v_i=D_{\omega_i}^v(p_i)\) 是点上的变形向量；是从 shape instance space到 template space的\(\Delta\)</li>
                <li>\(\lVert (p_i+v_i) - (p_j+v_j) \rVert_2\)其实就是这对相关点\(p_i\)和\(p_j\)在template space下的距离</li>
              </ul>
            </li>
            <li>不确定性大的区域 comform well to 形状之间的 <code class="language-plaintext highlighter-rouge">structure discrepancy</code> 结构不符<br>下图展示的是形状A（表面）上的点，在形状B（表面）上找到的相关的点的不确定性；红色高不确定性，蓝色低不确定性<br><img src="media/image-20210104200614483.png" alt="image-20210104200614483">
</li>
          </ul>
        </li>
      </ul>
    </li>
    <li>
<strong>results</strong>
      <ul>
        <li>texture transfer <br><img src="media/image-20201222155357538.png" alt="image-20201222155357538"><br><img src="media/image-20210104173728589.png" alt="image-20210104173728589">
</li>
        <li>label transfer：可以看到对于 椅子把 这种时有时无的结构也可以handle<br><img src="media/image-20201222155611605.png" alt="image-20201222155611605">
</li>
      </ul>
    </li>
    <li>
<strong>Ablation study / discussions</strong>
      <ul>
        <li>单纯的位置修正就已经可以构成变形场；但是本篇发现，仅仅位置修正不够，加入标量修正可以：
          <ul>
            <li>① 加入标量修正对生成所需shape有帮助
              <ul>
                <li><img src="media/image-20210104165611897.png" alt="image-20210104165611897"></li>
              </ul>
            </li>
            <li>② 实验发现 <strong><u>加入标量修正对于学习高质量的相关性也很重要</u></strong>
              <ul class="task-list">
                <li class="task-list-item">
<input type="checkbox" class="task-list-item-checkbox" disabled>Q: why ? <br>试图解释：标量修正可以控制形状的一部分 特征： <code class="language-plaintext highlighter-rouge">膨胀</code>？<code class="language-plaintext highlighter-rouge">结构/拓扑改变</code>？，从而更容易学到简单、plausible的对应关系？
                  <ul class="task-list">
                    <li class="task-list-item">
<input type="checkbox" class="task-list-item-checkbox" disabled>Q: 类似CGAN中，用一个随机噪声z控制一些<code class="language-plaintext highlighter-rouge">"不想要"</code>的特征？</li>
                    <li class="task-list-item">
<input type="checkbox" class="task-list-item-checkbox" disabled>Q: 除了标量修正这种控制<code class="language-plaintext highlighter-rouge">"额外"</code>/<code class="language-plaintext highlighter-rouge">"不想要"</code>的特征的方式以外，可否设法引入其他方式控制其他<code class="language-plaintext highlighter-rouge">"不想要"</code>的特征？</li>
                  </ul>
                </li>
              </ul>
            </li>
          </ul>
        </li>
        <li>template implicit field ≠ template shape
          <ul>
            <li>template implicit field并不是template shape；甚至都不是valid SDF</li>
            <li>instead，template implicit field 捕捉的是 <strong>一个category中不同物体的shape <code class="language-plaintext highlighter-rouge">结构</code></strong>
</li>
            <li>在实验中，发现如果loss不合适的情况下，template implicit field <strong><code class="language-plaintext highlighter-rouge">degenerates</code> to a <u>valid shape SDF</u></strong> representing a certain shape, 导致重建的 <strong>精确度下降</strong>、<strong>相关性降低</strong>
</li>
          </ul>
        </li>
        <li>几个training loss对结果的影响<br><img src="media/image-20210104120743115.png" alt="image-20210104120743115">
</li>
      </ul>
    </li>
    <li>
<strong>implementation details</strong>
      <ul>
        <li>网络结构<br><img src="media/image-20210104121544233.png" alt="image-20210104121544233">
</li>
      </ul>
    </li>
  </ul>

</details>

<hr>

<p><strong><code class="language-plaintext highlighter-rouge">"Deep Implicit Templates for 3D Shape Representation"</code></strong><br>
<strong>[</strong> <code class="language-plaintext highlighter-rouge">arXiv2020</code> <strong>]</strong> <strong><a href="https://arxiv.org/pdf/2011.14565.pdf">[paper]</a></strong> <strong><a href="http://www.liuyebin.com/dit/dit.html">[web]</a></strong> <strong><a href="http://www.liuyebin.com/dit/assets/supp_vid.mp4">[supp_video]</a></strong> <strong><a href="https://github.com/ZhengZerong/DeepImplicitTemplates">[code]</a></strong>  <strong>[</strong> <img class="emoji" title=":mortar_board:" alt=":mortar_board:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f393.png" height="20" width="20"> <code class="language-plaintext highlighter-rouge">Tsinghua</code> <strong>]</strong> <br>
<strong>[</strong>  <code class="language-plaintext highlighter-rouge">Zerong Zheng</code>, <code class="language-plaintext highlighter-rouge">Tao Yu</code>, <code class="language-plaintext highlighter-rouge">Qionghai Dai</code>, <a href="http://www.liuyebin.com/"><code class="language-plaintext highlighter-rouge">Yebin Liu</code></a>  <strong>]</strong><br>
<strong>[</strong> <em><code class="language-plaintext highlighter-rouge">spatial warping LSTM</code>, <code class="language-plaintext highlighter-rouge">category shape correpondence</code></em> <strong>]</strong></p>

<details>
  <summary>Click to expand</summary>

  <ul>
    <li>
<strong>review</strong>
      <ul>
        <li>这种变形场类方法，最大的问题应该在于当 层级结构 / 拓扑 发生大的改变时，这种很大程度由位置决定的对应关系是否无法准确反应结构上的变化，从而导致degenerates的行为</li>
        <li>和 <em>deformed implicit field</em> 思路很像，那篇也是清华的
          <ul>
            <li>deformed implicit field 除了位置修正外还有标量\(\Delta s\)修正；本篇只有位置修正
              <ul>
                <li>deformed implicit field在表面上的点变形后不一定还在表面上；需要用 <strong><u>最近邻算法</u></strong> 来计算变形后的形状相关点的位置</li>
                <li>本篇在表面上的点，变形后一定还在表面上（变形前后的点的SDF值均为0）</li>
              </ul>
            </li>
            <li>deformed implicit field 是一个超参数网络，从code得到位置修正、\(\Delta S\)修正的网络 <strong><u>参数</u></strong>；本篇是一个LSTM，输入code+p输出位置修正</li>
            <li>对于模板的理解与deformed implicit field 完全不同：
              <ul>
                <li>deformed implicit field认为模板是一种对类别中形状公共捕捉/”存储”，甚至模板本身不一定是一个valid SDF</li>
                <li>本篇认为模板就是一个valid shape，甚至可以选择数据集中的某个具体物体形状作为模板（<code class="language-plaintext highlighter-rouge">user defined templates</code>）</li>
              </ul>
            </li>
            <li>
<img class="emoji" title=":pushpin:" alt=":pushpin:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f4cc.png" height="20" width="20"> 对于<code class="language-plaintext highlighter-rouge">structure discrepancy</code>结构差异性的考虑，<strong><u>本篇不如deformed implicit field.</u></strong>
              <ul>
                <li>deformed implicit field有考虑用一个标量修正来cover一定的结构修改；位置修正只包括形状修改</li>
                <li>而本篇把结构修改和几何修改全部都用位置变化来cover
                  <ul>
                    <li>比如下图，仔细看最上面一行chair的关键点，其实就是有问题的：最左边的chair，黄色的点是【可以坐的区域 / 椅面的边缘】，而最右边的chair，黄色的点是【沙发把手的边缘】；这显然<strong><u>在语义上就不是相关的两个点</u></strong><br><img src="media/image-20210111155948737.png" alt="image-20210111155948737">
</li>
                  </ul>
                </li>
              </ul>
            </li>
          </ul>
        </li>
        <li>
<del>因为有很多谨慎的设计（1. 使用LSTM warp而不是MLP warp 2.对canonical的正则化 3. 对空间扭曲的正则化），从transfer的效果上看要比deformed implicit field好一些？</del> <br>效果不如deformed implicit field</li>
      </ul>
    </li>
  </ul>

  <table>
    <thead>
      <tr>
        <th> </th>
        <th>本篇：Deep Implicit Templates for 3D Shape Representation</th>
        <th>deformed implicit field</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>texture transfer</td>
        <td><img src="media/image-20201229180439537.png" alt="image-20201229180439537"></td>
        <td><img src="media/image-20201229180759966.png" alt="image-20201229180759966"></td>
      </tr>
      <tr>
        <td>label transfer</td>
        <td>keypoint detection PCK accuracy<br><img src="media/image-20210104120316992.png" alt="image-20210104120316992">
</td>
        <td>label transfer IOU banchmark<br><img src="media/image-20210104114757341.png" alt="image-20210104114757341">
</td>
      </tr>
      <tr>
        <td>细节对比：本篇结果出现了错误的语义对应</td>
        <td>
<img src="media/image-20210116172741451.png" alt="image-20210116172741451"><img src="media/image-20210116172801005.png" alt="image-20210116172801005">
</td>
        <td><img src="media/image-20210116173201127.png" alt="image-20210116173201127"></td>
      </tr>
    </tbody>
  </table>

  <ul>
    <li>
<strong>Motivation</strong>
      <ul>
        <li>把一个具体shape表征为 <code class="language-plaintext highlighter-rouge">conditional deformations</code> of a <code class="language-plaintext highlighter-rouge">template</code>，建立起 category level 的dense correspondence<br>注意是 <strong><u>conditional</u></strong> deformations，相当与Deformed NeRF那篇，有一个deformation code <br><img src="media/image-20201229171836408.png" alt="image-20201229171836408">
</li>
        <li>把一个条件空间变换 分解为 若干个仿射变换</li>
        <li>training loss经过谨慎设计，无监督地保证重建的精度 + plausible template</li>
      </ul>
    </li>
    <li>
<strong>overview</strong>
      <ul>
        <li><img src="media/image-20201229171755900.png" alt="image-20201229171755900"></li>
        <li><img src="media/image-20201229173256894.png" alt="image-20201229173256894"></li>
        <li>warping函数把首先把一个点p映射到一个<code class="language-plaintext highlighter-rouge">canonical position</code> ，然后在模板SDF中query这个canonical position来获取SDF值</li>
        <li>照搬原DeepSDF训练是不行的：尤其容易学出一个过分简单的template和过拟合到一个复杂的transformer（这里译作变换器更合适），最终带来不准确的correspondence</li>
        <li>目标：
          <ul>
            <li>一个最优的template，能够表达一组物体的公共结构</li>
            <li>together with a 空间变换器，能够建立精确的稠密的相关性</li>
            <li>学到的模型应保留DeepSDF的表达能力和泛化能力，因此支持mesh补间和形状补完</li>
          </ul>
        </li>
      </ul>
    </li>
    <li>
<strong>spatial warping LSTM</strong>
      <ul>
        <li>实践发现用MLP来表达warping function不太合适：
          <ul class="task-list">
            <li class="task-list-item">
<input type="checkbox" class="task-list-item-checkbox" disabled>Q: 考虑理论上的原因</li>
            <li class="task-list-item">MLP和LSTM作warping的对比：warping的补间<br><img src="media/image-20201229172212443.png" alt="image-20201229172212443">
</li>
          </ul>
        </li>
        <li>把一个点的空间变换表示为多步仿射变换：
          <ul>
            <li>
\[(\alpha^{(i)},\beta^{(i)},\phi^{(i)},\psi^{i})={\rm LSTMCell}(c,p^{(i-1)},\phi^{(i-1)},\psi^{(i-1)})\]
            </li>
            <li>其中\(\phi\)和\(\psi\)是输出和cell state，\(\alpha\)和\(\beta\)是仿射变换的参数，角标\((i)\)代表迭代的<em>i</em>-th step</li>
            <li>点\(p\)的更新：\(p^{(i)}=p^{(i-1)}+(\alpha^{(i)} p^{(i-1)}+\beta^{(i)})\)</li>
            <li>迭代重复S=8次，得到最终的warping的输出</li>
          </ul>
        </li>
        <li>训练loss
          <ul>
            <li>
<strong>reconstruction loss</strong>
              <ul>
                <li>因为warping函数是迭代的，从 <code class="language-plaintext highlighter-rouge">Curriculum deepsdf, Yueqi Duan et al.2020</code>得到启发，用progressive reconstruction loss</li>
                <li><img src="media/image-20201229175034719.png" alt="image-20201229175034719"></li>
              </ul>
            </li>
            <li>
<strong>regularization loss</strong>
              <ul>
                <li>
<u>point-wise regularization</u>
                  <ul class="task-list">
                    <li>认为 <strong><u>所有</u></strong> meshes都normlized 到一个<code class="language-plaintext highlighter-rouge">单位球</code>，并和<code class="language-plaintext highlighter-rouge">canonical pose</code>对齐</li>
                    <li>因此，引入一个逐点的loss，通过 ==<strong><u>约束每个点的在warping前后的变化</u></strong>== 来实现这种正则化</li>
                    <li><img src="media/image-20201229175243291.png" alt="image-20201229175243291"></li>
                    <li>
<a href="https://en.wikipedia.org/wiki/Huber_loss">Huber kernel</a>：原点附近是二次函数，以外是线性函数</li>
                    <li class="task-list-item">
<input type="checkbox" class="task-list-item-checkbox" disabled>Q: 这样似乎只能保证canonical pose对齐，并不能保证canonical space具有单位大小
                      <ul>
                        <li>A: 笔者推测：用泛泛的位置变化的大小，来提供一种对所有物体的表征都处于canonical pose的约束；</li>
                      </ul>
                    </li>
                  </ul>
                </li>
                <li>
<u>point pair regularization</u> 对空间扭曲程度的限制
                  <ul class="task-list">
                    <li>尽管deform时空间扭曲是不可避免的，极端的空间扭曲还是可以避免的</li>
                    <li><img src="media/image-20201229175618093.png" alt="image-20201229175618093"></li>
                    <li class="task-list-item">
<input type="checkbox" class="task-list-item-checkbox" disabled>其中，\(\Delta p=T(p,c)-p\)是点p的position shift，<br>\(\epsilon = 0.5\)是控制扭曲容忍度的参数，对于防止shape collapse（形状崩塌，指学到过于简单的shape template）很关键</li>
                    <li class="task-list-item">笔者理解：距离越接近的一对点，position shift的差距(大小差距)应越小；即，距离越接近的一对点，变形的差距应越小
                      <ul class="task-list">
                        <li class="task-list-item">
<input type="checkbox" class="task-list-item-checkbox" disabled>Q: 考虑这里只有模的差距？如果考虑方向的差距，是否对法向量也会有一定的约束？<br>A: 注意这里是”位移向量”的方向差距，不是”法向量”的方向差距</li>
                      </ul>
                    </li>
                    <li class="task-list-item">下图是在有无此loss的情况下学到的template；<br>可见，如果没有point pair regularization，会学到过于简单的template<br><img src="media/image-20201229175909233.png" alt="image-20201229175909233">
</li>
                  </ul>
                </li>
              </ul>
            </li>
          </ul>
        </li>
      </ul>
    </li>
    <li>
<strong>results</strong>
      <ul>
        <li>形状补间的效果：<br><img src="media/image-20201229180146944.png" alt="image-20201229180146944">
</li>
        <li>因为已经建立起了shape correspondense，可以做关键点检测的迁移<br><img src="media/image-20201229180103315.png" alt="image-20201229180103315">
</li>
        <li>应用：texture transfer，等<br><img src="media/image-20201229172342590.png" alt="image-20201229172342590">
</li>
      </ul>
    </li>
  </ul>

</details>

<hr>

<p><strong><code class="language-plaintext highlighter-rouge">"Learning to Infer Implicit Surfaces without 3D Supervision"</code></strong><br>
<strong>[</strong> <code class="language-plaintext highlighter-rouge">NeuIPS2019</code> <strong>]</strong> <strong><a href="https://papers.nips.cc/paper/2019/file/bdf3fd65c81469f9b74cedd497f2f9ce-Paper.pdf">[paper]</a></strong>  <strong>[</strong> <img class="emoji" title=":mortar_board:" alt=":mortar_board:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f393.png" height="20" width="20"> <code class="language-plaintext highlighter-rouge">University of Southern California</code> <strong>]</strong> <br>
<strong>[</strong>  <code class="language-plaintext highlighter-rouge">Shichen Liu</code>, <code class="language-plaintext highlighter-rouge">Shunsuke Saito</code>, <code class="language-plaintext highlighter-rouge">Weikai Chen</code>, <code class="language-plaintext highlighter-rouge">Hao Li</code>  <strong>]</strong><br>
<strong>[</strong> <em><code class="language-plaintext highlighter-rouge">ray casting</code></em> <strong>]</strong></p>

<details>
  <summary>Click to expand</summary>

  <ul>
    <li>
<strong>Motivation</strong>
      <ul>
        <li><img src="media/image-20201207222950762.png" alt="image-20201207222950762"></li>
        <li>implicit occupancy field<img src="media/image-20201207195643392.png" alt="image-20201207195643392">
</li>
      </ul>
    </li>
  </ul>

</details>

<hr>

<p><strong><code class="language-plaintext highlighter-rouge">&lt;IGR&gt; "Implicit Geometric Regularization for Learning Shapes"</code></strong><br>
<strong>[</strong> <code class="language-plaintext highlighter-rouge">ICML2020</code> <strong>]</strong> <strong><a href="https://arxiv.org/pdf/2002.10099.pdf">[paper]</a></strong> <strong><a href="https://github.com/amosgropp/IGR">[code]</a></strong> <strong><a href="https://youtu.be/6cOvBGBQF9g">[video]</a></strong> <strong>[</strong> <img class="emoji" title=":mortar_board:" alt=":mortar_board:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f393.png" height="20" width="20"> <code class="language-plaintext highlighter-rouge">Weizmann Institute of Science</code> <strong>]</strong> <br>
<strong>[</strong>  <code class="language-plaintext highlighter-rouge">Amos Gropp</code>, <code class="language-plaintext highlighter-rouge">Lior Yariv</code>, <code class="language-plaintext highlighter-rouge">Niv Haim</code>, <code class="language-plaintext highlighter-rouge">Matan Atzmon</code>, <code class="language-plaintext highlighter-rouge">Yaron Lipman</code>  <strong>]</strong><br>
<strong>[</strong> <em><code class="language-plaintext highlighter-rouge">SDF</code>, <code class="language-plaintext highlighter-rouge">Implicit geometrical regularization</code></em> <strong>]</strong></p>

<details>
  <summary>Click to expand</summary>

  <ul>
    <li>
<strong>Motivation</strong>
      <ul>
        <li>从raw 点云中直接学习DeepSDF，在with or without 法向量数据的情况下</li>
        <li>用隐式的shape先验，就可以获得plausible solutions<br>其实就是简单的loss函数，鼓励输入点云处的函数值为0，鼓励空间散布的点的梯度是单位模梯度<br><img src="media/image-20201228164827136.png" alt="image-20201228164827136">
</li>
      </ul>
    </li>
    <li>
<strong>overview</strong>
      <ul>
        <li>given raw input pointcloud \(\mathcal{X}=\{x_i\}_{i\in I} \subset \mathbb{R}^3\),  <u>**with or without normal**</u> data \(\mathcal{N}=\{n_i\}_{i\in I} \subset \mathbb{R}^3\)，从中学出一个 <strong><u>plausible</u></strong> 的surface \(\mathcal{M}\)</li>
        <li>学SDF时的常规loss：<br>有数据处函数值为0，法向量为真值；<br>(无数据处)空间分布的点法向量2-norm为1<br><img src="media/image-20201228172709924.png" alt="image-20201228172709924">
</li>
        <li>然而只有上述loss存在问题
          <ul>
            <li>首先，不能保证学到的是SDF</li>
            <li>其次，即使能学到SDF，也不能保证学到的是一个 <strong><u>plausible one</u></strong>
</li>
          </ul>
        </li>
        <li>本篇通过理论证明，如果对上述loss使用梯度下降算法，，就可以避免bad critical solutions
          <ul>
            <li>是从平面的线性问题考虑的，把这种属性叫做 <strong><u>plane reduction</u></strong> <br><img src="media/image-20201228173842422.png" alt="image-20201228173842422">
</li>
          </ul>
        </li>
      </ul>
    </li>
  </ul>

</details>

<hr>

<p><strong><code class="language-plaintext highlighter-rouge">"MeshSDF: Differentiable Iso-Surface Extraction"</code></strong><br>
<strong>[</strong> <code class="language-plaintext highlighter-rouge">NeurIPS2020</code> <strong>]</strong> <strong><a href="https://arxiv.org/pdf/2006.03997.pdf">[paper]</a></strong> <strong><a href="https://github.com/cvlab-epfl/MeshSDF">[code]</a></strong> <strong>[</strong> <img class="emoji" title=":mortar_board:" alt=":mortar_board:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f393.png" height="20" width="20"> <code class="language-plaintext highlighter-rouge">EPFL</code> <strong>]</strong> <strong>[</strong> <img class="emoji" title=":office:" alt=":office:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f3e2.png" height="20" width="20"> <code class="language-plaintext highlighter-rouge">Neuralconcept</code>, <code class="language-plaintext highlighter-rouge">Intel</code> <strong>]</strong><br>
<strong>[</strong>  <code class="language-plaintext highlighter-rouge">Edoardo Remelli</code>, <code class="language-plaintext highlighter-rouge">Pascal Fua</code>   <strong>]</strong><br>
<strong>[</strong> <em><code class="language-plaintext highlighter-rouge">differentiable iso-surface extraction</code>, <code class="language-plaintext highlighter-rouge">marching cubes</code>, <code class="language-plaintext highlighter-rouge">SDF</code></em>  <strong>]</strong></p>

<p>[differentiable iso-surface extraction]</p>

<details>
  <summary>Click to expand</summary>

  <ul>
    <li>
<strong>review</strong>
      <ul>
        <li>和DVR思路类似，首先手动推导出表面点坐标对网络参数的梯度，实际计算时就可以先用采样-based方法得出点坐标，再代入手动推导出的梯度式子构成完整的反向传播链路</li>
        <li>手动推导表面点坐标对网络参数的梯度过程中，用到了SDF的特殊性质（某一点函数值的梯度就是该点的法向量），不适用于一般性implicit occupancy field</li>
      </ul>
    </li>
  </ul>

</details>

<hr>

<p><strong><code class="language-plaintext highlighter-rouge">"PatchNets: Patch-Based Generalizable Deep Implicit 3D Shape Representations"</code></strong><br>
<strong>[</strong> <code class="language-plaintext highlighter-rouge">ECCV2020</code> <strong>]</strong> <strong><a href="https://arxiv.org/pdf/2008.01639.pdf">[paper]</a></strong>  <strong><a href="http://gvv.mpi-inf.mpg.de/projects/PatchNets/data/patchnets_slides.pdf">[slice]</a></strong> <strong>[</strong> <img class="emoji" title=":mortar_board:" alt=":mortar_board:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f393.png" height="20" width="20"> <code class="language-plaintext highlighter-rouge">MPI</code> <strong>]</strong> <strong>[</strong> <img class="emoji" title=":office:" alt=":office:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f3e2.png" height="20" width="20"> <code class="language-plaintext highlighter-rouge">facebook</code> <strong>]</strong><br>
<strong>[</strong>  <code class="language-plaintext highlighter-rouge">Edgar Tretschk</code>, <code class="language-plaintext highlighter-rouge">Ayush Tewari</code>, <code class="language-plaintext highlighter-rouge">Vladislav Golyanik</code>, <code class="language-plaintext highlighter-rouge">Michael Zollhöfer</code>, <code class="language-plaintext highlighter-rouge">Carsten Stoll</code>, <code class="language-plaintext highlighter-rouge">Christian Theobalt</code>  <strong>]</strong><br>
<strong>[</strong> <em><code class="language-plaintext highlighter-rouge">implicit functions</code>, <code class="language-plaintext highlighter-rouge">patch-based surface representation</code>, <code class="language-plaintext highlighter-rouge">SDF</code></em> <strong>]</strong></p>

<details>
  <summary>Click to expand</summary>

  <ul>
    <li>
<strong>Motivation</strong>
      <ul>
        <li>mid-level patch-based SDF</li>
        <li>因为在patch层次，不同类别的物体有相似性，用上这种相似性就可以做更泛化的模型</li>
        <li>在一个canonical space下学到这些patch-based representation</li>
        <li>从ShapeNet的一个类别学出来的representation，可以用于表征任何一个其他类别的非常细节的shapes；并且可以用更少的shape来训练</li>
        <li><img src="media/image-20201217115545052.png" alt="image-20201217115545052"></li>
      </ul>
    </li>
    <li>
<strong>Overview</strong>
      <ul>
        <li>auto-decoder</li>
        <li>losses：重建loss和patch extrinsics的guidance loss，还有regularization</li>
      </ul>
    </li>
    <li>
<strong>extrinsic loss</strong>
      <ul>
        <li>这个loss保证所有的patch都对surface有贡献，并且处于caonical space</li>
        <li>第<em>i</em>个物体的patch extrinsics: \(\boldsymbol{e}_i=[\boldsymbol{e}_{i,0},\boldsymbol{e}_{i,1},\ldots,\boldsymbol{e}_{i,N_P-1}]\)</li>
        <li>
\[\mathcal{L}_{ext}(\boldsymbol{e}_i) = \mathcal{L}_{sur}(\boldsymbol{e}_i) + \mathcal{L}_{cov}(\boldsymbol{e}_i) + \mathcal{L}_{rot}(\boldsymbol{e}_i) + \mathcal{L}_{scl}(\boldsymbol{e}_i) + \mathcal{L}_{var}(\boldsymbol{e}_i)\]
        </li>
        <li>\(\mathcal{L}_{sur}(\boldsymbol{e}_i)\) 保证每个patch都离surface很近
          <ul>
            <li>
\[\underset{逐patch}{\max}[surface上的所有点到该patch距离的最小值]\]
            </li>
          </ul>
        </li>
        <li>\(\mathcal{L}_{cov}(\boldsymbol{e}_i)\) symmetric coverage loss，鼓励surface上的每个点都至少被一个patch涵盖</li>
        <li>\(\mathcal{L}_{rot}(\boldsymbol{e}_i)\) 把patches和surface normals对齐</li>
        <li>\(\mathcal{L}_{scl}(\boldsymbol{e}_i)\) 鼓励patches to be reasonably small，防止不同patch之间显著的重叠</li>
        <li>\(\mathcal{L}_{var}(\boldsymbol{e}_i)\) 鼓励所有patch大小相似</li>
      </ul>
    </li>
    <li>
<strong>result</strong>
      <ul>
        <li><img src="media/image-20201217122000974.png" alt="image-20201217122000974"></li>
        <li><img src="media/image-20201217122618035.png" alt="image-20201217122618035"></li>
      </ul>
    </li>
  </ul>

</details>

<hr>

<p><strong><code class="language-plaintext highlighter-rouge">&lt;OverfitSDF&gt; "Overfit Neural Networks as a Compact Shape Representation"</code></strong><br>
<strong>[</strong> <code class="language-plaintext highlighter-rouge">2020</code> <strong>]</strong> <strong><a href="https://arxiv.org/pdf/2009.09808.pdf">[paper]</a></strong> <strong><a href="https://github.com/nathanrgodwin/overfit-shapes">[code]</a></strong> <strong>[</strong> <img class="emoji" title=":mortar_board:" alt=":mortar_board:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f393.png" height="20" width="20"> <code class="language-plaintext highlighter-rouge">University of Toronto</code>, <code class="language-plaintext highlighter-rouge">McGill University</code> <strong>]</strong> <br>
<strong>[</strong>  <code class="language-plaintext highlighter-rouge">Thomas Davies</code>, <code class="language-plaintext highlighter-rouge">Derek Nowrouzezahrai</code>, <code class="language-plaintext highlighter-rouge">Alec Jacobson</code>  <strong>]</strong><br>
<strong>[</strong> <em><code class="language-plaintext highlighter-rouge">compact representation</code></em> <strong>]</strong></p>

<details>
  <summary>Click to expand</summary>

  <ul>
    <li>
<strong>Motivation</strong>
      <ul>
        <li>现在的DeepSDF倾向于做category类别的泛化/生成；</li>
        <li>本篇主要提出其实overfit到一个具体的shape的SDF可以作为mesh的一种更<code class="language-plaintext highlighter-rouge">compact</code>紧致的表征，而且相比于显式地mesh更省空间</li>
        <li>同时，做了很多具体shape optimization的优化，比如采样时基于重要度采样，一些biased points，等</li>
        <li><img src="media/image-20201223092223714.png" alt="image-20201223092223714"></li>
      </ul>
    </li>
  </ul>

</details>

<h3 id="initialization--priors-for-auto-decoders">initialization / priors for auto-decoders</h3>

<hr>

<p><strong><code class="language-plaintext highlighter-rouge">"MetaSDF: Meta-learning Signed Distance Functions"</code></strong><br>
<strong>[</strong> <code class="language-plaintext highlighter-rouge">NeurIPS2020</code> <strong>]</strong> <strong><a href="https://arxiv.org/pdf/2006.09662.pdf">[paper]</a></strong> <strong><a href="https://github.com/vsitzmann/metasdf">[code]</a></strong> <strong>[</strong> <img class="emoji" title=":mortar_board:" alt=":mortar_board:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f393.png" height="20" width="20"> <code class="language-plaintext highlighter-rouge">Stanford</code> <strong>]</strong> <strong>[</strong> <img class="emoji" title=":office:" alt=":office:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f3e2.png" height="20" width="20"> <code class="language-plaintext highlighter-rouge">Google</code> <strong>]</strong><br>
<strong>[</strong>  <code class="language-plaintext highlighter-rouge">Vincent Sitzmann</code>, <code class="language-plaintext highlighter-rouge">Eric R. Chan</code>, <code class="language-plaintext highlighter-rouge">Richard Tucker</code>, <code class="language-plaintext highlighter-rouge">Noah Snavely</code>, <code class="language-plaintext highlighter-rouge">Gordon Wetzstein</code>  <strong>]</strong><br>
<strong>[</strong> <em><code class="language-plaintext highlighter-rouge">meta-learning</code>, <code class="language-plaintext highlighter-rouge">SDF</code></em> <strong>]</strong></p>

<details>
  <summary>Click to expand</summary>

  <ul>
    <li>
<strong>Review</strong>
      <ul>
        <li>DeepSDF / deep implicit field类方法往往都喜欢用auto-decoder，因为set-encoder有欠拟合的问题</li>
        <li>auto-decoder 在测试时也需要infer，需要很多步迭代，infer一次比较耗时<br>因此用meta-learning（MAML类）找出一个合适的auto-decoder优化的初值code<br>这样在测试时infer就只需要少量步数的迭代就可以得到很好的效果</li>
        <li><img src="media/image-20201210102054355.png" alt="image-20201210102054355"></li>
      </ul>
    </li>
  </ul>

</details>

<hr>

<p><strong><code class="language-plaintext highlighter-rouge">"Learned Initializations for Optimizing Coordinate-Based Neural Representations"</code></strong><br>
<strong>[</strong> <code class="language-plaintext highlighter-rouge">arXiv2020</code> <strong>]</strong> <strong><a href="https://arxiv.org/pdf/2012.02189.pdf">[paper]</a></strong>  <strong>[</strong> <img class="emoji" title=":mortar_board:" alt=":mortar_board:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f393.png" height="20" width="20"> <code class="language-plaintext highlighter-rouge">UCB</code> <strong>]</strong> <strong>[</strong> <img class="emoji" title=":office:" alt=":office:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f3e2.png" height="20" width="20"> <code class="language-plaintext highlighter-rouge">Google</code> <strong>]</strong><br>
<strong>[</strong>  <code class="language-plaintext highlighter-rouge">Matthew Tancik</code>, <code class="language-plaintext highlighter-rouge">Ben Mildenhall</code>, <code class="language-plaintext highlighter-rouge">Terrance Wang</code>, <code class="language-plaintext highlighter-rouge">Divi Schmidt</code>, <code class="language-plaintext highlighter-rouge">Pratul P. Srinivasan</code>, <code class="language-plaintext highlighter-rouge">Jonathan T. Barron</code>, <code class="language-plaintext highlighter-rouge">Ren Ng</code>  <strong>]</strong><br>
<strong>[</strong> <em><code class="language-plaintext highlighter-rouge">meta-learning</code></em> <strong>]</strong></p>

<details>
  <summary>Click to expand</summary>

  <ul>
    <li>
<strong>Motivation</strong>
      <ul>
        <li>对于coordinate-based neural representations在auto-decoder时，用meta-learned 的initialization</li>
        <li><img src="media/image-20201215193905125.png" alt="image-20201215193905125"></li>
        <li>与MetaSDF的差别：进一步拓展到更多种类的neural coordinate-based signals，并且把the power of using initial weight settings开发为一种先验信息</li>
      </ul>
    </li>
  </ul>

</details>

<hr>

<p><strong><code class="language-plaintext highlighter-rouge">"Deep Optimized Priors for 3D Shape Modeling and Reconstruction"</code></strong><br>
<strong>[</strong> <code class="language-plaintext highlighter-rouge">arXiv2020</code> <strong>]</strong> <strong><a href="https://arxiv.org/pdf/2012.07241.pdf">[paper]</a></strong>  <strong>[</strong> <img class="emoji" title=":mortar_board:" alt=":mortar_board:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f393.png" height="20" width="20"> <code class="language-plaintext highlighter-rouge">南方科技大学</code>, <strong>]</strong> <strong>[</strong> <img class="emoji" title=":office:" alt=":office:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f3e2.png" height="20" width="20"> <code class="language-plaintext highlighter-rouge">Tencent America</code> <strong>]</strong><br>
<strong>[</strong>  <code class="language-plaintext highlighter-rouge">Mingyue Yang</code>, <code class="language-plaintext highlighter-rouge">Yuxin Wen</code>, <code class="language-plaintext highlighter-rouge">Weikai Chen</code>, <code class="language-plaintext highlighter-rouge">Yongwei Chen</code>, <code class="language-plaintext highlighter-rouge">Kui Jia贾奎</code>  <strong>]</strong><br>
<strong>[</strong> <em><code class="language-plaintext highlighter-rouge">better shape priors</code></em> <strong>]</strong></p>

<details>
  <summary>Click to expand</summary>

  <ul>
    <li>
<strong>Motivation</strong>
      <ul>
        <li>现有的很多方法test time都是从fixed trained priors</li>
        <li>本篇提出在training以后，仍然从physical measurements进一步最优化learned prior</li>
      </ul>
    </li>
  </ul>

</details>

<hr>

<p><strong><code class="language-plaintext highlighter-rouge">"Iso-Points: Optimizing Neural Implicit Surfaces with Hybrid Representations"</code></strong><br>
<strong>[</strong> <code class="language-plaintext highlighter-rouge">arXiv2020</code> <strong>]</strong> <strong><a href="https://arxiv.org/pdf/2012.06434.pdf">[paper]</a></strong> <strong><a href="https://yifita.github.io/project/neural-shape/">[web]</a></strong> <strong>[</strong> <img class="emoji" title=":mortar_board:" alt=":mortar_board:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f393.png" height="20" width="20"> <code class="language-plaintext highlighter-rouge">ETH</code>, <code class="language-plaintext highlighter-rouge">cambridge</code> <strong>]</strong> <br>
<strong>[</strong>  <code class="language-plaintext highlighter-rouge">Wang Yifan</code>, <code class="language-plaintext highlighter-rouge">Shihao Wu</code>, <code class="language-plaintext highlighter-rouge">Cengiz Oztireli</code>, <code class="language-plaintext highlighter-rouge">Olga Sorkine-Hornung</code>  <strong>]</strong><br>
<strong>[</strong> <em><code class="language-plaintext highlighter-rouge">iso-points</code>, <code class="language-plaintext highlighter-rouge">hybrid representation</code>, <code class="language-plaintext highlighter-rouge">SDF</code></em> <strong>]</strong></p>

<details>
  <summary>Click to expand</summary>

  <ul>
    <li>
<strong>Motivation</strong>
      <ul>
        <li>目前这些输入点云deep implicit field学surface的方法，optimizing时，精确、鲁棒的重建仍然非常有挑战性</li>
        <li>本篇提出用等值面上的点作为一个额外的显式表征；被计算、更新on-the-fly，有效提高收敛率和最终质量</li>
      </ul>
    </li>
    <li>
<strong>overview</strong>
      <ul>
        <li>目标：给定一个neural implicit function \(f_t(\boldsymbol{\rm p};\theta_t)\) at <em>t</em>-th iteration，efficiently generate and utilize 一组<code class="language-plaintext highlighter-rouge">稠密的、均匀分布的iso-points</code> (points at zero level set)
          <ul>
            <li>这组iso-points可以用于
              <ul>
                <li>改进training data的sampling</li>
                <li>提供最优化时的regularization</li>
              </ul>
            </li>
          </ul>
        </li>
      </ul>
    </li>
    <li>
<strong>iso-surface sampling</strong>：如何得到iso-surface上均匀分布的点
      <ul>
        <li><img src="media/image-20201223112811012.png" alt="image-20201223112811012"></li>
        <li>
<strong>projection</strong>：projecing a point onto the iso-surface 可以被视作 在一个给定点用牛顿法估计一个方程的根
          <ul>
            <li>考虑这里和贾奎那篇analytic marching算法初始找到表面上一个点的思路是很像的</li>
            <li>给定隐函数\(f(\boldsymbol{\rm p}): \mathbb{R}^3\rightarrow \mathbb{R}\)，初始点\(\boldsymbol{\rm q}_0\in\mathbb{R}^3\)<br>牛顿法求根：\(\boldsymbol{\rm q}_{k+1}=\boldsymbol{\rm q}_{k}-J_f(\boldsymbol{\rm q}_k)^+ f(\boldsymbol{\rm q}_k)\), where \(J_f(\boldsymbol{\rm q}_k)^+\)是Jacobian的Moore-Penrose 伪逆</li>
            <li>\(J_f\)是一个row 3-vector，所以\(J_f(\boldsymbol{\rm q}_k)^+ = \frac {J_f^{\top}(\boldsymbol{\rm q}_k)} {\lVert J_f(\boldsymbol{\rm q}_k) \rVert^2}\), where \(J_f(\boldsymbol{\rm q}_k)\) 可以直接通过反向传播计算</li>
            <li>不过，由于一些同时代的工作常采用sine activation functions或者positional encoding，SDF噪声很大，梯度高度non-smooth，直接使用牛顿法会导致overshooting和oscillation</li>
            <li>当然可以用一些更精致的line search算法，不过这里直接用简单的clipping操作</li>
            <li>点\(\mathcal{Q}_t\)集合的初始化：刚开始就用一个unit sphere shape初始化，后面用\(\mathcal{Q}_{t-1}\)初始化</li>
            <li>最大10个牛顿迭代，停止阈值从\(10^{-4}\)逐渐缩小到\(10^{-5}\)</li>
          </ul>
        </li>
        <li>
<strong>uniform resampling</strong>
          <ul>
            <li>迭代地把点从high-density regions移开</li>
            <li>这步和f没有关系了，移开的方向都是由邻居点定义的</li>
          </ul>
        </li>
        <li>
<strong>upsampling</strong>
          <ul>
            <li>基于 <code class="language-plaintext highlighter-rouge">EAR</code> (edge-aware resampling)
              <ul>
                <li><em>Edge-aware point set resampling, SIGGRAPH Asia 2013</em></li>
              </ul>
            </li>
          </ul>
        </li>
      </ul>
    </li>
    <li>
      <p><strong>results</strong></p>

      <ul>
        <li><img src="media/image-20201229153051697.png" alt="image-20201229153051697"></li>
      </ul>
    </li>
  </ul>

</details>

<h3 id="differentiable-renderer">differentiable renderer</h3>

<ul>
  <li>keyword
    <ul>
      <li>DIST and its citations</li>
    </ul>
  </li>
</ul>

<hr>

<p><strong><code class="language-plaintext highlighter-rouge">&lt;DVR&gt; "Differentiable Volumetric Rendering: Learning Implicit 3D Representations without 3D Supervision"</code></strong><br>
<strong>[</strong> <code class="language-plaintext highlighter-rouge">CVPR2020</code> <strong>]</strong> <strong><a href="https://arxiv.org/pdf/1912.07372.pdf">[paper]</a></strong> <strong><a href="https://github.com/autonomousvision/differentiable_volumetric_rendering">[code]</a></strong> <strong>[</strong> <img class="emoji" title=":mortar_board:" alt=":mortar_board:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f393.png" height="20" width="20"> <code class="language-plaintext highlighter-rouge">MPI</code>, <code class="language-plaintext highlighter-rouge">University of Tubingen</code> <strong>]</strong> <br>
<strong>[</strong> <code class="language-plaintext highlighter-rouge">Michael Niemeyer</code>, <code class="language-plaintext highlighter-rouge">Andreas Geiger</code> <strong>]</strong><br>
<strong>[</strong>  <em><code class="language-plaintext highlighter-rouge">differentiable volumetric rendering</code>, <code class="language-plaintext highlighter-rouge">ray casting</code></em> <strong>]</strong></p>

<details>
  <summary>Click to expand</summary>

  <ul>
    <li>
<strong>review</strong>
      <ul>
        <li>思路：首先手动推导出每个camera ray和隐表面交点的点坐标对网络参数的梯度，在实际计算时，就可以先在camera ray上采样得出交点坐标(类似二分法)，然后代入所手动推导出的式子构成完整的反向传播链路</li>
      </ul>
    </li>
  </ul>

</details>

<hr>

<p><strong><code class="language-plaintext highlighter-rouge">"DIST: Rendering Deep Implicit Signed Distance Function with Differentiable Sphere Tracing"</code></strong><br>
<strong>[</strong> <code class="language-plaintext highlighter-rouge">CVPR2020</code> <strong>]</strong> <strong><a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Liu_DIST_Rendering_Deep_Implicit_Signed_Distance_Function_With_Differentiable_Sphere_CVPR_2020_paper.pdf">[paper]</a></strong> <strong><a href="https://github.com/B1ueber2y/DIST-Renderer">[code]</a></strong> <strong><a href="http://b1ueber2y.me/projects/DIST-Renderer/">[web]</a></strong> <strong><a href="http://b1ueber2y.me/projects/DIST-Renderer/dist-slides.pdf">[slice]</a></strong> <strong>[</strong> <img class="emoji" title=":mortar_board:" alt=":mortar_board:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f393.png" height="20" width="20"> <code class="language-plaintext highlighter-rouge">ETH</code>, <code class="language-plaintext highlighter-rouge">Tsinghua University</code>, <code class="language-plaintext highlighter-rouge">Peking University</code>, <code class="language-plaintext highlighter-rouge">MPI</code> <strong>]</strong> <strong>[</strong> <img class="emoji" title=":office:" alt=":office:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f3e2.png" height="20" width="20"> <code class="language-plaintext highlighter-rouge">Google</code>, <code class="language-plaintext highlighter-rouge">Peng Cheng Laboratory</code> <strong>]</strong><br>
<strong>[</strong>  <code class="language-plaintext highlighter-rouge">Shaohui Liu</code>, <code class="language-plaintext highlighter-rouge">Yinda Zhang</code>, <code class="language-plaintext highlighter-rouge">Songyou Peng</code>, <code class="language-plaintext highlighter-rouge">Boxin Shi</code>, <code class="language-plaintext highlighter-rouge">Marc Pollefeys</code>, <code class="language-plaintext highlighter-rouge">Zhaopeng Cui</code>  <strong>]</strong><br>
<strong>[</strong> <em><code class="language-plaintext highlighter-rouge">SDF</code>, <code class="language-plaintext highlighter-rouge">differentiable renderer</code>, <code class="language-plaintext highlighter-rouge">sphere tracing</code></em> <strong>]</strong></p>

<details>
  <summary>Click to expand</summary>

  <ul>
    <li>需要silhouette真值</li>
    <li>
<strong>Motivation</strong>
      <ul>
        <li>给SDF加上一个differentiable renderer，来为inverse graphics models和deep implicit surface field建设桥梁</li>
        <li>solving vision problem as inverse graphics process is one of the foundamental approaches, where the solution is the visual structure that best explains given observations 把视觉问题看做逆向图形学过程来解决；寻找能最好地解释给定观测的视觉结构
          <ul>
            <li>3D geometry理解 领域：很早就被使用(1974, 1999, etc.)</li>
            <li>常常需要一个高效的renderer来从从一个optimizable 的3D结构 精确地simulate这些观测(e.g. depth maps)，同时需要是可微的，来反向传播局部观测的误差</li>
            <li>(first) a differentiable renderer for learning-based SDF</li>
          </ul>
        </li>
        <li>用一个可微分的renderer来把learning-based SDF可微分地渲染为 depth image, surface normal, silhouettes，从任意相机viewpoints</li>
        <li>应用：可用于infer 3D shape from various inputs, e.g. multi-view images and single depth image</li>
      </ul>
    </li>
    <li>
<strong>overview</strong>
      <ul>
        <li><img src="media/image-20201215111010407.png" alt="image-20201215111010407"></li>
        <li>[auto-decoder] 给定一个已经pre-trained generative model, e.g. DeepSDF, 通过在latent code space 寻找能产生和给定观测最一致的3D shape</li>
        <li><img src="media/image-20210105074006003.png" alt="image-20210105074006003"></li>
      </ul>
    </li>
    <li>[sphere tracing] 使用一个类似sphere tracing的框架来做可微分的渲染
      <ul>
        <li>直接应用sphere tracing因为需要对network做反复的query并且在反向传播时产生递归的计算图（笔者注：就像SRN那样），计算费时、费内存；所以需要对前向传播和反向传播过程都要做出优化</li>
        <li>sphere-traced results (i.e. camera ray上的距离)，可以用于产生各种输出，如<u>深度图</u>、<u>表面法向量</u>、<u>轮廓</u>等，因此可以用loss来方便地形成端到端的manner</li>
        <li>前向通路</li>
        <li><img src="media/image-20210105073826931.png" alt="image-20210105073826931"></li>
      </ul>
    </li>
    <li>
<img src="media/image-20201215164421303.png" alt="image-20201215164421303">
      <ul>
        <li>用一种coarse-to-fine的方法来save computation at initial steps
          <ul>
            <li>考虑到在sphere tracing的前面几步，不同pixel的ray都非常接近</li>
            <li>从图像的1/4分辨率开始tracing，然后每3步以后把每个像素分成4份</li>
            <li>在6步后，full resolution下的每个像素都有一个对应的ray，一直marching直到收敛</li>
          </ul>
        </li>
        <li>一个aggresive 策略来加速ray marching
          <ul>
            <li>marching步长是\(\alpha=1.5\)倍的queried SDF value</li>
            <li>在距离表面很远的时候更快地朝表面march</li>
            <li>在ill-posed情况下能加速收敛（当表面法向量和ray direction的夹角很小时）
              <ul class="task-list">
                <li class="task-list-item">
<input type="checkbox" class="task-list-item-checkbox" disabled>Q: what?</li>
              </ul>
            </li>
            <li>ray可以射穿表面，能够采样到表面内部(SDF&lt;0)；对表面的两侧都可以应用supervision</li>
          </ul>
        </li>
        <li>dynamic synchronized inference</li>
        <li>一个safe convergence criteria来防止不必要的网络query，同时保留分辨率</li>
      </ul>
    </li>
    <li>反向传播
      <ul>
        <li>用SDF的梯度的近似值，对训练影响不大，但是显著减少计算和内存占用</li>
      </ul>
    </li>
    <li>
<strong>评价：文中出现了非常多技术细节的详细解释，值得一读</strong>
      <ul>
        <li>sphere tracing<br><img src="media/image-20201215111200177.png" alt="image-20201215111200177">
</li>
        <li>训练一个神经网络，同时为每个3D location 预测signed distance 和color</li>
      </ul>
    </li>
    <li>
<strong>实验</strong>
      <ul>
        <li>收敛速度<br><img src="media/image-20201215112912115.png" alt="image-20201215112912115">
</li>
        <li>
<strong>Texture Re-rendering</strong><br><img src="media/image-20201215114347510.png" alt="image-20201215114347510">
</li>
        <li>
<strong>Shape Completion from Sparse Depths</strong><br><img src="media/image-20201215114703816.png" alt="image-20201215114703816">
</li>
        <li>
<strong>Shape Completion over Different Sparsity</strong><br><img src="media/image-20201215114227972.png" alt="image-20201215114227972">
</li>
        <li>
<strong>Inverse Optimization over Camera Extrinsics</strong><br><img src="media/image-20201215113343946.png" alt="image-20201215113343946">
</li>
        <li>
<strong>Multi-view Reconstruction from Video Sequences 从多视角视频序列重建</strong><br><img src="media/image-20201215115145581.png" alt="image-20201215115145581">
</li>
      </ul>
    </li>
  </ul>

</details>

<hr>

<p><strong><code class="language-plaintext highlighter-rouge">"SDF-SRN: Learning Signed Distance 3D Object Reconstruction from Static Images"</code></strong><br>
<strong>[</strong> <code class="language-plaintext highlighter-rouge">NeurIPS2020</code> <strong>]</strong> <strong><a href="https://papers.nips.cc/paper/2020/file/83fa5a432ae55c253d0e60dbfa716723-Paper.pdf">[paper]</a></strong> <strong><a href="https://github.com/chenhsuanlin/signed-distance-SRN">[code]</a></strong> <strong><a href="https://chenhsuanlin.bitbucket.io/signed-distance-SRN/">[web]</a></strong> <strong>[</strong> <img class="emoji" title=":mortar_board:" alt=":mortar_board:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f393.png" height="20" width="20"> <code class="language-plaintext highlighter-rouge">CMU</code> <strong>]</strong> <br>
<strong>[</strong>  <code class="language-plaintext highlighter-rouge">Chen-Hsuan Lin</code>, <code class="language-plaintext highlighter-rouge">Chaoyang Wang</code>, <code class="language-plaintext highlighter-rouge">Simon Lucey</code>  <strong>]</strong><br>
<strong>[</strong> <em><code class="language-plaintext highlighter-rouge">single-view</code></em> <strong>]</strong></p>

<details>
  <summary>Click to expand</summary>

  <ul>
    <li>训练时需要single view silhouette</li>
    <li>
<strong>Motivation</strong>
      <ul>
        <li>单视角3D物体重建，过去的方法往往都有3D形状真值</li>
        <li>最近的方法可以没有3D监督信号，但是还是需要训练时多视角的对同个instance的silhouettes标注；因此大多只能应对合成数据集</li>
        <li>本篇提出SDF-SRN，只需要单视角图片(只在训练时+silhouette)输入<br><img src="media/image-20201221153940813.png" alt="image-20201221153940813">
</li>
      </ul>
    </li>
    <li>
<strong>overview</strong>
      <ul>
        <li>single-view一般需要encoder<br><img src="media/image-20201215173359177.png" alt="image-20201215173359177">
</li>
      </ul>
    </li>
    <li>
<strong>Results</strong>
      <ul>
        <li>学出的形状奇奇怪怪；不过总归是纯图片输入，而且只有训练时需要silhouette<br><img src="media/image-20201221153429857.png" alt="image-20201221153429857">
</li>
        <li>颜色重建的质量也一般<br><img src="media/image-20201221155058978.png" alt="image-20201221155058978">
</li>
      </ul>
    </li>
  </ul>

</details>

<hr>

<p><strong><code class="language-plaintext highlighter-rouge">"SDFDiff: Differentiable Rendering of Signed Distance Fields for 3D Shape Optimization"</code></strong><br>
<strong>[</strong> <code class="language-plaintext highlighter-rouge">CVPR2020(Oral)</code> <strong>]</strong> <strong><a href="https://arxiv.org/pdf/1912.07109.pdf">[paper]</a></strong> <strong><a href="https://github.com/YueJiang-nj/CVPR2020-SDFDiff">[code]</a></strong> <strong>[</strong> <img class="emoji" title=":mortar_board:" alt=":mortar_board:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f393.png" height="20" width="20"> <code class="language-plaintext highlighter-rouge">University of Maryland</code> <strong>]</strong> <br>
<strong>[</strong>  <code class="language-plaintext highlighter-rouge">Yue Jiang</code>, <code class="language-plaintext highlighter-rouge">Dantong Ji</code>, <code class="language-plaintext highlighter-rouge">Zhizhong Han</code>, <code class="language-plaintext highlighter-rouge">Matthias Zwicker</code>  <strong>]</strong><br>
<strong>[</strong> <em><code class="language-plaintext highlighter-rouge">SDF</code>, <code class="language-plaintext highlighter-rouge">differentiable rendering</code>, <code class="language-plaintext highlighter-rouge">multi-view</code>, <code class="language-plaintext highlighter-rouge">single-view</code>, <code class="language-plaintext highlighter-rouge">multi-resolution strategy</code></em> <strong>]</strong></p>

<details>
  <summary>Click to expand</summary>

  <ul>
    <li>需要分割好的多视角图片</li>
    <li>
<strong>Motivation</strong>
      <ul>
        <li>
<strong><code class="language-plaintext highlighter-rouge">&lt;u&gt;image-based shape optimization&lt;/u&gt;</code></strong> using differentiable rendering of 3D shapes represented by SDF
          <ul>
            <li>SDF作为形状表征的优势：可以表征具有任意拓扑的形状，并且可以保证watertight</li>
          </ul>
        </li>
      </ul>
    </li>
    <li>
<strong>Overview</strong>
      <ul>
        <li>learn SDF on a <code class="language-plaintext highlighter-rouge">3D grid</code>
</li>
        <li>perform ray-casting via <code class="language-plaintext highlighter-rouge">sphere tracing</code>
</li>
      </ul>
    </li>
    <li>
<strong>differentiable renderer</strong>
      <ul>
        <li>学到的是voxelized SDF，然后通过linear interpolation获取任意连续位置处的SDF</li>
        <li>给定像素值的导数只与interpolation时的8个邻居体素有关
          <ul>
            <li>或者说，sphere tracing本身不需要是可微分的</li>
            <li>只需要 local 8个邻居的 local 计算需要可微分</li>
          </ul>
        </li>
      </ul>
    </li>
    <li>
<strong>energy function &amp; losses</strong>
      <ul>
        <li>从geometry相机位置等\(\Theta\)，可以render出image\(I\)：\(I=R(\Theta)\)<br>inverse rendering就是\(\Theta=R^{-1}(I)\)<br>但是inverse rendering并不直接可逆，因此把问题建模为<code class="language-plaintext highlighter-rouge">energy minimization problem</code>能量最小问题<br>\(\Theta^*=\underset{\Theta}{\arg\min} \mathcal{L}_{img}(R(\Theta),I)\)</li>
        <li>重点在于一个differentiable renderer：本篇强调shape。输入camera pose和shape，输出渲染图像</li>
        <li>\(\mathcal{L}_{img}\)衡量render图像和\(I\)的差别</li>
        <li>\(\mathcal{L}_{reg}\) 正则化项，保证\(\Theta\)是一个valid signed distance field（i.e. 梯度是单位向量）<br>实践中，是用\(\Delta\)近似的梯度</li>
      </ul>
    </li>
    <li>single view：从图像encode到一个voxelized 稀疏SDF，经过一些3D卷积refinement，经过differentiable renderer到image<img src="media/image-20201222103114471.png" alt="image-20201222103114471">
</li>
    <li>multi view：就用auto-decoder直接训练</li>
    <li>
<strong>results</strong>
      <ul>
        <li>single view<br><img src="media/image-20201222110938605.png" alt="image-20201222110938605">
</li>
      </ul>
    </li>
  </ul>

</details>

<hr>

<p><strong><code class="language-plaintext highlighter-rouge">"Multiview Neural Surface Reconstruction by Disentangling Geometry and Appearance"</code></strong><br>
<strong>[</strong> <code class="language-plaintext highlighter-rouge">NeurIPS2020</code> <strong>]</strong> <strong><a href="https://proceedings.neurips.cc/paper/2020/file/1a77befc3b608d6ed363567685f70e1e-Paper.pdf">[paper]</a></strong> <strong><a href="https://github.com/lioryariv/idr">[code]</a></strong> <strong>[</strong> <img class="emoji" title=":mortar_board:" alt=":mortar_board:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f393.png" height="20" width="20"> <code class="language-plaintext highlighter-rouge">Weizmann Institute of Science</code> <strong>]</strong> <br>
<strong>[</strong>  <code class="language-plaintext highlighter-rouge">Lior Yariv</code>, <code class="language-plaintext highlighter-rouge">Yoni Kasten</code>, <code class="language-plaintext highlighter-rouge">Dror Moran</code>, <code class="language-plaintext highlighter-rouge">Meirav Galun</code>, <code class="language-plaintext highlighter-rouge">Matan Atzmon</code>, <code class="language-plaintext highlighter-rouge">Ronen Basri</code>, <code class="language-plaintext highlighter-rouge">Yaron Lipman</code>  <strong>]</strong><br>
<strong>[</strong> <em><code class="language-plaintext highlighter-rouge">multi-view</code>, <code class="language-plaintext highlighter-rouge">unposed images</code>, <code class="language-plaintext highlighter-rouge">single masked object image</code>, <code class="language-plaintext highlighter-rouge">SDF</code></em> <strong>]</strong></p>

<details>
  <summary>Click to expand</summary>

  <ul>
    <li>训练需要multi view分割好的unposed images，不一定需要相机pose</li>
    <li>IDR=implicit differentiable renderer</li>
    <li>
<strong>Motivation</strong>
      <ul>
        <li>SDF的differentiable renderer，用于从multi view image重建3D表面，在<code class="language-plaintext highlighter-rouge">未知相机参数</code>的情况下</li>
        <li>DVR和本篇很像，但是DVR不能处理泛化的外观，并且不能处理未知的、大噪声的相机位置</li>
        <li>SDF的优势
          <ul>
            <li>可以高效地用sphere tracing来做ray casting</li>
            <li>平滑的、真实的表面</li>
          </ul>
        </li>
        <li><img src="media/image-20201222112206328.png" alt="image-20201222112206328"></li>
      </ul>
    </li>
    <li>
<strong>review</strong>
      <ul>
        <li>有一点SDF与NeRF结合的味道，因为其颜色是从坐标位置、几何参数、观测方向共同得来的</li>
        <li>公式推导比较细致，因为值除了对几何参数有导数表达式外，还对相机参数有导数表达式</li>
        <li>重点比较对象是DVR</li>
      </ul>
    </li>
    <li>
<strong>Overview</strong>
      <ul>
        <li>把3D surface表达为一个deep implicit field \(f\) 的zero level set<br>\(\mathcal{S}_{\theta}=\{ \boldsymbol{x}\in\mathbb{R}^3 \vert f(\boldsymbol{x},\theta)=0 \}\)
          <ul>
            <li>为了avoid everywhere 0 solution，\(f\) 一般都会regularized，比如SDF的regularization；本篇用了 <code class="language-plaintext highlighter-rouge">implicit geometric regularization</code>(IGR)</li>
          </ul>
        </li>
        <li>
<strong><u>三个未知量（也是被优化的量）</u></strong>：<code class="language-plaintext highlighter-rouge">geometry</code>几何\(\theta\in\mathbb{R}^m\)，<code class="language-plaintext highlighter-rouge">appearance</code>外观\(\gamma\in\mathbb{R}^n\)，<code class="language-plaintext highlighter-rouge">cameras</code>相机参数\(\tau\in\mathbb{R}^k\)
          <ul>
            <li>注意本篇中的相机参数也是一个未知量、被优化的值，因此所有值除了需要对几何参数\(\theta\)有导数表达式外，还需要对相机参数\(\tau\)（i.e.相机中心点\(\boldsymbol{c}\)和view direction \(\boldsymbol{v}\)）有导数表达式</li>
          </ul>
        </li>
        <li>把一个像素处的颜色/radiance建模为一个射线交点坐标\(\boldsymbol{\hat x}_p\)、表面法向量\(\boldsymbol{\hat n}_p\)、view direction\(\boldsymbol{\hat v}_p\)、几何参数\(\boldsymbol{\hat z}_p\)、外观参数\(\gamma\)的映射<br>\(L_p(\theta,\gamma,\tau)=M(\boldsymbol{\hat x}_p, \boldsymbol{\hat n}_p, \boldsymbol{\hat z}_p, \boldsymbol{\hat v}_p;\gamma)\)
          <ul>
            <li>某种程度上像NeRF</li>
            <li>射线交点坐标、表面法向量、几何参数、view direction 与几何\(\theta\)、相机参数\(\tau\)有关，因为\(\boldsymbol{\hat x}_p=\boldsymbol{\hat x}_p(\theta,\tau)\)</li>
            <li>M是又一个MLP</li>
          </ul>
        </li>
        <li>losses
          <ul>
            <li>RGB loss，是L1-Norm，逐像素</li>
            <li>MASK loss，在render的时候就可以render出一个近似的可微分的mask，于是这里可以直接cross-entropy loss，逐像素</li>
            <li>reg loss，Eikonal regularization，保证是个SDF，即网络梯度模为1；bbox中均匀采点
              <ul>
                <li>\({\rm loss}_E(\theta)=\mathbb{E}_{\boldsymbol{x}}(\lVert \nabla_{\boldsymbol{x}}f(\boldsymbol{x};\theta) \rVert -1)^2\), where \(\boldsymbol{x}\)在scene的一个bbox中均匀分布</li>
              </ul>
            </li>
          </ul>
        </li>
      </ul>
    </li>
    <li>
<strong>Differentiable intersections of view directions and geometry</strong>
      <ul>
        <li>假设交叉点坐标表示为\(\boldsymbol{\hat x}_(\theta,\tau)=\boldsymbol{c}+t(\theta,\boldsymbol{c},\boldsymbol{v})\boldsymbol{v}\)，关键是t这个标量值是\(\theta\), 相机中心点位置\(\boldsymbol{c}\), 观测方向\(\boldsymbol{v}\)的函数</li>
        <li>
\[\boldsymbol{\hat x}_p(\theta,\tau)=\boldsymbol{c}+t_0\boldsymbol{v} - \frac {\boldsymbol{v}}{\nabla_x f(\boldsymbol{x}_0;\theta_0) \cdot \boldsymbol{v}_0} f(\boldsymbol{c}+t_0\boldsymbol{v};\theta)\]
          <ul class="task-list">
            <li>并且 <code class="language-plaintext highlighter-rouge">is exact in value and first derivatives of</code> \(\theta\)和\(\tau\) at \(\theta=\theta_0, \tau=\tau_0\)</li>
            <li class="task-list-item">
<input type="checkbox" class="task-list-item-checkbox" disabled>Q: what?</li>
          </ul>
        </li>
        <li>用隐函数微分；</li>
        <li>SDF在一点的法向量就是其梯度，是因为梯度的模就是1</li>
      </ul>
    </li>
    <li><strong>approximation of the surface light field</strong></li>
    <li>
<strong>masked rendering</strong>
      <ul>
        <li>==[*]== 在render的时候额外render出一个<code class="language-plaintext highlighter-rouge">可微分</code>的<code class="language-plaintext highlighter-rouge">近似binary</code>的mask</li>
      </ul>
    </li>
    <li>
<strong>results</strong>
      <ul>
        <li><img src="media/image-20201222121157452.png" alt="image-20201222121157452"></li>
        <li>可以做外观transfer <br><img src="media/image-20201222122008369.png" alt="image-20201222122008369">
</li>
      </ul>
    </li>
  </ul>

</details>

<h3 id="compositional--multi-object-scene">compositional / multi object scene</h3>

<hr>

<p><strong><code class="language-plaintext highlighter-rouge">"Semi-Supervised Learning of Multi-Object 3D Scene Representations"</code></strong><br>
<strong>[</strong> <code class="language-plaintext highlighter-rouge">ICLR2021</code> <strong>]</strong> <strong><a href="https://openreview.net/pdf?id=GwjkaD3g-V1">[paper]</a></strong> <strong>[[code]]</strong> <strong>[</strong> <img class="emoji" title=":mortar_board:" alt=":mortar_board:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f393.png" height="20" width="20"> <code class="language-plaintext highlighter-rouge">University</code> <strong>]</strong> <strong>[</strong> <img class="emoji" title=":office:" alt=":office:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f3e2.png" height="20" width="20"> <code class="language-plaintext highlighter-rouge">company</code> <strong>]</strong><br>
<strong>[</strong>  <code class="language-plaintext highlighter-rouge">xxxx</code>  <strong>]</strong><br>
<strong>[</strong> <em><code class="language-plaintext highlighter-rouge">recurrent encoder</code>, <code class="language-plaintext highlighter-rouge">SDF</code>, <code class="language-plaintext highlighter-rouge">differentiable renderer</code></em> <strong>]</strong></p>

<details>
  <summary>Click to expand</summary>

  <ul>
    <li>
<strong>Motivation</strong>
      <ul>
        <li>把场景表征为多个物体</li>
        <li>输入input RGB image，通过一个recurrent encoder，回归出每个物体的shape, pose, texture；shape通过SDF表征</li>
        <li>半监督体现在训练时候用的是RGB-D，测试时候只需要RGB</li>
        <li>single view见所有物体；物体个数是已知的<br><img src="media/image-20201222094044348.png" alt="image-20201222094044348">
</li>
      </ul>
    </li>
    <li>
<strong>review</strong>
      <ul>
        <li>只用了clevrn类数据集，而且甚至还是简单的低分辨率渲染，实验比较简单</li>
      </ul>
    </li>
    <li>
<strong>Overview</strong>
      <ul class="task-list">
        <li>首先从example shapes有监督地训练SDF（的decoder）；</li>
        <li>然后自监督地通过RGB-D训练differentiable renderer和recurrent encoder</li>
        <li class="task-list-item">
<input type="checkbox" class="task-list-item-checkbox" disabled>Q: recurrent真的能这样设计吗？<br><img src="media/image-20201222090334334.png" alt="image-20201222090334334">
</li>
        <li class="task-list-item">可以看到recurrent的主要目的是迭代、逐个地得出object的code，倒是和之前<em>Multi-object representation learning with iterative variational inference.</em>那篇有些像<br>每个物体输出深度估计，图像估计，与occulusion mask<br><img src="media/image-20201222091509810.png" alt="image-20201222091509810">
</li>
      </ul>
    </li>
    <li>
<strong>results</strong>
      <ul>
        <li><img src="media/image-20201222090527412.png" alt="image-20201222090527412"></li>
      </ul>
    </li>
  </ul>

</details>

<h3 id="analytic-exact-solution">analytic exact solution</h3>

<hr>

<p><strong><code class="language-plaintext highlighter-rouge">"Analytic Marching: An Analytic Meshing Solution from Deep Implicit Surface Networks"</code></strong><br>
<strong>[</strong> <code class="language-plaintext highlighter-rouge">ICML2020</code> <strong>]</strong> <strong><a href="http://proceedings.mlr.press/v119/lei20a/lei20a.pdf">[paper]</a></strong> <strong><a href="http://proceedings.mlr.press/v119/lei20a/lei20a-supp.pdf">[supp]</a></strong> <strong><a href="https://slides.games-cn.org/pdf/Games2020148JiabaoLei.pdf">[slice]</a></strong> <strong>[</strong> <img class="emoji" title=":mortar_board:" alt=":mortar_board:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f393.png" height="20" width="20"> <code class="language-plaintext highlighter-rouge">南方科技大学</code> <strong>]</strong> <strong>[</strong> <img class="emoji" title=":office:" alt=":office:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f3e2.png" height="20" width="20"> <a href="https://www.pazhoulab.com/"><code class="language-plaintext highlighter-rouge">琶洲实验室</code></a> <strong>]</strong><br>
<strong>[</strong>  <code class="language-plaintext highlighter-rouge">Jiabao Lei</code>, <code class="language-plaintext highlighter-rouge">Kui Jia贾奎</code>  <strong>]</strong><br>
<strong>[</strong> <em><code class="language-plaintext highlighter-rouge">learning surface mesh via implicit field functions</code>, <code class="language-plaintext highlighter-rouge">MLP analytic solution</code></em> <strong>]</strong></p>

<details>
  <summary>Click to expand</summary>

  <ul>
    <li>
<strong>Motivation</strong>
      <ul>
        <li>deep learning领域出现了很多研究，surface 的implicit functions用MLP+ReLU实现</li>
        <li>为了实现meshing <strong><u>(exactly recover meshes)</u></strong> from <strong><u>learned</u></strong> implicit functions (MLP+ReLU)
          <ul>
            <li>现有的方法采用的事实上都是标准的marching cubes采样算法；虽然效果还行，但是损失了学到的MLP的精确度，due to 离散化的本质</li>
            <li>基于ReLU-based MLP 把input空间分为很多线性区域的事实，本篇把这些区域识别为analytic cells与analytic faces，与implicit function的零值等值面有关</li>
            <li>推导了这些identified analytic faces在什么理论条件下可以保证形成一个闭合的、piecewise的planar surface</li>
            <li>基于本篇的这些理论推导，提出了一个可并行化的算法，在这些analytic cells上做marching，来==<strong><u>exactly recover</u></strong>==这些由learned MLP学出来的mesh</li>
          </ul>
        </li>
      </ul>
    </li>
    <li>
<strong>overview</strong>
      <ul>
        <li>算法的初始：先用SGD \(\underset {\boldsymbol{x}\in\mathbb{R}^3}{\min} \lvert F(\boldsymbol{x}) \rvert\) 找到表面上的一个点</li>
        <li><img src="media/image-20201223105803235.png" alt="image-20201223105803235"></li>
      </ul>
    </li>
    <li>
<strong>效果</strong>：解析解就是降维打击。精确度无限(exact 解) + CPU跑都比别人GPU跑快十几倍
      <ul>
        <li><img src="media/image-20201209113035559.png" alt="image-20201209113035559"></li>
        <li><img src="media/image-20201209111706863.png" alt="image-20201209111706863"></li>
        <li><img src="media/image-20201209105817256.png" alt="image-20201209105817256"></li>
        <li><img src="media/image-20201209105846197.png" alt="image-20201209105846197"></li>
      </ul>
    </li>
  </ul>

</details>

<h2 id="learning-parameterization--implicitization">learning parameterization / implicitization</h2>

<h2 id="others">others</h2>

<hr>

<p><strong><code class="language-plaintext highlighter-rouge">"SkeletonNet: A Topology-Preserving Solution for Learning Mesh Reconstruction of Object Surfaces from RGB Images"</code></strong><br>
<strong>[</strong> <code class="language-plaintext highlighter-rouge">TPAMI2020</code> <strong>]</strong> <strong><a href="https://arxiv.org/pdf/2008.05742">[paper]</a></strong> <strong><a href="https://github.com/tangjiapeng/SkeletonNet">[code]</a></strong> <strong>[</strong> <img class="emoji" title=":mortar_board:" alt=":mortar_board:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f393.png" height="20" width="20"> <code class="language-plaintext highlighter-rouge">南方科技大学</code>, <code class="language-plaintext highlighter-rouge">CUHK</code> <strong>]</strong> <strong>[</strong> <img class="emoji" title=":office:" alt=":office:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f3e2.png" height="20" width="20"> <code class="language-plaintext highlighter-rouge">Microsoft Research Asia</code> <strong>]</strong><br>
<strong>[</strong>  <code class="language-plaintext highlighter-rouge">Jiapeng Tang</code>, <code class="language-plaintext highlighter-rouge">Xiaoguang Han</code>, <code class="language-plaintext highlighter-rouge">Mingkui Tan</code>, <code class="language-plaintext highlighter-rouge">Xin Tong</code>, <code class="language-plaintext highlighter-rouge">Kui Jia</code>  <strong>]</strong><br>
<strong>[</strong> <em><code class="language-plaintext highlighter-rouge">skeleton</code>, <code class="language-plaintext highlighter-rouge">topology preserving</code>, <code class="language-plaintext highlighter-rouge">GCN</code>, <code class="language-plaintext highlighter-rouge">implicit surface</code></em> <strong>]</strong></p>

<details>
  <summary>Click to expand</summary>

  <ul>
    <li>
<strong>Motivation</strong>
      <ul>
        <li>学习shape时保留本身的拓扑结构：先学skeleton，再从skeleton推shape</li>
      </ul>
    </li>
    <li>
<strong>overview</strong>
      <ul>
        <li>(skeleton-GCN) skeleton-based graph convolutional neural network</li>
        <li>(skeleton-DISN) skeleton-regularized deep implicit surface network
          <ul>
            <li>主要基于<em>Disn:Deep implicit surface network for high-quality single-view 3d reconstruction</em>，加入skeleton正则项</li>
          </ul>
        </li>
      </ul>
    </li>
    <li>
<strong>效果</strong>
      <ul>
        <li><img src="media/image-20201209120443432.png" alt="image-20201209120443432"></li>
      </ul>
    </li>
  </ul>

</details>

<hr>

<p><strong><code class="language-plaintext highlighter-rouge">"NodeSLAM: Neural Object Descriptors for Multi-View Shape Reconstruction"</code></strong><br>
<strong>[</strong> <code class="language-plaintext highlighter-rouge">2020</code> <strong>]</strong> <strong><a href="https://arxiv.org/pdf/2004.04485.pdf">[paper]</a></strong> <strong><a href="https://edgarsucar.github.io/NodeSLAM/">[web]</a></strong> <strong>[</strong> <img class="emoji" title=":mortar_board:" alt=":mortar_board:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f393.png" height="20" width="20"> <code class="language-plaintext highlighter-rouge">ICL</code> <strong>]</strong> <strong>[</strong> <img class="emoji" title=":office:" alt=":office:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f3e2.png" height="20" width="20"> <code class="language-plaintext highlighter-rouge">Dyson Robotic Lab</code> <strong>]</strong><br>
<strong>[</strong>  <code class="language-plaintext highlighter-rouge">Edgar Sucar</code>, <code class="language-plaintext highlighter-rouge">Kentaro Wada</code>, <code class="language-plaintext highlighter-rouge">Andrew Davison</code>  <strong>]</strong><br>
<strong>[</strong> <em><code class="language-plaintext highlighter-rouge">differential rendering engine</code>, <code class="language-plaintext highlighter-rouge">VAE</code>, <code class="language-plaintext highlighter-rouge">multi-class learned object descriptor</code></em> <strong>]</strong></p>

<details>
  <summary>Click to expand</summary>

  <ul>
    <li>
<strong>Motivation</strong>
      <ul>
        <li>用的是voxelized occupancy表征</li>
        <li>shape and pose reference</li>
        <li>需要depth + masked image输入；相当于pose未知情况下的object SLAM，object voxelized representation是descriptor</li>
        <li><img src="media/image-20201222164917541.png" alt="image-20201222164917541"></li>
      </ul>
    </li>
  </ul>

</details>

  </article>

  

</div>

      </div>
    </div>

    <footer>

  <div class="wrapper">
    © Copyright 2021 Jianfei Guo.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank">Unsplash</a>.

    
  </div>

</footer>


    <!-- Load jQuery -->
<script src="//code.jquery.com/jquery-1.12.4.min.js"></script>

<!-- Load Common JS -->
<script src="https://ventusff.github.io/assets/js/common.js"></script>




<!-- Load Anchor JS -->
<script src="//cdnjs.cloudflare.com/ajax/libs/anchor-js/3.2.2/anchor.min.js"></script>
<script>
  anchors.options.visible = 'always';
  anchors.add('article h2, article h3, article h4, article h5, article h6');
</script>



<!-- Load fancybox -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css">
<script src="https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js"></script>


<!-- Include custom icon fonts -->
<link rel="stylesheet" href="https://ventusff.github.io/assets/css/fontawesome-all.min.css">
<link rel="stylesheet" href="https://ventusff.github.io/assets/css/academicons.min.css">
<link rel="stylesheet" href="https://ventusff.github.io/assets/iconfont/iconfont.css">

<!-- Google Analytics -->
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-XXXXXXXXX', 'auto');
ga('send', 'pageview');
</script>

    
  </body>
  
<!-- Mermaid Js -->
<script>

    // mermaid rendering: inspired by https://stackoverflow.com/questions/53883747/how-to-make-github-pages-markdown-support-mermaid-diagram
    // mermiad in details tag: inspired by https://gitlab.com/gitlab-org/gitlab/-/issues/28495
    //          and by https://gitlab.com/gitlab-org/gitlab/-/blob/master/app/assets/javascripts/behaviors/markdown/render_mermaid.js

    function renderMermaids($els) {
        if (!$els.length) return;

        var config = {
            startOnLoad: false,
            theme: 'neutral', // forest
            securityLevel: 'loose',
            flowchart:{
                    useMaxWidth:true,
                    htmlLabels:false, // important for not squeezing blocks
                }
        };
        // const theme =localStorage.getItem("theme");
        // if(theme == "dark") config.theme = 'dark';  // currently needs to refresh to make dark mode toggle
        mermaid.initialize(config);

        $els.each((i, el) => {

            // Mermaid doesn't like `<br />` tags, so collapse all like tags into `<br>`, which is parsed correctly.
            const source = el.textContent.replace(/<br\s*\/>/g, '<br>');

            // Remove any extra spans added by the backend syntax highlighting.
            Object.assign(el, { textContent: source });

            mermaid.init(undefined, el, id => {
                const svg = document.getElementById(id);

                // As of https://github.com/knsv/mermaid/commit/57b780a0d,
                // Mermaid will make two init callbacks:one to initialize the
                // flow charts, and another to initialize the Gannt charts.
                // Guard against an error caused by double initialization.
                if (svg.classList.contains('mermaid')) {
                    console.log("return");
                    return;
                }

                // svg.classList.add('mermaid'); //will add new bug

                // pre > code > svg
                svg.closest('pre').replaceWith(svg);

                // We need to add the original source into the DOM to allow Copy-as-GFM
                // to access it.
                const sourceEl = document.createElement('text');
                sourceEl.classList.add('source');
                sourceEl.setAttribute('display', 'none');
                sourceEl.textContent = source;

                svg.appendChild(sourceEl);

            });
        });

    }

    const $els = $(document).find('.language-mermaid');
    if ($els.length)
    {
        const visibleMermaids = $els.filter(function filter() {
            return $(this).closest('details').length === 0 && $(this).is(':visible');
        });

        renderMermaids(visibleMermaids);

        $els.closest('details').one('toggle', function toggle() {
            if (this.open) {
                renderMermaids($(this).find('.language-mermaid'));
            }
        });
    }



</script>


  <!-- Auto config image to fancybox -->
<script>
    $(document).ready(function() {
        $("article img[class!='emoji']").each(function() {
            var currentImage = $(this);
            currentImage.wrap("<a href='" + currentImage.attr("src") + "' data-fancybox='lightbox' data-caption='" + currentImage.attr("alt") + "'></a>");
        });
    });
</script>

</html>
