<!DOCTYPE html>
<html>
  <head>
    
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  <title>Jianfei Guo | object aware encoder / generative models</title>
  <meta name="description" content="Jianfei Guo. ffventus (at) gmail.com. I am a learner in representation learning and decision making. 
">

  <link rel="shortcut icon" href="https://ventusff.github.io/assets/img/favicon.ico?v=1">

  <link rel="stylesheet" href="https://ventusff.github.io/assets/css/main.css">
  <link rel="canonical" href="https://ventusff.github.io/notes/object_aware">
  

    
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams',
      inlineMath: [ ['$','$'], ["\\(","\\)"] ], // http://docs.mathjax.org/en/latest/input/tex/delimiters.html
      displayMath: [['\\[','\\]'], ['$$','$$']]
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    
<!-- Mermaid Js -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/mermaid/8.7.0/mermaid.min.js"></script>
<!-- <script src="https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js"></script> // this is the old version --> 

  </head>

  <body>

    <header class="site-header">

  <div class="wrapper">

    
    <span class="site-title">
        
        <strong>Jianfei</strong> Guo
    </span>
    

    <nav class="site-nav">
      <input type="checkbox" id="nav-trigger" class="nav-trigger">
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewbox="0 0 18 15" width="18px" height="15px">
              <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"></path>
              <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"></path>
              <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"></path>
            </svg>
          </span>
        </label>

      <div class="trigger">
        <!-- About -->
        <a class="page-link" href="https://ventusff.github.io/">about</a>

        <!-- Blog -->
        <a class="page-link" href="https://ventusff.github.io/blog/">blog</a>

        <!-- Pages -->
        
          
        
          
        
          
        
          
            <a class="page-link" href="https://ventusff.github.io/notes/">notes</a>
          
        
          
        
          
        
          
        

        <!-- CV link -->
        <!-- <a class="page-link" href="https://ventusff.github.io/assets/pdf/CV.pdf">vitae</a> -->

      </div>
    </nav>

  </div>

</header>



    <div class="page-content">
      <div class="wrapper">
        <div class="post">

  <header class="post-header">
    <h1 class="post-title">object aware encoder / generative models</h1>
    <p class="post-subtitle">object-aware的encoder/生成模型</p>
    <p class="post-meta"></p>
  </header>

  <article class="post-content">
    <hr>

<h2 id="object-aware-representations">object aware representations</h2>

<ul>
  <li>keyword:
    <ul>
      <li><strong>blockGAN被引</strong></li>
      <li><strong>SRN被引中带object</strong></li>
    </ul>
  </li>
</ul>

<hr>

<p><strong><code class="language-plaintext highlighter-rouge">[Encoder] &lt;ROOTS&gt; "Learning to Infer 3D Object Models from Images"/"Object-Centric Representation and Rendering of 3D Scenes"</code></strong><br>
<strong>[</strong> <code class="language-plaintext highlighter-rouge">2020</code> <strong>]</strong> <strong><a href="https://arxiv.org/pdf/2006.06130.pdf">[paper]</a></strong> <strong><a href="https://arxiv.org/pdf/2006.06130v1.pdf">[paper_v1]</a></strong> <strong>[</strong> <img class="emoji" title=":mortar_board:" alt=":mortar_board:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f393.png" height="20" width="20"> <code class="language-plaintext highlighter-rouge">Rutgers University</code> <strong>]</strong> 
<strong>[</strong>  <code class="language-plaintext highlighter-rouge">Chang Chen</code>, <code class="language-plaintext highlighter-rouge">Fei Deng</code>, <code class="language-plaintext highlighter-rouge">Sungjin Ahn</code> <strong>]</strong><br>
<strong>[</strong> <em><code class="language-plaintext highlighter-rouge">scene encoder</code>, <code class="language-plaintext highlighter-rouge">GQN-based</code>, <code class="language-plaintext highlighter-rouge">3D object detection</code></em> <strong>]</strong></p>

<p>main preliminary: GQN</p>

<details>
  <summary>Click to expand</summary>

  <ul>
    <li>
<strong>Motivation</strong>
      <ul>
        <li><img src="media/image-20201027191207023.png" alt="image-20201027191207023"></li>
      </ul>
    </li>
    <li>
<strong>前景背景区分方式</strong>： 通过其<code class="language-plaintext highlighter-rouge">Scene Encoder</code>；其实是在GVFM下做3D物体检测
      <ul>
        <li>把3D 空间分为 \(N_{max}=N_x \times N_y \times N_z\) 个cell，每个最多检测1个物体（类似Yolo，扩展到三维）；</li>
        <li>检测是否有一个物体其中心落在了cell内；如果有，那么回归出一个连续量 \(\boldsymbol{z}_{ijk}^{where} \in \mathbb{R}^3\) 来specify坐标</li>
        <li>具体做法：把一系列context 观测 \(\mathcal{C}=\{(\boldsymbol{x}_c, \boldsymbol{y}_c)\}\) encode into a Geometric Volume Feature Map 三维体素特征空间 \(\boldsymbol{r} \in \mathbb{R}^{N_x \times N_y \times N_z \times d}\) ，逐个cell infer 是否有物体以及中心点坐标
          <ul>
            <li>GVFM需要把一系列partial observation aggregate起来；</li>
            <li>① 对\(\mathcal{C}\) 计算一个order-invariant summary \(\psi\) ：\(\psi=\sum_{c=1}^{\lvert\mathcal{C} \rvert} \psi_{\mathcal{c}}=\sum_{c=1}^{\lvert\mathcal{C} \rvert} f_\psi(x_c, y_c)\)</li>
            <li>② 对 \(\psi\) 应用一个3D transposed convolution 来把 scene-level 表征\(\psi\) split 成单个的\(\boldsymbol{r}_{ijk}\) slots</li>
          </ul>
        </li>
      </ul>
    </li>
    <li>
<strong>主要贡献</strong>
      <ul>
        <li>object-aware scene encoder，把一系列观测首先映射到体素特征空间，再逐cell检测回归有无物体及中心坐标
          <ul>
            <li>==思考== ：
              <ul>
                <li>这个decoder可以设法用于我们的拓扑图构建</li>
                <li>我们是用拓扑图的形式来organize各个物体；每个feature只来自于一个物体的观测，也只存一个物体的信息</li>
              </ul>
            </li>
          </ul>
        </li>
        <li>重点考虑了object level如何重建图片；对我们会有一定帮助</li>
      </ul>
    </li>
    <li><strong>效果</strong></li>
  </ul>

  <p><img src="media/bcd05a95-3328-4004-af8f-42e62294b993.png" alt="img"></p>

</details>

<hr>

<p><strong><code class="language-plaintext highlighter-rouge">"Self-supervised Learning of 3D Objects from Natural Images"</code></strong><br>
<strong>[</strong> <code class="language-plaintext highlighter-rouge">2019</code> <strong>]</strong> <strong><a href="https://arxiv.org/pdf/1911.08850.pdf">[paper]</a></strong>  <strong>[</strong> <img class="emoji" title=":mortar_board:" alt=":mortar_board:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f393.png" height="20" width="20"> <code class="language-plaintext highlighter-rouge">The University of Tokyo</code> <strong>]</strong> <strong>[</strong> <img class="emoji" title=":office:" alt=":office:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f3e2.png" height="20" width="20"> <code class="language-plaintext highlighter-rouge">RIKEN</code> <strong>]</strong><br>
<strong>[</strong>  <code class="language-plaintext highlighter-rouge">Hiroharu Kato and Tatsuya Harada</code>  <strong>]</strong> &lt; RGBD-GAN 同组&gt; 
<strong>[</strong> <em><code class="language-plaintext highlighter-rouge">natural images</code></em> <strong>]</strong></p>

<details>
  <summary>Click to expand</summary>

  <ul>
    <li><strong>Motivation</strong></li>
    <li><strong>自监督的方式，从真实世界图片中提取出3D物体</strong></li>
    <li>
<strong>两阶段训练</strong>
      <ul>
        <li>首先学习一个base shape，然后从base shape到full model</li>
        <li>如果不用两阶段训练，学习到的形状都变成了一个椭球，变得模糊化了。</li>
        <li>
<br><img src="media/0473e31c-1289-4436-9167-a4483a143024.png" alt="img">
</li>
      </ul>
    </li>
    <li>
<br><img src="media/d7558150-cfef-48ad-9ce7-05cc20ceb89e.png" alt="img">
</li>
    <li><strong>主要贡献</strong></li>
    <li>
      <blockquote>
        <p>为了应对复杂的形状、复杂的真实世界背景，使用对于物体表面和背景的强regularization
To suppress it, we propose using strong regularization and constraints on object surfaces and background images. 
可以从cifar10, pascal这样的数据集中重建出各种各样的物体
由于数据集中经常有ill-posed摆放的图片，学习出并且利用先验知识是关键。
Since this is a severely ill-posed problem, learning and leveraging the prior knowledge of objects is the key to this task.</p>
      </blockquote>
    </li>
  </ul>

</details>

<hr>

<p><strong><code class="language-plaintext highlighter-rouge">"Multi-Object Representation Learning with Iterative Variational Inference"</code></strong><br>
<strong>[</strong> <code class="language-plaintext highlighter-rouge">2019</code> <strong>]</strong> <strong><a href="https://arxiv.org/pdf/1903.00450.pdf">[paper]</a></strong> <strong>[[code]]</strong> <strong>[</strong> <img class="emoji" title=":mortar_board:" alt=":mortar_board:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f393.png" height="20" width="20"> <code class="language-plaintext highlighter-rouge">University</code> <strong>]</strong> <strong>[</strong> <img class="emoji" title=":office:" alt=":office:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f3e2.png" height="20" width="20"> <code class="language-plaintext highlighter-rouge">company</code> <strong>]</strong><br>
<strong>[</strong>  <code class="language-plaintext highlighter-rouge">xxxx</code>  <strong>]</strong><br>
<strong>[</strong> <em><code class="language-plaintext highlighter-rouge">abcd</code></em> <strong>]</strong></p>

<details>
  <summary>Click to expand</summary>

  <ul>
    <li><strong>Motivation</strong></li>
  </ul>

</details>

<hr>

<p><strong><code class="language-plaintext highlighter-rouge">"Reconstruction Bottlenecks in Object-Centric Generative Models"</code></strong><br>
<strong>[</strong> <code class="language-plaintext highlighter-rouge">2020</code> <strong>]</strong> <strong><a href="https://arxiv.org/pdf/2007.06245.pdf">[paper]</a></strong> <strong>[[code]]</strong> <strong>[</strong> <img class="emoji" title=":mortar_board:" alt=":mortar_board:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f393.png" height="20" width="20"> <code class="language-plaintext highlighter-rouge">University</code> <strong>]</strong> <strong>[</strong> <img class="emoji" title=":office:" alt=":office:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f3e2.png" height="20" width="20"> <code class="language-plaintext highlighter-rouge">company</code> <strong>]</strong><br>
<strong>[</strong>  <code class="language-plaintext highlighter-rouge">xxxx</code>  <strong>]</strong><br>
<strong>[</strong> <em><code class="language-plaintext highlighter-rouge">GENESIS-based</code>, <code class="language-plaintext highlighter-rouge">efgh</code></em> <strong>]</strong></p>

<details>
  <summary>Click to expand</summary>

  <ul>
    <li><strong>Motivation</strong></li>
  </ul>

</details>

<hr>

<p><strong><code class="language-plaintext highlighter-rouge">"RELATE: Physically Plausible Multi-Object Scene Synthesis Using Structured Latent Spaces"</code></strong><br>
<strong>[</strong> <code class="language-plaintext highlighter-rouge">0000</code> <strong>]</strong> <strong><a href="https://arxiv.org/pdf/2007.01272.pdf">[paper]</a></strong> <strong><a href="https://github.com/hyenal/relate">[code]</a></strong> <strong><a href="http://geometry.cs.ucl.ac.uk/projects/2020/relate/">[web]</a></strong> <strong><a href="http://geometry.cs.ucl.ac.uk/projects/2020/relate/paper_docs/EhrhardtGrothEtAl_Relate_NeurIPS_2020.webm">[video]</a></strong> <strong>[</strong> <img class="emoji" title=":mortar_board:" alt=":mortar_board:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f393.png" height="20" width="20"> <code class="language-plaintext highlighter-rouge">University</code> <strong>]</strong> <strong>[</strong> <img class="emoji" title=":office:" alt=":office:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f3e2.png" height="20" width="20"> <code class="language-plaintext highlighter-rouge">company</code> <strong>]</strong><br>
<strong>[</strong>  <code class="language-plaintext highlighter-rouge">xxxx</code>  <strong>]</strong><br>
<strong>[</strong> <em><code class="language-plaintext highlighter-rouge">baseline-&gt;GENESIS</code>, <code class="language-plaintext highlighter-rouge">efgh</code></em> <strong>]</strong></p>

<details>
  <summary>Click to expand</summary>

  <ul>
    <li>
<strong>Motivation</strong>
      <ul>
        <li><img src="media/teaser.png" alt="teaser"></li>
      </ul>
    </li>
  </ul>

</details>

<hr>

<p><strong><code class="language-plaintext highlighter-rouge">"Learning 3D Object-Oriented World Models from Unlabeled Videos"</code></strong><br>
<strong>[</strong> <code class="language-plaintext highlighter-rouge">ICMLW2020</code> <strong>]</strong> <strong><a href="http://e2crawfo.github.io/pdfs/icml_ool_2020.pdf">[paper]</a></strong>  <strong>[</strong> <img class="emoji" title=":mortar_board:" alt=":mortar_board:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f393.png" height="20" width="20"> <code class="language-plaintext highlighter-rouge">McGill University</code> <strong>]</strong><br>
<strong>[</strong>  <code class="language-plaintext highlighter-rouge">Eric Crawford</code>, <code class="language-plaintext highlighter-rouge">Joelle Pineau</code>  <strong>]</strong><br>
<strong>[</strong> <em><code class="language-plaintext highlighter-rouge">SRN-based</code>, <code class="language-plaintext highlighter-rouge">3D objects moving through a 3D world</code>, <code class="language-plaintext highlighter-rouge">probabilistic</code>, <code class="language-plaintext highlighter-rouge">encoder/detector focused</code></em> <strong>]</strong></p>

<p>main preliminary: SRN</p>

<details>
  <summary>Click to expand</summary>

  <table>
    <thead>
      <tr>
        <th><img src="media/40c89125-3bd2-4651-9ec0-2cef3245ac11.png" alt="img"></th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td> </td>
      </tr>
    </tbody>
  </table>

  <ul>
    <li>
<strong>Motivation</strong>
      <ul>
        <li>不仅可以从感知流中分割出物体</li>
        <li>而且可以提取物体的3D信息、在3D空间中跟踪他们</li>
      </ul>
    </li>
    <li>
<strong>主要特点</strong>
      <ul>
        <li><strong>用的是SRN</strong></li>
        <li><strong>物体是运动的；视频输入</strong></li>
      </ul>
    </li>
  </ul>

</details>

<hr>

<p><strong><code class="language-plaintext highlighter-rouge">"Neural Graphics Pipeline for Controllable Image Generation"</code></strong><br>
<strong>[</strong> <code class="language-plaintext highlighter-rouge">2020</code> <strong>]</strong> <strong><a href="https://arxiv.org/pdf/2006.10569.pdf">[paper]</a></strong> <strong>[[code]]</strong> <strong>[</strong> <img class="emoji" title=":mortar_board:" alt=":mortar_board:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f393.png" height="20" width="20"> <code class="language-plaintext highlighter-rouge">University</code> <strong>]</strong> <strong>[</strong> <img class="emoji" title=":office:" alt=":office:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f3e2.png" height="20" width="20"> <code class="language-plaintext highlighter-rouge">company</code> <strong>]</strong><br>
<strong>[</strong>  <code class="language-plaintext highlighter-rouge">xxxx</code>  <strong>]</strong><br>
<strong>[</strong> <em><code class="language-plaintext highlighter-rouge">abcd</code></em> <strong>]</strong></p>

<details>
  <summary>Click to expand</summary>

  <ul>
    <li>
<strong>Motivation</strong>
      <ul>
        <li><img src="media/image-20201028160442930.png" alt="image-20201028160442930"></li>
      </ul>
    </li>
  </ul>

</details>

<hr>

<p><strong><code class="language-plaintext highlighter-rouge">"Rotationally-Temporally Consistent Novel View Synthesis of Human Performance Video"</code></strong><br>
<strong>[</strong> <code class="language-plaintext highlighter-rouge">ECCV2020</code> <strong>]</strong> <strong><a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123490375.pdf">[paper]</a></strong>  <strong>[</strong> <img class="emoji" title=":mortar_board:" alt=":mortar_board:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f393.png" height="20" width="20"> <code class="language-plaintext highlighter-rouge">University of North Carolina</code> <strong>]</strong> <strong>[</strong> <img class="emoji" title=":office:" alt=":office:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f3e2.png" height="20" width="20"> <code class="language-plaintext highlighter-rouge">Adobe</code>,<code class="language-plaintext highlighter-rouge">Korea Advanced Institute of Science and Technology</code> <strong>]</strong><br>
<strong>[</strong>  <code class="language-plaintext highlighter-rouge">Youngjoong Kwon</code>, <code class="language-plaintext highlighter-rouge">Stefano Petrangeli</code>, <code class="language-plaintext highlighter-rouge">Dahun Kim</code>, <code class="language-plaintext highlighter-rouge">Haoliang Wang</code>, <code class="language-plaintext highlighter-rouge">Eunbyung Park</code>, <code class="language-plaintext highlighter-rouge">Viswanathan Swaminathan</code>, <code class="language-plaintext highlighter-rouge">Henry Fuchs</code> <strong>]</strong><br>
<strong>[</strong> <em><code class="language-plaintext highlighter-rouge">Novel View Video Synthesis</code>, <code class="language-plaintext highlighter-rouge">Synthetic Human Dataset</code></em> <strong>]</strong></p>

<details>
  <summary>Click to expand</summary>

  <ul>
    <li>
<strong>Motivation</strong>
      <ul>
        <li><img src="media/image-20201027202644934.png" alt="image-20201027202644934"></li>
      </ul>
    </li>
  </ul>

</details>

<hr>

<p><strong><code class="language-plaintext highlighter-rouge">&lt; 3D object aware video generation&gt; "Unsupervised object-centric video generation and decomposition in 3D"</code></strong><br>
<strong>[</strong> <code class="language-plaintext highlighter-rouge">NeurIPS2020</code> <strong>]</strong> <strong><a href="https://arxiv.org/pdf/2007.06705.pdf">[paper]</a></strong> <strong><a href="https://github.com/pmh47/o3v">[code]</a></strong> <strong><a href="https://www.pmh47.net/o3v/">[web]</a></strong> <strong>[</strong> <img class="emoji" title=":office:" alt=":office:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f3e2.png" height="20" width="20"> <code class="language-plaintext highlighter-rouge">IST Austria</code> <strong>]</strong><br>
<strong>[</strong>  <code class="language-plaintext highlighter-rouge">Paul Henderson</code>, <code class="language-plaintext highlighter-rouge">Christoph H. Lampert</code>  <strong>]</strong><br>
<strong>[</strong> <em><code class="language-plaintext highlighter-rouge">encoder-decoder</code>, <code class="language-plaintext highlighter-rouge">object-centric generative models</code>, <code class="language-plaintext highlighter-rouge">inspired by classical image segmentation</code>,<code class="language-plaintext highlighter-rouge">object tracking</code></em> <strong>]</strong></p>

<p><strong>[</strong> <code class="language-plaintext highlighter-rouge">review</code>: 从视频中分解出物体（及运动）与背景的3D表征，传统机器学习功底深厚，loss设计值得研究；利用了视频中的一些hint (object tracking)，没有用到多视几何 <strong>]</strong></p>

<details>
  <summary>Click to expand</summary>

  <ul>
    <li>
<strong>Motivation</strong>
      <ul>
        <li><img src="media/6e9983a7-f1e3-4121-b15a-b7f05f0d6f9e.png" alt="img"></li>
      </ul>
    </li>
    <li>
<strong>前景背景区分方式</strong>
      <ul>
        <li>通过encoder 编码 context views of a video \(\{x_z, v_z\}\) 为两个隐向量：scene embedding和camera paramter embedding，把这两个embedding decode为逐个cell中的是否有物体 + 各个物体的外观、pose参数 + 背景形状、材质</li>
        <li>借鉴了传统CV中图像分割的一些思想，依靠强大的loss设计，用encoder-decoder的方式实现了视频生成。泛化性未知</li>
      </ul>
    </li>
    <li>
<strong>主要贡献</strong>
      <ul>
        <li>通过预测<strong>分割</strong>mask、随时间跟踪物体，把一段given <strong>video</strong> <strong>分解</strong>为其组成物体、背景</li>
        <li>通过预测深度、3D bbox， determine 场景及物体的3D结构</li>
        <li>生成连贯的视频，视频中物体在一个3D背景中进行3D空间移动</li>
      </ul>
    </li>
    <li>
<strong>主要特点</strong>
      <ul>
        <li>从single latent code z  <strong>decode into</strong> G 个 objects的参数 和 背景的形状、材质</li>
        <li>每个object逐个被 decode为  pose parameter 和 apperance embedding
          <ul>
            <li>pose parameter具体操作：</li>
            <li>把空间划分为grid，每个cell infer是否有物体(0/1) ，infer每个物体距离cell中心的位移、旋转（并且都是随时间变化的）</li>
          </ul>
        </li>
        <li>把每个物体的pose时间t序列 、外观embedding、背景形状、材质 随时间逐帧渲染</li>
        <li>
<em><strong>ego-centric</strong></em> model</li>
      </ul>
    </li>
    <li>
<strong>==loss / regularization==</strong> （<u>*由于从2D videos中infer 3D结构是inherently ambiguous，因此需要regularization来避免degerate solutions*</u>）
      <ul>
        <li>L1 regularization on 物体速度大小：discourages local minima, 防止模型不能track物体</li>
        <li>hinge regularization on 物体存在概率：discourages 物体在优化早期在shape还没适应(学到)时就消失</li>
        <li>(inspired by <em>图像分割</em> 任务中的<em>Markov random fiields</em>) we penalize edges in the reconstructed foreground mask for occurring in areas of the original image that have small gradients. ：This discourages undesirable but mathematically-correct solutions where an object is in front of an untextured surface, and parts of that surface are incorporated in the object rather than the background.
          <ul class="task-list">
            <li class="task-list-item">
<input type="checkbox" class="task-list-item-checkbox" disabled>Q: what ??</li>
          </ul>
        </li>
        <li>standard mesh regulirazers for 背景、mesh物体，避免degenerate shapes：L2 on Laplacian curvature, L1 on angles between faces, L1 on edge lenghts variance</li>
      </ul>
    </li>
  </ul>

</details>

<hr>

<p><strong><code class="language-plaintext highlighter-rouge">"Unsupervised Discovery of Object Landmarks via Contrastive Learning"</code></strong><br>
<strong>[</strong> <code class="language-plaintext highlighter-rouge">0000</code> <strong>]</strong> <strong><a href="https://arxiv.org/pdf/2006.14787.pdf">[paper]</a></strong> <strong>[[code]]</strong> <strong>[</strong> <img class="emoji" title=":mortar_board:" alt=":mortar_board:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f393.png" height="20" width="20"> <code class="language-plaintext highlighter-rouge">MIT</code> <strong>]</strong> <strong>[</strong> <img class="emoji" title=":office:" alt=":office:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f3e2.png" height="20" width="20"> <code class="language-plaintext highlighter-rouge">company</code> <strong>]</strong><br>
<strong>[</strong>  <code class="language-plaintext highlighter-rouge">xxxx</code>  <strong>]</strong><br>
<strong>[</strong> <em><code class="language-plaintext highlighter-rouge">abcd</code></em> <strong>]</strong></p>

<details>
  <summary>Click to expand</summary>

  <ul>
    <li><strong>Motivation</strong></li>
  </ul>

</details>

<hr>

<p><strong><code class="language-plaintext highlighter-rouge">"UNSUPERVISED DISCOVERY OF PARTS, STRUCTURE, AND DYNAMICS"</code></strong><br>
<strong>[</strong> <code class="language-plaintext highlighter-rouge">0000</code> <strong>]</strong> <strong>[[paper]]</strong> <strong>[[code]]</strong> <strong>[</strong> <img class="emoji" title=":mortar_board:" alt=":mortar_board:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f393.png" height="20" width="20"> <code class="language-plaintext highlighter-rouge">University</code> <strong>]</strong> <strong>[</strong> <img class="emoji" title=":office:" alt=":office:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f3e2.png" height="20" width="20"> <code class="language-plaintext highlighter-rouge">company</code> <strong>]</strong><br>
<strong>[</strong>  <code class="language-plaintext highlighter-rouge">xxxx</code>  <strong>]</strong><br>
<strong>[</strong> <em><code class="language-plaintext highlighter-rouge">abcd</code></em> <strong>]</strong></p>

<details>
  <summary>Click to expand</summary>

  <ul>
    <li><strong>Motivation</strong></li>
  </ul>

</details>

<hr>

<p><strong><code class="language-plaintext highlighter-rouge">"FroDO: From Detections to 3D Objects"</code></strong><br>
<strong>[</strong> <code class="language-plaintext highlighter-rouge">CVPR2020</code> <strong>]</strong> <strong><a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Runz_FroDO_From_Detections_to_3D_Objects_CVPR_2020_paper.pdf">[paper]</a></strong> <strong>[[code]]</strong> <strong>[</strong> <img class="emoji" title=":mortar_board:" alt=":mortar_board:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f393.png" height="20" width="20"> <code class="language-plaintext highlighter-rouge">UCL</code>, <code class="language-plaintext highlighter-rouge">The University of Adelaide</code> <strong>]</strong> <strong>[</strong> <img class="emoji" title=":office:" alt=":office:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f3e2.png" height="20" width="20"> <code class="language-plaintext highlighter-rouge">Facebook</code> <strong>]</strong><br>
<strong>[</strong>  <code class="language-plaintext highlighter-rouge">Martin Runz</code>, <code class="language-plaintext highlighter-rouge">Kejie Li</code>, <code class="language-plaintext highlighter-rouge">Meng Tang</code>, <code class="language-plaintext highlighter-rouge">Lingni Ma</code>, <code class="language-plaintext highlighter-rouge">Chen Kong</code>, <code class="language-plaintext highlighter-rouge">Tanner Schmidt</code>, <code class="language-plaintext highlighter-rouge">Ian Reid</code>, <code class="language-plaintext highlighter-rouge">Lourdes Agapito</code>, <code class="language-plaintext highlighter-rouge">Julian Straub</code>, <code class="language-plaintext highlighter-rouge">Steven Lovegrove</code>, <code class="language-plaintext highlighter-rouge">Richard Newcombed</code><strong>]</strong><br>
<strong>[</strong> <em><code class="language-plaintext highlighter-rouge">natural dataset</code>, <code class="language-plaintext highlighter-rouge">mask-RCNN</code>, <code class="language-plaintext highlighter-rouge">pre-learnt shape priors</code></em> <strong>]</strong></p>

<details>
  <summary>Click to expand</summary>

  <ul>
    <li>
<strong>Motivation</strong>
      <ul>
        <li>从一个定位好的RGB输入序列，检测出物体并infer他们的pose和一个progressively fine grained and expressive 物体shape表征<br><img src="media/7493d181-c6cb-486d-a9a4-239e5415c984.png" alt="img">
</li>
      </ul>
    </li>
    <li>
<strong>前景背景区分方式</strong>
      <ul>
        <li><strong>使用一个标准的检测、分割框架：mask RCNN</strong></li>
        <li>重点不在分割，而在对分割出来的物体multi view encoder成一个合适的object shape embedding</li>
      </ul>
    </li>
  </ul>

  <table>
    <thead>
      <tr>
        <th><img src="media/85c27bf6-aca7-4c46-9b60-a2c8a3615a43.png" alt="img"></th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>把</td>
      </tr>
    </tbody>
  </table>

  <table>
    <thead>
      <tr>
        <th><img src="media/a21da736-1ecf-441f-91b0-ea7f2ed54e75.png" alt="img"></th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>提出了一种新的joint shape embedding，利用了稀疏点云表征的效率和稠密surface表征的表达能力</td>
      </tr>
    </tbody>
  </table>

</details>

  </article>

  

</div>

      </div>
    </div>

    <footer>

  <div class="wrapper">
    © Copyright 2021 Jianfei Guo.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank">Unsplash</a>.

    
  </div>

</footer>


    <!-- Load jQuery -->
<script src="//code.jquery.com/jquery-1.12.4.min.js"></script>

<!-- Load Common JS -->
<script src="https://ventusff.github.io/assets/js/common.js"></script>




<!-- Load Anchor JS -->
<script src="//cdnjs.cloudflare.com/ajax/libs/anchor-js/3.2.2/anchor.min.js"></script>
<script>
  anchors.options.visible = 'always';
  anchors.add('article h2, article h3, article h4, article h5, article h6');
</script>



<!-- Load fancybox -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css">
<script src="https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js"></script>


<!-- Include custom icon fonts -->
<link rel="stylesheet" href="https://ventusff.github.io/assets/css/fontawesome-all.min.css">
<link rel="stylesheet" href="https://ventusff.github.io/assets/css/academicons.min.css">
<link rel="stylesheet" href="https://ventusff.github.io/assets/iconfont/iconfont.css">

<!-- Google Analytics -->
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-XXXXXXXXX', 'auto');
ga('send', 'pageview');
</script>

    
  </body>
  
<!-- Mermaid Js -->
<script>

    // mermaid rendering: inspired by https://stackoverflow.com/questions/53883747/how-to-make-github-pages-markdown-support-mermaid-diagram
    // mermiad in details tag: inspired by https://gitlab.com/gitlab-org/gitlab/-/issues/28495
    //          and by https://gitlab.com/gitlab-org/gitlab/-/blob/master/app/assets/javascripts/behaviors/markdown/render_mermaid.js

    function renderMermaids($els) {
        if (!$els.length) return;

        var config = {
            startOnLoad: false,
            theme: 'neutral', // forest
            securityLevel: 'loose',
            flowchart:{
                    useMaxWidth:true,
                    htmlLabels:false, // important for not squeezing blocks
                }
        };
        // const theme =localStorage.getItem("theme");
        // if(theme == "dark") config.theme = 'dark';  // currently needs to refresh to make dark mode toggle
        mermaid.initialize(config);

        $els.each((i, el) => {

            // Mermaid doesn't like `<br />` tags, so collapse all like tags into `<br>`, which is parsed correctly.
            const source = el.textContent.replace(/<br\s*\/>/g, '<br>');

            // Remove any extra spans added by the backend syntax highlighting.
            Object.assign(el, { textContent: source });

            mermaid.init(undefined, el, id => {
                const svg = document.getElementById(id);

                // As of https://github.com/knsv/mermaid/commit/57b780a0d,
                // Mermaid will make two init callbacks:one to initialize the
                // flow charts, and another to initialize the Gannt charts.
                // Guard against an error caused by double initialization.
                if (svg.classList.contains('mermaid')) {
                    console.log("return");
                    return;
                }

                // svg.classList.add('mermaid'); //will add new bug

                // pre > code > svg
                svg.closest('pre').replaceWith(svg);

                // We need to add the original source into the DOM to allow Copy-as-GFM
                // to access it.
                const sourceEl = document.createElement('text');
                sourceEl.classList.add('source');
                sourceEl.setAttribute('display', 'none');
                sourceEl.textContent = source;

                svg.appendChild(sourceEl);

            });
        });

    }

    const $els = $(document).find('.language-mermaid');
    if ($els.length)
    {
        const visibleMermaids = $els.filter(function filter() {
            return $(this).closest('details').length === 0 && $(this).is(':visible');
        });

        renderMermaids(visibleMermaids);

        $els.closest('details').one('toggle', function toggle() {
            if (this.open) {
                renderMermaids($(this).find('.language-mermaid'));
            }
        });
    }



</script>


  <!-- Auto config image to fancybox -->
<script>
    $(document).ready(function() {
        $("article img[class!='emoji']").each(function() {
            var currentImage = $(this);
            currentImage.wrap("<a href='" + currentImage.attr("src") + "' data-fancybox='lightbox' data-caption='" + currentImage.attr("alt") + "'></a>");
        });
    });
</script>

</html>
