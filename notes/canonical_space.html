<!DOCTYPE html>
<html>
  <head>
    
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  <title>Jianfei Guo | works in <strong>canonical spaces</strong></title>
  <meta name="description" content="Jianfei Guo. ffventus (at) gmail.com. I am a learner in representation learning and decision making. 
">

  <link rel="shortcut icon" href="https://ventusff.github.io/assets/img/favicon.ico?v=1">

  <link rel="stylesheet" href="https://ventusff.github.io/assets/css/main.css">
  <link rel="canonical" href="https://ventusff.github.io/notes/canonical_space">
  

    
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams',
      inlineMath: [ ['$','$'], ["\\(","\\)"] ], // http://docs.mathjax.org/en/latest/input/tex/delimiters.html
      displayMath: [['\\[','\\]'], ['$$','$$']]
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    
<!-- Mermaid Js -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/mermaid/8.7.0/mermaid.min.js"></script>
<!-- <script src="https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js"></script> // this is the old version --> 

  </head>

  <body>

    <header class="site-header">

  <div class="wrapper">

    
    <span class="site-title">
        
        <strong>Jianfei</strong> Guo
    </span>
    

    <nav class="site-nav">
      <input type="checkbox" id="nav-trigger" class="nav-trigger">
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewbox="0 0 18 15" width="18px" height="15px">
              <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"></path>
              <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"></path>
              <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"></path>
            </svg>
          </span>
        </label>

      <div class="trigger">
        <!-- About -->
        <a class="page-link" href="https://ventusff.github.io/">about</a>

        <!-- Blog -->
        <a class="page-link" href="https://ventusff.github.io/blog/">blog</a>

        <!-- Pages -->
        
          
        
          
        
          
        
          
            <a class="page-link" href="https://ventusff.github.io/notes/">notes</a>
          
        
          
        
          
        
          
        

        <!-- CV link -->
        <!-- <a class="page-link" href="https://ventusff.github.io/assets/pdf/CV.pdf">vitae</a> -->

      </div>
    </nav>

  </div>

</header>



    <div class="page-content">
      <div class="wrapper">
        <div class="post">

  <header class="post-header">
    <h1 class="post-title">works in <strong>canonical spaces</strong>
</h1>
    <p class="post-subtitle">标准正视图空间的有关研究</p>
    <p class="post-meta"></p>
  </header>

  <article class="post-content">
    <ul id="markdown-toc">
  <li><a href="#nocsnormalized-object-coordinate-space-%E7%B3%BB%E5%88%97" id="markdown-toc-nocsnormalized-object-coordinate-space-系列">NOCS(Normalized Object Coordinate Space) 系列</a></li>
  <li><a href="#learning-a-canonical-representation-from-non-canonical-data" id="markdown-toc-learning-a-canonical-representation-from-non-canonical-data">learning a canonical representation from non-canonical data</a></li>
</ul>

<hr>

<h2 id="nocsnormalized-object-coordinate-space-系列">NOCS(Normalized Object Coordinate Space) 系列</h2>

<ul>
  <li>
<strong><a href="https://geometry.stanford.edu/projects/NOCS_CVPR2019/">[NOCS]</a></strong> NOCS for Pose Estimation [CVPR2019]</li>
  <li>
<strong><a href="https://geometry.stanford.edu/projects/xnocs/">[X-NOCS]</a></strong> Two-intersection NOCS for shape reconstruction [NeurIPS2019]</li>
  <li>
<strong><a href="https://articulated-pose.github.io/">[ANCSH]</a></strong> Articulated Pose Estimation [CVPR2020]</li>
  <li>
<strong><a href="https://geometry.stanford.edu/projects/pix2surf/">[S-NOCS]</a></strong> Shape reconstruction in NOCS with Surfaces (This work) [ECCV2020]</li>
  <li>
<strong><a href="https://geometry.stanford.edu/projects/caspr/">[T-NOCS]</a></strong> NOCS along Time Axis [arXiv2020 Pre-print]</li>
</ul>

<hr>

<p><strong><code class="language-plaintext highlighter-rouge">&lt;NOCS&gt; "Normalized Object Coordinate Space for Category-Level 6D Object Pose and Size Estimation"</code></strong><br>
<strong>[</strong> <code class="language-plaintext highlighter-rouge">CVPR2019</code> <strong>]</strong> <strong><a href="https://geometry.stanford.edu/projects/NOCS_CVPR2019/pub/NOCS_CVPR2019.pdf">[paper]</a></strong> <strong><a href="https://geometry.stanford.edu/projects/NOCS_CVPR2019/pub/NOCS_CVPR2019_Supp.pdf">[supp]</a></strong> <strong><a href="https://github.com/hughw19/NOCS_CVPR2019">[code]</a></strong> <strong><a href="https://geometry.stanford.edu/projects/NOCS_CVPR2019/">[web]</a></strong> <strong>[</strong> <img class="emoji" title=":mortar_board:" alt=":mortar_board:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f393.png" height="20" width="20"> <code class="language-plaintext highlighter-rouge">Stanford</code>, <code class="language-plaintext highlighter-rouge">Princeton University</code>  <strong>]</strong> <strong>[</strong> <img class="emoji" title=":office:" alt=":office:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f3e2.png" height="20" width="20"> <code class="language-plaintext highlighter-rouge">Facebook</code>, <code class="language-plaintext highlighter-rouge">Google</code> <strong>]</strong><br>
<strong>[</strong>  <code class="language-plaintext highlighter-rouge">He Wang</code>, <code class="language-plaintext highlighter-rouge">Srinath Sridhar</code>, <code class="language-plaintext highlighter-rouge">Jingwei Huang</code>, <code class="language-plaintext highlighter-rouge">Julien Valentin</code>, <code class="language-plaintext highlighter-rouge">Shuran Song</code>, <code class="language-plaintext highlighter-rouge">Leonidas J. Guibas</code>  <strong>]</strong><br>
<strong>[</strong> <em><code class="language-plaintext highlighter-rouge">abcd</code></em> <strong>]</strong></p>

<details>
  <summary>Click to expand</summary>

  <ul>
    <li><strong>Motivation</strong></li>
  </ul>

</details>

<hr>

<p><strong><code class="language-plaintext highlighter-rouge">&lt;X-NOCS&gt;"Multiview Aggregation for Learning Category-Specific Shape Reconstruction"</code></strong><br>
<strong>[</strong> <code class="language-plaintext highlighter-rouge">NeurIPS2019</code> <strong>]</strong> <strong><a href="https://geometry.stanford.edu/projects/xnocs/pub/xnocs.pdf">[paper]</a></strong> <strong><a href="https://github.com/drsrinathsridhar/xnocs/blob/master/README.md#2-download-the-datasets-see-below-for-details">[code]</a></strong> <strong><a href="https://geometry.stanford.edu/projects/xnocs/">[web]</a></strong> <strong>[</strong> <img class="emoji" title=":mortar_board:" alt=":mortar_board:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f393.png" height="20" width="20"> <code class="language-plaintext highlighter-rouge">Stanford</code> <strong>]</strong> <strong>[</strong> <img class="emoji" title=":office:" alt=":office:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f3e2.png" height="20" width="20"> <code class="language-plaintext highlighter-rouge">Google</code>, <code class="language-plaintext highlighter-rouge">Facebook</code> <strong>]</strong><br>
<strong>[</strong>  <code class="language-plaintext highlighter-rouge">Srinath Sridhar</code>, <code class="language-plaintext highlighter-rouge">Davis Rempe</code>, <code class="language-plaintext highlighter-rouge">Julien Valentin</code>, <code class="language-plaintext highlighter-rouge">Sofien Bouaziz</code>, <code class="language-plaintext highlighter-rouge">Leonidas J. Guibas</code>  <strong>]</strong><br>
<strong>[</strong> <em><code class="language-plaintext highlighter-rouge">abcd</code></em> <strong>]</strong></p>

<details>
  <summary>Click to expand</summary>

  <ul>
    <li><strong>Motivation</strong></li>
  </ul>

</details>

<hr>

<p><strong><code class="language-plaintext highlighter-rouge">&lt;ANCSH&gt; "Category-Level Articulated Object Pose Estimation"</code></strong><br>
<strong>[</strong> <code class="language-plaintext highlighter-rouge">CVPR2020</code> <strong>]</strong> <strong><a href="https://articulated-pose.github.io/paper.pdf">[paper]</a></strong> <strong><a href="https://github.com/dragonlong/articulated-pose">[code]</a></strong> <strong><a href="https://articulated-pose.github.io/">[web]</a></strong> <strong>[</strong> <img class="emoji" title=":mortar_board:" alt=":mortar_board:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f393.png" height="20" width="20"> <code class="language-plaintext highlighter-rouge">Virginia Tech</code>, <code class="language-plaintext highlighter-rouge">Stanford</code>, <code class="language-plaintext highlighter-rouge">Columbia University</code> <strong>]</strong> <strong>[</strong> <img class="emoji" title=":office:" alt=":office:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f3e2.png" height="20" width="20"> <code class="language-plaintext highlighter-rouge">Google</code> <strong>]</strong><br>
<strong>[</strong>  <code class="language-plaintext highlighter-rouge">Xiaolong Li</code>, <code class="language-plaintext highlighter-rouge">He Wang</code>, <code class="language-plaintext highlighter-rouge">Li Yi</code>, <code class="language-plaintext highlighter-rouge">Leonidas Guibas</code>, <code class="language-plaintext highlighter-rouge">A. Lynn Abbott</code>, <code class="language-plaintext highlighter-rouge">Shuran Song</code>  <strong>]</strong><br>
<strong>[</strong> <em><code class="language-plaintext highlighter-rouge">abcd</code></em> <strong>]</strong></p>

<details>
  <summary>Click to expand</summary>

  <ul>
    <li><strong>Motivation</strong></li>
  </ul>

</details>

<h2 id="learning-a-canonical-representation-from-non-canonical-data">learning a canonical representation from non-canonical data</h2>

<ul>
  <li>GIRAFFE</li>
  <li>spectralGAN</li>
  <li>Deep Implicit Templates for 3D Shape Representation
    <ul>
      <li>文中的regularizations loss中的point-wise regularizations，通过惩罚变形场前后的整体的位置变化，保证尽量学到一种所有shape统一、和canonical pose对齐的表征</li>
    </ul>
  </li>
</ul>

  </article>

  

</div>

      </div>
    </div>

    <footer>

  <div class="wrapper">
    © Copyright 2021 Jianfei Guo.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank">Unsplash</a>.

    
  </div>

</footer>


    <!-- Load jQuery -->
<script src="//code.jquery.com/jquery-1.12.4.min.js"></script>

<!-- Load Common JS -->
<script src="https://ventusff.github.io/assets/js/common.js"></script>




<!-- Load Anchor JS -->
<script src="//cdnjs.cloudflare.com/ajax/libs/anchor-js/3.2.2/anchor.min.js"></script>
<script>
  anchors.options.visible = 'always';
  anchors.add('article h2, article h3, article h4, article h5, article h6');
</script>



<!-- Load fancybox -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css">
<script src="https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js"></script>


<!-- Include custom icon fonts -->
<link rel="stylesheet" href="https://ventusff.github.io/assets/css/fontawesome-all.min.css">
<link rel="stylesheet" href="https://ventusff.github.io/assets/css/academicons.min.css">
<link rel="stylesheet" href="https://ventusff.github.io/assets/iconfont/iconfont.css">

<!-- Google Analytics -->
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-XXXXXXXXX', 'auto');
ga('send', 'pageview');
</script>

    
  </body>
  
<!-- Mermaid Js -->
<script>

    // mermaid rendering: inspired by https://stackoverflow.com/questions/53883747/how-to-make-github-pages-markdown-support-mermaid-diagram
    // mermiad in details tag: inspired by https://gitlab.com/gitlab-org/gitlab/-/issues/28495
    //          and by https://gitlab.com/gitlab-org/gitlab/-/blob/master/app/assets/javascripts/behaviors/markdown/render_mermaid.js

    function renderMermaids($els) {
        if (!$els.length) return;

        var config = {
            startOnLoad: false,
            theme: 'neutral', // forest
            securityLevel: 'loose',
            flowchart:{
                    useMaxWidth:true,
                    htmlLabels:false, // important for not squeezing blocks
                }
        };
        // const theme =localStorage.getItem("theme");
        // if(theme == "dark") config.theme = 'dark';  // currently needs to refresh to make dark mode toggle
        mermaid.initialize(config);

        $els.each((i, el) => {

            // Mermaid doesn't like `<br />` tags, so collapse all like tags into `<br>`, which is parsed correctly.
            const source = el.textContent.replace(/<br\s*\/>/g, '<br>');

            // Remove any extra spans added by the backend syntax highlighting.
            Object.assign(el, { textContent: source });

            mermaid.init(undefined, el, id => {
                const svg = document.getElementById(id);

                // As of https://github.com/knsv/mermaid/commit/57b780a0d,
                // Mermaid will make two init callbacks:one to initialize the
                // flow charts, and another to initialize the Gannt charts.
                // Guard against an error caused by double initialization.
                if (svg.classList.contains('mermaid')) {
                    console.log("return");
                    return;
                }

                // svg.classList.add('mermaid'); //will add new bug

                // pre > code > svg
                svg.closest('pre').replaceWith(svg);

                // We need to add the original source into the DOM to allow Copy-as-GFM
                // to access it.
                const sourceEl = document.createElement('text');
                sourceEl.classList.add('source');
                sourceEl.setAttribute('display', 'none');
                sourceEl.textContent = source;

                svg.appendChild(sourceEl);

            });
        });

    }

    const $els = $(document).find('.language-mermaid');
    if ($els.length)
    {
        const visibleMermaids = $els.filter(function filter() {
            return $(this).closest('details').length === 0 && $(this).is(':visible');
        });

        renderMermaids(visibleMermaids);

        $els.closest('details').one('toggle', function toggle() {
            if (this.open) {
                renderMermaids($(this).find('.language-mermaid'));
            }
        });
    }



</script>


  <!-- Auto config image to fancybox -->
<script>
    $(document).ready(function() {
        $("article img[class!='emoji']").each(function() {
            var currentImage = $(this);
            currentImage.wrap("<a href='" + currentImage.attr("src") + "' data-fancybox='lightbox' data-caption='" + currentImage.attr("alt") + "'></a>");
        });
    });
</script>

</html>
