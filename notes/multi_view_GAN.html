<!DOCTYPE html>
<html>
  <head>
    
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  <title>Jianfei Guo | multi-view + GAN / multi-view image generation</title>
  <meta name="description" content="Jianfei Guo. ffventus (at) gmail.com. I am a learner in representation learning and decision making. 
">

  <link rel="shortcut icon" href="https://ventusff.github.io/assets/img/favicon.ico?v=1">

  <link rel="stylesheet" href="https://ventusff.github.io/assets/css/main.css">
  <link rel="canonical" href="https://ventusff.github.io/notes/multi_view_GAN">
  

    
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams',
      inlineMath: [ ['$','$'], ["\\(","\\)"] ], // http://docs.mathjax.org/en/latest/input/tex/delimiters.html
      displayMath: [['\\[','\\]'], ['$$','$$']]
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    
<!-- Mermaid Js -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/mermaid/8.7.0/mermaid.min.js"></script>
<!-- <script src="https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js"></script> // this is the old version --> 

  </head>

  <body>

    <header class="site-header">

  <div class="wrapper">

    
    <span class="site-title">
        
        <strong>Jianfei</strong> Guo
    </span>
    

    <nav class="site-nav">
      <input type="checkbox" id="nav-trigger" class="nav-trigger">
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewbox="0 0 18 15" width="18px" height="15px">
              <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"></path>
              <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"></path>
              <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"></path>
            </svg>
          </span>
        </label>

      <div class="trigger">
        <!-- About -->
        <a class="page-link" href="https://ventusff.github.io/">about</a>

        <!-- Blog -->
        <a class="page-link" href="https://ventusff.github.io/blog/">blog</a>

        <!-- Pages -->
        
          
        
          
        
          
        
          
            <a class="page-link" href="https://ventusff.github.io/notes/">notes</a>
          
        
          
        
          
        
          
        

        <!-- CV link -->
        <!-- <a class="page-link" href="https://ventusff.github.io/assets/pdf/CV.pdf">vitae</a> -->

      </div>
    </nav>

  </div>

</header>



    <div class="page-content">
      <div class="wrapper">
        <div class="post">

  <header class="post-header">
    <h1 class="post-title">multi-view + GAN / multi-view image generation</h1>
    <p class="post-subtitle">多视角与GAN相关的结合</p>
    <p class="post-meta"></p>
  </header>

  <article class="post-content">
    <ul id="markdown-toc">
  <li><a href="#multi-view-image-generation" id="markdown-toc-multi-view-image-generation">multi-view image generation</a></li>
  <li><a href="#video-gan--video-generation" id="markdown-toc-video-gan--video-generation">video-GAN / video generation</a></li>
</ul>

<hr>

<h2 id="multi-view-image-generation">multi-view image generation</h2>
<ul>
  <li>keywords
    <ul>
      <li>GAN 3D scene multi view</li>
      <li>GAN multi view geometry</li>
      <li>multi view image generation from graph</li>
    </ul>
  </li>
  <li>GRAF</li>
  <li>pi-GAN</li>
</ul>

<hr>

<p><strong><code class="language-plaintext highlighter-rouge">"CR-GAN: Learning Complete Representations for Multi-view Generation"</code></strong><br>
<strong>[</strong> <code class="language-plaintext highlighter-rouge">IJCAI 2018</code> <strong>]</strong> <strong><a href="https://www.ijcai.org/Proceedings/2018/0131.pdf">[paper]</a></strong> <strong>[</strong> <img class="emoji" title=":mortar_board:" alt=":mortar_board:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f393.png" height="20" width="20"> <code class="language-plaintext highlighter-rouge">Rutgers University</code> <strong>]</strong><br>
<strong>[</strong>  <code class="language-plaintext highlighter-rouge">Yu Tian</code>,  <code class="language-plaintext highlighter-rouge">Dimitris N. Metaxas</code> <strong>]</strong><br>
<strong>[</strong> <em><code class="language-plaintext highlighter-rouge">complete representation</code>, <code class="language-plaintext highlighter-rouge">GAN</code>, <code class="language-plaintext highlighter-rouge">encoder-decoder</code></em>  <strong>]</strong></p>

<details>
  <summary>Click to expand</summary>

  <ul>
    <li>
<strong>Motivation</strong>
      <ul>
        <li>第一个调查GAN模型的”complete representations”</li>
        <li>用CR-GAN来学习完整的表达，使用一种两通路的模式(<code class="language-plaintext highlighter-rouge">reconstruction path</code> + <code class="language-plaintext highlighter-rouge">generation path</code>)</li>
        <li>CR-GAN可以利用<code class="language-plaintext highlighter-rouge">unlabeled data</code>来<code class="language-plaintext highlighter-rouge">self supervision</code>，使得生成的质量更好</li>
        <li>即使对于<strong>unseen</strong>的dataset，对于<strong>wild conditions</strong>，CR-GAN可以产生高质量的<strong>multi view</strong>图片</li>
      </ul>
    </li>
    <li>
      <p><strong>之前的GAN-based方法</strong>：encoder-decoder+discriminator</p>

      <ul>
        <li>
          <table>
            <thead>
              <tr>
                <th style="text-align: center"><img src="media/56666611.png" alt="img"></th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td style="text-align: center">相比于之前的GAN-based方法，多了一条<code class="language-plaintext highlighter-rouge">generation path</code>，试图补全z space</td>
              </tr>
            </tbody>
          </table>
        </li>
        <li>encoder把图片map到一个latent space，然后操作embedding，然后decoder生成新视角</li>
        <li>[CVPR 2017] <a href="https://openaccess.thecvf.com/content_cvpr_2017/papers/Tran_Disentangled_Representation_Learning_CVPR_2017_paper.pdf">[paper]</a> <dr-gan> Disentangled Representation Learning GAN for Pose-Invariant Face Recognition</dr-gan>
</li>
        <li>[2017]  Multi-view image generation from a single-view.</li>
        <li>
<strong><em>之前的GAN-based方法的问题</em></strong>：
          <ul>
            <li>学到的都是“不完整”的表征，对于”unseen”data\无边界的data的泛化性很差</li>
            <li>==<strong>思考</strong>==：encoder网络学到的大概率就是不完整的表征；这也是为什么用auto-decoder而不是encoder-decoder</li>
          </ul>
        </li>
      </ul>
    </li>
    <li>
<strong>proposal</strong>
      <ul>
        <li>除了<code class="language-plaintext highlighter-rouge">reconstruction path</code>外，引入另一条<code class="language-plaintext highlighter-rouge">generation path</code>来 从随机采样的sample 创建view-specific images</li>
        <li>两条path <strong>共享</strong>同样的G参数：在生成通路学到的G 会引导reconstruction path中的E和D的学习，反过来也是一样</li>
        <li>E is force to be G的逆向过程，使得学到的<strong>representation可以span the entire Z space</strong>
</li>
        <li>更重要的是，两通路的学习过程可以很容易地利用<strong>有label、无label</strong>的数据，对于自监督学习而言，从而大大丰富了Z space，对于自然的生成来说。</li>
      </ul>
    </li>
    <li>
<strong>discriminators</strong>
      <ul>
        <li><img src="media/57229841-1603684635887.png" alt="img"></li>
        <li>
<strong>==问题==</strong> ：原来这些GAN-based方法中的discriminator都是干什么用的？单纯只是增加图像的细节程度？</li>
        <li>DR-GAN中：discriminator有两个任务：① id 分类。discriminator输出一个分类输出。② pose分类。分类器输出。</li>
      </ul>
    </li>
  </ul>

</details>

<hr>

<p><strong><code class="language-plaintext highlighter-rouge">&lt; multi-view BiGAN &gt; "Multi-view Generative Adversarial Networks"</code></strong><br>
<strong>[</strong> <code class="language-plaintext highlighter-rouge">2016</code> <strong>]</strong> <strong><a href="https://arxiv.org/pdf/1611.02019.pdf">[paper]</a></strong> <strong>[</strong> <img class="emoji" title=":mortar_board:" alt=":mortar_board:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f393.png" height="20" width="20"> <code class="language-plaintext highlighter-rouge">UPMC</code> <strong>]</strong> 
<strong>[</strong>  <code class="language-plaintext highlighter-rouge">Mickaël Chen</code>, <code class="language-plaintext highlighter-rouge">Ludovic Denoyer</code>  <strong>]</strong><br>
<strong>[</strong> <em><code class="language-plaintext highlighter-rouge">BiGAN</code></em> <strong>]</strong></p>

<details>
  <summary>Click to expand</summary>
  <p>too old</p>

  <ul>
    <li>
<strong>Motivation</strong>
      <ul>
        <li>
<img src="media/54910786.png" alt="img">
          <ul>
            <li>把BiGAN改造成适应conditional 概率；一个可以学到P(y|x)，x可以是单张图片或者是多张view的集合</li>
          </ul>
        </li>
        <li>
<img src="media/55062754.png" alt="img">
          <ul>
            <li>创造了一个multi-view model，给予任意一组subset of views，评估可能的输出的分布</li>
            <li>如果说，一种自然的把BiGAN 延伸到适应multi view输入的方式 是 定义一个从a set of view到一个representation space的mapping function，那么，这种方法已经被证明会有不达到要求的表现</li>
            <li>因此，我们提出了一种约束模型的方式：基于一个想法：对任意一组subset of views添加一个view都应该降低输出分布的不确定性。给的view越多，方差越小。用KL散度来正则化</li>
          </ul>
        </li>
      </ul>
    </li>
  </ul>

</details>

<hr>

<p><strong><code class="language-plaintext highlighter-rouge">"Conditional Single-view Shape Generation for Multi-view Stereo Reconstruction"</code></strong><br>
<strong>[</strong> <code class="language-plaintext highlighter-rouge">2019</code> <strong>]</strong> <strong><a href="https://arxiv.org/pdf/1904.06699.pdf">[paper]</a></strong> <strong><a href="https://github.com/weiyithu/OptimizeMVS">[code]</a></strong> <strong>[</strong> <img class="emoji" title=":mortar_board:" alt=":mortar_board:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f393.png" height="20" width="20"> <code class="language-plaintext highlighter-rouge">Tsinghua University</code> <strong>]</strong><br>
<strong>[</strong>  <code class="language-plaintext highlighter-rouge">Yi Wei</code>, <code class="language-plaintext highlighter-rouge">Shaohui Liu</code>, <code class="language-plaintext highlighter-rouge">Jiwen Lu</code>  <strong>]</strong><br>
<strong>[</strong> <em><code class="language-plaintext highlighter-rouge">multi view geometry constraint</code>, <code class="language-plaintext highlighter-rouge">image based shape generation</code></em> <strong>]</strong></p>

<details>
  <summary>Click to expand</summary>

  <ul>
    <li>
<strong>Motivation</strong>
      <ul>
        <li>
<strong>task</strong>: image based shape generation</li>
        <li>把多张图片的重建问题 建模为 计算每个单张图片重建出的shape 空间的 交集</li>
      </ul>
    </li>
  </ul>

</details>

<hr>

<p><strong><code class="language-plaintext highlighter-rouge">"Multi-view Relighting using a Geometry-Aware Network"</code></strong><br>
<strong>[</strong> <code class="language-plaintext highlighter-rouge">ACM T-Graphics 2019</code> <strong>]</strong> <strong><a href="https://repo-sam.inria.fr/fungraph/deep-relighting/Multi-view-Relighting.pdf">[paper]</a></strong> <strong>[</strong> <img class="emoji" title=":mortar_board:" alt=":mortar_board:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f393.png" height="20" width="20"> <code class="language-plaintext highlighter-rouge">Université Côte d'Azur and Inria</code>, <code class="language-plaintext highlighter-rouge">UCB</code> <strong>]</strong> <strong>[</strong> <img class="emoji" title=":office:" alt=":office:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f3e2.png" height="20" width="20"> <code class="language-plaintext highlighter-rouge">Adobe</code> <strong>]</strong><br>
<strong>[</strong>  <code class="language-plaintext highlighter-rouge">Julien Philip</code>, <code class="language-plaintext highlighter-rouge">MICHAËL GHARBI</code>,<code class="language-plaintext highlighter-rouge">TINGHUI ZHOU</code>, <code class="language-plaintext highlighter-rouge">ALEXEI A. EFROS</code>, <code class="language-plaintext highlighter-rouge">GEORGE DRETTAKIS</code> <strong>]</strong><br>
<strong>[</strong> <em><code class="language-plaintext highlighter-rouge">multi view video relighting</code></em> <strong>]</strong></p>

<details>
  <summary>Click to expand</summary>

  <ul>
    <li>
<strong>Motivation</strong>
      <ul>
        <li>multi-view video relighting</li>
        <li>首先从multi view的视频创建一个proxy geometry，然后考虑relighting</li>
        <li><img src="media/54422638.png" alt="img"></li>
        <li><img src="media/54653900.png" alt="img"></li>
      </ul>
    </li>
  </ul>

</details>

<hr>

<p><strong><code class="language-plaintext highlighter-rouge">&lt; GcGAN &gt; "Geometry-Consistent Generative Adversarial Networks for One-Sided Unsupervised Domain Mapping"</code></strong><br>
<strong>[</strong> <code class="language-plaintext highlighter-rouge">CVPR2019</code> <strong>]</strong> <strong><a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Fu_Geometry-Consistent_Generative_Adversarial_Networks_for_One-Sided_Unsupervised_Domain_Mapping_CVPR_2019_paper.pdf">[paper]</a></strong> <strong><a href="https://github.com/hufu6371/GcGAN">[code]</a></strong> <strong>[</strong> <img class="emoji" title=":mortar_board:" alt=":mortar_board:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f393.png" height="20" width="20"> <code class="language-plaintext highlighter-rouge">University of Sydney</code>, <code class="language-plaintext highlighter-rouge">University of Pittsburgh</code>, <code class="language-plaintext highlighter-rouge">CMU</code>,<code class="language-plaintext highlighter-rouge">Universite Paris-Est</code> <strong>]</strong> <br>
<strong>[</strong>  <code class="language-plaintext highlighter-rouge">Huan Fu</code>, <code class="language-plaintext highlighter-rouge">Mingming Gong</code>, <code class="language-plaintext highlighter-rouge">Chaohui Wang</code>, <code class="language-plaintext highlighter-rouge">Kayhan Batmanghelich</code>, <code class="language-plaintext highlighter-rouge">Kun Zhang</code>, <code class="language-plaintext highlighter-rouge">Dacheng Tao</code>  <strong>]</strong><br>
<strong>[</strong> <em><code class="language-plaintext highlighter-rouge">abcd</code></em> <strong>]</strong></p>

<details>
  <summary>Click to expand</summary>

  <ul>
    <li><strong>Motivation</strong></li>
  </ul>

</details>

<hr>

<p><strong><code class="language-plaintext highlighter-rouge">"RGBD-GAN: Unsupervised 3D Representation Learning From Natural Image Datasets via RGBD Image Synthesis"</code></strong><br>
<strong>[</strong> <code class="language-plaintext highlighter-rouge">ICLR2020</code> <strong>]</strong> <strong><a href="https://arxiv.org/abs/1909.12573">[paper]</a></strong>  <strong>[</strong> <img class="emoji" title=":mortar_board:" alt=":mortar_board:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f393.png" height="20" width="20"> <code class="language-plaintext highlighter-rouge">The University of Tokyo</code> <strong>]</strong> <strong>[</strong> <img class="emoji" title=":office:" alt=":office:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f3e2.png" height="20" width="20"> <code class="language-plaintext highlighter-rouge">RIKEN</code> <strong>]</strong><br>
<strong>[</strong>  <code class="language-plaintext highlighter-rouge">Atsuhiro Noguchi</code>, <code class="language-plaintext highlighter-rouge">Tatsuya Harada</code>  <strong>]</strong><br>
<strong>[</strong> <em><code class="language-plaintext highlighter-rouge">multi view geometry constraint</code>, <code class="language-plaintext highlighter-rouge">natural dataset</code>, <code class="language-plaintext highlighter-rouge">3D representation</code></em> <strong>]</strong></p>

<details>
  <summary>Click to expand</summary>

  <ul>
    <li>
<strong>Motivation</strong>
      <ul>
        <li>
<strong>natural datasets</strong>下，通过RGBD图像生成，进行<strong>无监督</strong>的<strong>3D表征</strong>学习</li>
        <li>
<strong>3.2.2 SELF-SUPERVISED RGBD CONSISTENCY LOSS</strong>
          <ul>
            <li><img src="media/65147862.png" alt="img"></li>
          </ul>
        </li>
      </ul>
    </li>
  </ul>

</details>

<hr>

<p><strong><code class="language-plaintext highlighter-rouge">"Towards Unsupervised Learning of Generative Models for 3D Controllable Image Synthesis"</code></strong><br>
<strong>[</strong> <code class="language-plaintext highlighter-rouge">CVPR2020</code> <strong>]</strong> <strong><a href="https://arxiv.org/pdf/1912.05237.pdf">[paper]</a></strong> <strong><a href="https://github.com/autonomousvision/controllable_image_synthesis">[code]</a></strong> <strong>[</strong> <img class="emoji" title=":mortar_board:" alt=":mortar_board:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f393.png" height="20" width="20"> <code class="language-plaintext highlighter-rouge">University of Tubingen</code>, <code class="language-plaintext highlighter-rouge">MPI-IS</code> <strong>]</strong> <strong>[</strong> <img class="emoji" title=":office:" alt=":office:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f3e2.png" height="20" width="20"> <code class="language-plaintext highlighter-rouge">Amazon</code> <strong>]</strong><br>
<strong>[</strong>  <code class="language-plaintext highlighter-rouge">Yiyi Liao</code>,<code class="language-plaintext highlighter-rouge">Katja Schwarz</code>,<code class="language-plaintext highlighter-rouge">Lars Mescheder</code>, <code class="language-plaintext highlighter-rouge">Andreas Geiger</code>  <strong>]</strong><br>
<strong>[</strong> <a href="https://github.com/autonomousvision">autonomous_vision lab</a> <strong>]</strong><br>
<strong>[</strong> <em><code class="language-plaintext highlighter-rouge">HoloGAN-&gt;baseline</code>,<code class="language-plaintext highlighter-rouge">RGBD-GAN based</code>, <code class="language-plaintext highlighter-rouge">multi object multi view</code>,  <code class="language-plaintext highlighter-rouge">Controllable Image Synthesis</code></em> <strong>]</strong></p>

<details>
  <summary>Click to expand</summary>

  <ul>
    <li>
<strong>Motivation</strong>
      <ul>
        <li>首先 从一个高斯采样的latent code 映射到一系列3D primitives（一些原初3D物体表征）<br> 再渲染物体 再渲染背景<br><img src="media/59007435.png" alt="img">
</li>
      </ul>
    </li>
  </ul>

  <table>
    <thead>
      <tr>
        <th>Input</th>
        <th style="text-align: left">unlabeled image</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>output</td>
        <td style="text-align: left">multi view images</td>
      </tr>
      <tr>
        <td>dataset</td>
        <td style="text-align: left">
<img src="media/59114641.png" alt="img"> <br><strong>随机背景、随机物体、随机view point</strong> <br>3D primitives: no label <br> instance segmentation: no label <br> pose annotations: no label</td>
      </tr>
    </tbody>
  </table>

  <ul>
    <li>训练这样的模型是有挑战的：
      <ul>
        <li>比如有可能把2个物体理解为同一个primitive，甚至…；</li>
        <li>因此，使用多个loss来鼓励一个解耦、可解释的3D表征；同时从训练集分布中生成图片。</li>
      </ul>
    </li>
    <li>
<strong>loss</strong>
      <ul>
        <li>
<em><strong>adversarial loss</strong></em>：标准的real/fake loss + condition
          <ul>
            <li>
              <blockquote>
                <p>condition on: 是完全的composite image还是background image</p>

                <p>实验证明，这个condition有助于从背景中解耦物体</p>
              </blockquote>
            </li>
            <li>因此在训练时，收集两组数据集：带有物体的和没有物体的</li>
          </ul>
        </li>
        <li>
<em><strong>compactness loss</strong></em> ：紧凑性loss
          <ul>
            <li>
              <blockquote>
                <p>To bias solutions towards compact representations and to encourage the 3D primitives to tightly encase the objects, we minimize the projected shape of each object.</p>

                <p>为了让solutions 倾向于完整的表征，鼓励3D primitives能够紧贴合物体，我们最小化每个物体的投影shape</p>
              </blockquote>
            </li>
            <li>惩罚每个物体<code class="language-plaintext highlighter-rouge">alpha map</code>的 <code class="language-plaintext highlighter-rouge">L1-范数</code>
</li>
            <li>
              <blockquote>
                <p><img src="media/63491889.png" alt="img"></p>

                <p>\(\tau=0.1\) 是一个防止收缩到一个固定最小值以下的截短阈值， \(A_i\) 依赖于模型参数和 latent code z（so 这个loss可以对模型参数有作用）</p>
              </blockquote>
            </li>
          </ul>
        </li>
        <li>
<strong>(==self supervised==) geometry consistency loss</strong>
          <ul>
            <li>
              <blockquote>
                <p>为了得到在不同的 <code class="language-plaintext highlighter-rouge">camera viewpoints</code> 和 <code class="language-plaintext highlighter-rouge">3D物体pose </code>中都<strong>consistent</strong>的solutions，遵循 <em><strong>[33]RGBD-GAN</strong></em> 来鼓励生成模型来遵守多视几何约束。</p>
              </blockquote>
            </li>
            <li>
              <blockquote>
                <p>比如，对于pose(外参)的改变应该改变物体的pose但是不应该alter它的颜色或者identity.</p>
              </blockquote>
            </li>
            <li>
              <blockquote>
                <p>这样formulate这个约束：</p>

                <p><img src="media/63872134.png" alt="img"></p>

                <p>\(X_i'\) \(D_i'\) 是 latent code z的2D generator 输出</p>

                <p>\(\tilde{X}_i'\) \(\tilde{D}_i'\) 是 同一个latent code对每个primitive的pose加入随机噪声 并且 [<strong>Warp</strong>ing the result back to the original viewpoint] (即<strong>重投影</strong>回加噪声之前的viewpoint)  后的2D generator输出</p>
              </blockquote>
            </li>
            <li>相当于是一个自监督的重投影误差loss</li>
          </ul>
        </li>
      </ul>
    </li>
  </ul>

</details>

<hr>

<p><strong><code class="language-plaintext highlighter-rouge">"Inverse Graphics GAN: Learning to Generate 3D Shapes from Unstructured 2D Data"</code></strong><br>
<strong>[</strong> <code class="language-plaintext highlighter-rouge">2020</code> <strong>]</strong> <strong>[<a href="https://arxiv.org/pdf/2002.12674.pdf">paper</a>, <a href="https://lunz-s.github.io/iggan/iggan_supplemental.pdf">supp</a>]</strong>  <strong>[</strong> <img class="emoji" title=":mortar_board:" alt=":mortar_board:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f393.png" height="20" width="20"> <code class="language-plaintext highlighter-rouge">University of Cambridge</code> <strong>]</strong><br>
<strong>[</strong>  <code class="language-plaintext highlighter-rouge">Sebastian Lunz</code>, <code class="language-plaintext highlighter-rouge">Yingzhen Li</code>, <code class="language-plaintext highlighter-rouge">Andrew Fitzgibbon</code>, <code class="language-plaintext highlighter-rouge">Nate Kushman</code>  <strong>]</strong><br>
<strong>[</strong> <em><code class="language-plaintext highlighter-rouge">Inverse Graphics</code>, <code class="language-plaintext highlighter-rouge">GAN</code>, <code class="language-plaintext highlighter-rouge">3D shape generation</code></em> <strong>]</strong></p>

<details>
  <summary>Click to expand</summary>

  <ul>
    <li>
<strong>Motivation</strong>
      <ul>
        <li>从非结构化的2D数据生成voxels类3D shape</li>
        <li><img src="media/image-20201026194450596.png" alt="image-20201026194450596"></li>
      </ul>
    </li>
    <li>results：
      <ul>
        <li><img src="media/image-20201026193704383.png" alt="image-20201026193704383"></li>
      </ul>
    </li>
  </ul>

</details>

<hr>

<p><strong><code class="language-plaintext highlighter-rouge">&lt; 3D multi object GAN &gt;"Fully Convolutional Refined Auto-Encoding Generative Adversarial Networks for 3D Multi Object Scenes"</code></strong><br>
<strong>[</strong> <code class="language-plaintext highlighter-rouge">blog 2018</code> <strong>]</strong>  <strong><a href="https://github.com/yunishi3/3D-FCR-alphaGAN">[code]</a></strong> <strong><a href="https://becominghuman.ai/3d-multi-object-gan-7b7cee4abf80">[blog]</a></strong> <strong>[</strong> <img class="emoji" title=":mortar_board:" alt=":mortar_board:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f393.png" height="20" width="20"> <code class="language-plaintext highlighter-rouge">Stanford AI Lab</code> <strong>]</strong> <br>
<strong>[</strong>  <code class="language-plaintext highlighter-rouge">Yu Nishimura</code>  <strong>]</strong><br>
<strong>[</strong> <em><code class="language-plaintext highlighter-rouge">3DGAN</code>, <code class="language-plaintext highlighter-rouge">3D shape generation</code></em> <strong>]</strong></p>

<details>
  <summary>Click to expand</summary>

  <ul>
    <li><strong>Motivation</strong></li>
    <li>
<strong>dataset</strong>
      <ul>
        <li>ground truth <strong>voxel</strong> data of SUNCG dataset.</li>
      </ul>
    </li>
    <li>
<strong>results</strong>
      <ul>
        <li><img src="media/image-20201026195610963.png" alt="image-20201026195610963"></li>
      </ul>
    </li>
  </ul>

</details>

<h2 id="video-gan--video-generation">video-GAN / video generation</h2>

<ul>
  <li>用的discriminator一般都是时空卷积；主要考虑的是时间域的分布，没有管multi view</li>
</ul>

  </article>

  

</div>

      </div>
    </div>

    <footer>

  <div class="wrapper">
    © Copyright 2021 Jianfei Guo.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank">Unsplash</a>.

    
  </div>

</footer>


    <!-- Load jQuery -->
<script src="//code.jquery.com/jquery-1.12.4.min.js"></script>

<!-- Load Common JS -->
<script src="https://ventusff.github.io/assets/js/common.js"></script>




<!-- Load Anchor JS -->
<script src="//cdnjs.cloudflare.com/ajax/libs/anchor-js/3.2.2/anchor.min.js"></script>
<script>
  anchors.options.visible = 'always';
  anchors.add('article h2, article h3, article h4, article h5, article h6');
</script>



<!-- Load fancybox -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css">
<script src="https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js"></script>


<!-- Include custom icon fonts -->
<link rel="stylesheet" href="https://ventusff.github.io/assets/css/fontawesome-all.min.css">
<link rel="stylesheet" href="https://ventusff.github.io/assets/css/academicons.min.css">
<link rel="stylesheet" href="https://ventusff.github.io/assets/iconfont/iconfont.css">

<!-- Google Analytics -->
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-XXXXXXXXX', 'auto');
ga('send', 'pageview');
</script>

    
  </body>
  
<!-- Mermaid Js -->
<script>

    // mermaid rendering: inspired by https://stackoverflow.com/questions/53883747/how-to-make-github-pages-markdown-support-mermaid-diagram
    // mermiad in details tag: inspired by https://gitlab.com/gitlab-org/gitlab/-/issues/28495
    //          and by https://gitlab.com/gitlab-org/gitlab/-/blob/master/app/assets/javascripts/behaviors/markdown/render_mermaid.js

    function renderMermaids($els) {
        if (!$els.length) return;

        var config = {
            startOnLoad: false,
            theme: 'neutral', // forest
            securityLevel: 'loose',
            flowchart:{
                    useMaxWidth:true,
                    htmlLabels:false, // important for not squeezing blocks
                }
        };
        // const theme =localStorage.getItem("theme");
        // if(theme == "dark") config.theme = 'dark';  // currently needs to refresh to make dark mode toggle
        mermaid.initialize(config);

        $els.each((i, el) => {

            // Mermaid doesn't like `<br />` tags, so collapse all like tags into `<br>`, which is parsed correctly.
            const source = el.textContent.replace(/<br\s*\/>/g, '<br>');

            // Remove any extra spans added by the backend syntax highlighting.
            Object.assign(el, { textContent: source });

            mermaid.init(undefined, el, id => {
                const svg = document.getElementById(id);

                // As of https://github.com/knsv/mermaid/commit/57b780a0d,
                // Mermaid will make two init callbacks:one to initialize the
                // flow charts, and another to initialize the Gannt charts.
                // Guard against an error caused by double initialization.
                if (svg.classList.contains('mermaid')) {
                    console.log("return");
                    return;
                }

                // svg.classList.add('mermaid'); //will add new bug

                // pre > code > svg
                svg.closest('pre').replaceWith(svg);

                // We need to add the original source into the DOM to allow Copy-as-GFM
                // to access it.
                const sourceEl = document.createElement('text');
                sourceEl.classList.add('source');
                sourceEl.setAttribute('display', 'none');
                sourceEl.textContent = source;

                svg.appendChild(sourceEl);

            });
        });

    }

    const $els = $(document).find('.language-mermaid');
    if ($els.length)
    {
        const visibleMermaids = $els.filter(function filter() {
            return $(this).closest('details').length === 0 && $(this).is(':visible');
        });

        renderMermaids(visibleMermaids);

        $els.closest('details').one('toggle', function toggle() {
            if (this.open) {
                renderMermaids($(this).find('.language-mermaid'));
            }
        });
    }



</script>


  <!-- Auto config image to fancybox -->
<script>
    $(document).ready(function() {
        $("article img[class!='emoji']").each(function() {
            var currentImage = $(this);
            currentImage.wrap("<a href='" + currentImage.attr("src") + "' data-fancybox='lightbox' data-caption='" + currentImage.attr("alt") + "'></a>");
        });
    });
</script>

</html>
